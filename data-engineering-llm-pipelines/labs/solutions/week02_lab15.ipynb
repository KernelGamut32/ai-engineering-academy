{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 15 ‚Äî Loading JSONL with ü§ó Datasets\n",
    "\n",
    "**Focus Area:** Load **JSONL** from Lab 13/14; **map/tokenize**; **shuffle & batch**; **train/test splits**; save reusable Arrow caches\n",
    "\n",
    "> This lab **builds on Lab 13 & 14 artifacts**. You will use ü§ó **datasets** to load JSONL files, create splits, and (optionally) tokenize **without internet** using a tiny local tokenizer. You‚Äôll finish with batched, shuffled, memory‚Äëefficient datasets ready for model training or evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Outcomes\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Load line‚Äëdelimited **JSONL** into `datasets.Dataset` and `DatasetDict`.  \n",
    "2. Create **deterministic** train/validation/test splits and **shuffle** with a fixed seed.  \n",
    "3. Apply **batched transforms** via `map` (cleaning, schema alignment, tokenization).  \n",
    "4. Batch with **dynamic padding** and persist datasets to disk for reuse.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites & Setup\n",
    "\n",
    "- Python 3.13 with `datasets`, `pandas`, `orjson`, `numpy`, `tokenizers` (for local tokenizer), `torch` (or `numpy` if not using PyTorch)  \n",
    "- JupyterLab or VS Code with Jupyter extension.\n",
    "- **Artifacts from previous labs (required):**\n",
    "  - `artifacts/jsonl/instruct_prompt_completion.jsonl` **or** `instruct_prompt_completion_cleansed.jsonl` (Lab 14)\n",
    "  - `artifacts/jsonl/instruct_trio.jsonl` (Lab 14) ‚Äî optional\n",
    "  - `artifacts/jsonl/rag_chunks_from_csv.jsonl` (Lab 13) ‚Äî optional\n",
    "\n",
    "**Start a notebook:** `week02_lab15.ipynb`\n",
    "\n",
    "Create working dirs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "for p in ['artifacts/hf_cache','artifacts/tokenizer','artifacts/datasets','artifacts/samples']:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **No internet assumption:** We‚Äôll **train a tiny tokenizer locally** using `tokenizers` to avoid downloads. If you already have a local tokenizer (e.g., a copied `gpt2` tokenizer folder), you can use that instead.\n",
    "\n",
    "---\n",
    "\n",
    "## Part A ‚Äî Load JSONL with Datasets\n",
    "\n",
    "### A1. Pick your source JSONL\n",
    "\n",
    "Choose **one** main file (prefer cleansed prompt‚Äëcompletion):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test JSONL file with 10 examples\n"
     ]
    }
   ],
   "source": [
    "# Create a larger test dataset with at least 10 examples\n",
    "test_data = [\n",
    "    {\"prompt\": \"What is AI?\", \"completion\": \"AI stands for Artificial Intelligence.\"},\n",
    "    {\"prompt\": \"Explain machine learning\", \"completion\": \"Machine learning is a subset of AI that learns from data.\"},\n",
    "    {\"prompt\": \"What is deep learning?\", \"completion\": \"Deep learning uses neural networks with multiple layers.\"},\n",
    "    {\"prompt\": \"Define NLP\", \"completion\": \"Natural Language Processing helps computers understand human language.\"},\n",
    "    {\"prompt\": \"What is computer vision?\", \"completion\": \"Computer vision enables machines to interpret visual information.\"},\n",
    "    {\"prompt\": \"Explain reinforcement learning\", \"completion\": \"Reinforcement learning trains agents through rewards and penalties.\"},\n",
    "    {\"prompt\": \"What is supervised learning?\", \"completion\": \"Supervised learning uses labeled data to train models.\"},\n",
    "    {\"prompt\": \"Define unsupervised learning\", \"completion\": \"Unsupervised learning finds patterns in unlabeled data.\"},\n",
    "    {\"prompt\": \"What is a neural network?\", \"completion\": \"A neural network is inspired by biological neurons and processes information.\"},\n",
    "    {\"prompt\": \"Explain gradient descent\", \"completion\": \"Gradient descent optimizes model parameters by minimizing loss.\"},\n",
    "]\n",
    "\n",
    "import json\n",
    "with open('artifacts/jsonl/instruct_prompt_completion.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for record in test_data:\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"Created test JSONL file with {len(test_data)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Load as Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/sysadmin/llm_venv/lib/python3.13/site-packages (4.3.0)\n",
      "Requirement already satisfied: filelock in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from datasets) (1.0.1)\n",
      "Requirement already satisfied: packaging in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: shellingham in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sysadmin/llm_venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 10 examples [00:00, 812.25 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install datasets\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define the source file (created in the previous cell)\n",
    "SRC = Path('artifacts/jsonl/instruct_prompt_completion.jsonl')\n",
    "\n",
    "# Verify file exists before loading\n",
    "if not SRC.exists():\n",
    "    raise FileNotFoundError(f\"JSONL file not found at {SRC}. Run the previous cell to create it.\")\n",
    "\n",
    "ds = load_dataset('json', data_files=str(SRC), split='train')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. Quick sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': Value('string'), 'completion': Value('string')}\n",
      "num_rows = 10\n",
      "columns  = ['prompt', 'completion']\n"
     ]
    }
   ],
   "source": [
    "# Peek at schema & first rows\n",
    "print(ds.features)\n",
    "ds.select(range(2))\n",
    "\n",
    "# Basic counts\n",
    "print('num_rows =', ds.num_rows)\n",
    "print('columns  =', ds.column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: `datasets` streams from Arrow cache on disk; transforms are lazy until materialized. Fast, memory‚Äëefficient.\n",
    "\n",
    "---\n",
    "\n",
    "## Part B ‚Äî Deterministic Splits & Shuffling\n",
    "\n",
    "### B1. Canonical column alignment\n",
    "\n",
    "Ensure required columns exist and are strings; add metadata fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 1177.51 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 971.08 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion', 'metadata'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ensure_schema(batch):\n",
    "    # Expect {prompt, completion, metadata?}\n",
    "    prompts = batch.get('prompt')\n",
    "    completions = batch.get('completion')\n",
    "    inputs = batch.get('input')\n",
    "    outputs = batch.get('output')\n",
    "    # Align Trio ‚Üí prompt/completion if needed\n",
    "    if (prompts is None or completions is None) and inputs is not None and outputs is not None:\n",
    "        prompts = [f\"### Instruction:\\n{ins}\\n\\n### Response:\\n\" for ins in inputs]\n",
    "        completions = outputs\n",
    "    return {\n",
    "        'prompt': list(map(lambda x: '' if x is None else str(x), prompts)),\n",
    "        'completion': list(map(lambda x: '' if x is None else str(x), completions)),\n",
    "        'metadata': batch.get('metadata') or [{}]*len(prompts)\n",
    "    }\n",
    "\n",
    "ds_aligned = ds.map(ensure_schema, batched=True, remove_columns=ds.column_names)\n",
    "ds_aligned = ds_aligned.filter(lambda ex: len(ex['prompt'])>0 and len(ex['completion'])>0)\n",
    "ds_aligned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Train/validation/test split (deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion', 'metadata'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'completion', 'metadata'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'completion', 'metadata'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 13\n",
    "\n",
    "# Check if dataset is large enough for splits\n",
    "if len(ds_aligned) < 10:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Only {len(ds_aligned)} examples. Creating minimal splits.\")\n",
    "    # For very small datasets, just use different percentages\n",
    "    train_test = ds_aligned.train_test_split(test_size=0.3, seed=seed)\n",
    "    \n",
    "    # If we have at least 3 examples in test, split it further\n",
    "    if len(train_test['test']) >= 2:\n",
    "        val_test = train_test['test'].train_test_split(test_size=0.5, seed=seed)\n",
    "        dsd = {\n",
    "            'train': train_test['train'],\n",
    "            'validation': val_test['train'],\n",
    "            'test': val_test['test']\n",
    "        }\n",
    "    else:\n",
    "        # Too small for 3-way split, just use train/test\n",
    "        print(\"‚ö†Ô∏è  Dataset too small for validation split. Using train/test only.\")\n",
    "        dsd = {\n",
    "            'train': train_test['train'],\n",
    "            'test': train_test['test']\n",
    "        }\n",
    "else:\n",
    "    # Normal splits for larger datasets\n",
    "    train_test = ds_aligned.train_test_split(test_size=0.2, seed=seed)\n",
    "    val_test = train_test['test'].train_test_split(test_size=0.5, seed=seed)\n",
    "    \n",
    "    dsd = {\n",
    "        'train': train_test['train'],\n",
    "        'validation': val_test['train'],\n",
    "        'test': val_test['test']\n",
    "    }\n",
    "\n",
    "from datasets import DatasetDict\n",
    "dds = DatasetDict(dsd)\n",
    "\n",
    "# Shuffle each split with same seed for determinism\n",
    "dds = DatasetDict({k: v.shuffle(seed=seed) for k,v in dds.items()})\n",
    "dds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Persist splits to disk (Arrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 951.25 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 163.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 159.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dds.save_to_disk('artifacts/datasets/instruct_pc_splits')\n",
    "# Load back later with:  datasets.load_from_disk(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part C ‚Äî Tokenization (Offline) with `tokenizers`\n",
    "\n",
    "We‚Äôll **train a tiny Byte‚ÄëLevel BPE tokenizer** on the prompts to avoid network downloads.\n",
    "\n",
    "### C1. Export prompt text for training the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('artifacts/tokenizer/train_prompts.txt'), 182)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "train_txt = Path('artifacts/tokenizer/train_prompts.txt')\n",
    "with train_txt.open('w', encoding='utf-8') as f:\n",
    "    for ex in dds['train']:\n",
    "        f.write(ex['prompt'].replace('\\n','\\n') + '\\n')\n",
    "train_txt, train_txt.stat().st_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2. Train a small tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /home/sysadmin/llm_venv/lib/python3.13/site-packages (0.22.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from tokenizers) (1.0.1)\n",
      "Requirement already satisfied: filelock in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
      "Requirement already satisfied: anyio in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers) (8.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install tokenizers\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.processors import ByteLevel as ByteLevelProcessor\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "\n",
    "vocab_size = 8_000  # small for classroom speed\n",
    "unk_token = \"<unk>\"\n",
    "\n",
    "_tok = Tokenizer(BPE(unk_token=unk_token))\n",
    "_tok.pre_tokenizer = ByteLevel()\n",
    "trainer = BpeTrainer(vocab_size=vocab_size, min_frequency=2, special_tokens=[unk_token, \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "_tok.train([str(train_txt)], trainer)\n",
    "_tok.post_processor = ByteLevelProcessor()\n",
    "_tok.decoder = ByteLevelDecoder()\n",
    "_tok.save('artifacts/tokenizer/bytebpe.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3. Wrap as a simple encode function for `datasets.map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "tok = Tokenizer.from_file('artifacts/tokenizer/bytebpe.json')\n",
    "pad_id = tok.token_to_id('<pad>')\n",
    "bos_id = tok.token_to_id('<bos>')\n",
    "eos_id = tok.token_to_id('<eos>')\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "def encode_batch(batch):\n",
    "    ids = []\n",
    "    attn = []\n",
    "    for p, c in zip(batch['prompt'], batch['completion']):\n",
    "        text = p + c  # simple concat; for chat models you may add BOS/EOS markers between\n",
    "        enc = tok.encode(text)\n",
    "        input_ids = enc.ids[:max_len-1] + [eos_id if eos_id is not None else 0]\n",
    "        # pad\n",
    "        pad_len = max_len - len(input_ids)\n",
    "        if pad_len > 0:\n",
    "            input_ids = input_ids + [pad_id]*pad_len\n",
    "        attention_mask = [1]*min(len(enc.ids)+1, max_len) + [0]*max(0, max_len - (min(len(enc.ids)+1, max_len)))\n",
    "        ids.append(input_ids)\n",
    "        attn.append(attention_mask)\n",
    "    return {'input_ids': ids, 'attention_mask': attn}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4. Apply batched tokenization with `map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 530.60 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 147.55 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 171.27 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_enc = dds.map(encode_batch, batched=True, batch_size=256, remove_columns=dds['train'].column_names)\n",
    "batched_enc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C5. Set format for PyTorch and create DataLoaders (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/sysadmin/llm_venv/lib/python3.13/site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 512]), torch.Size([8, 512]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install torch\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "for split in batched_enc:\n",
    "    batched_enc[split].set_format(type='torch', columns=['input_ids','attention_mask'])\n",
    "\n",
    "train_loader = DataLoader(batched_enc['train'], batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(batched_enc['validation'], batch_size=8)\n",
    "\n",
    "# Sanity: iterate one batch\n",
    "xb = next(iter(train_loader))\n",
    "xb['input_ids'].shape, xb['attention_mask'].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you prefer **numpy** only, call `set_format(type='numpy', columns=...)` and iterate normally.\n",
    "\n",
    "---\n",
    "\n",
    "## Part D ‚Äî Advanced: Map Pipelines, Filtering & Batching\n",
    "\n",
    "### D1. Length filtering before/after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 1528.26 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 173.70 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 185.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def too_short(ex):\n",
    "    return len(ex['completion'].split()) >= 5\n",
    "\n",
    "filtered = dds.filter(too_short)\n",
    "filtered = DatasetDict({k: v.shuffle(seed=7) for k,v in filtered.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2. Compose multiple maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 1117.25 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 171.15 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 182.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 1166.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 225.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 157.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "def strip_whitespace(batch):\n",
    "    return {\n",
    "        'prompt': [s.strip() for s in batch['prompt']],\n",
    "        'completion': [s.strip() for s in batch['completion']],\n",
    "        'metadata': batch['metadata']\n",
    "    }\n",
    "\n",
    "cleaned = dds.map(strip_whitespace, batched=True)\n",
    "cleaned.save_to_disk('artifacts/datasets/instruct_pc_cleaned')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D3. Dynamic padding collator (if using a framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: minimal dynamic pad at batch time (tokenizers path)\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "\n",
    "def collate_fn(examples: List[Dict]):\n",
    "    # Examples are already torch tensors after set_format(type='torch')\n",
    "    # Extract input_ids and attention_mask\n",
    "    input_ids_list = [e['input_ids'] for e in examples]\n",
    "    attention_mask_list = [e['attention_mask'] for e in examples]\n",
    "    \n",
    "    # Find max length in this batch\n",
    "    max_len = max(len(ids) for ids in input_ids_list)\n",
    "    pad_id = tok.token_to_id('<pad>')\n",
    "    \n",
    "    padded_ids = []\n",
    "    padded_masks = []\n",
    "    \n",
    "    for ids, mask in zip(input_ids_list, attention_mask_list):\n",
    "        # Convert tensor to list for manipulation\n",
    "        ids = ids.tolist() if torch.is_tensor(ids) else ids\n",
    "        mask = mask.tolist() if torch.is_tensor(mask) else mask\n",
    "        \n",
    "        # Pad if needed\n",
    "        if len(ids) < max_len:\n",
    "            ids = ids + [pad_id] * (max_len - len(ids))\n",
    "            mask = mask + [0] * (max_len - len(mask))\n",
    "        \n",
    "        padded_ids.append(ids)\n",
    "        padded_masks.append(mask)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.tensor(padded_ids, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(padded_masks, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "train_loader_dp = DataLoader(batched_enc['train'], batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "batch = next(iter(train_loader_dp))\n",
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Wrap‚ÄëUp\n",
    "\n",
    "Add a markdown cell and answer:\n",
    "\n",
    "1. Why prefer `datasets` over plain pandas for large JSONL corpora?  \n",
    "2. What seed did you use for deterministic splits & shuffles? Show how to reproduce the exact split on another machine.  \n",
    "3. Compare fixed‚Äëlength padding vs dynamic padding in your pipeline. Which will you choose for your training run and why?\n",
    "\n",
    "Confirm saved artifacts:\n",
    "\n",
    "- `artifacts/datasets/instruct_pc_splits/` (Arrow)  \n",
    "- `artifacts/datasets/instruct_pc_cleaned/` (optional)  \n",
    "- `artifacts/tokenizer/bytebpe.json` (tiny local tokenizer)  \n",
    "- `artifacts/samples/` (any debug exports you wrote)\n",
    "\n",
    "---\n",
    "\n",
    "- **Common pitfalls:** Mixing Trio and Prompt‚ÄëCompletion columns; forgetting to fix a random seed; applying tokenization unbatched (slow); holding arrays in Python lists instead of letting `datasets` manage Arrow memory.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution Snippets (reference)\n",
    "\n",
    "**Load from multiple JSONL files at once:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 195 examples [00:00, 19891.75 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'metadata'],\n",
       "        num_rows: 195\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "files = {\n",
    "  'train': 'artifacts/jsonl/instruct_prompt_completion.jsonl',\n",
    "  'validation': 'artifacts/jsonl/instruct_trio_from_rag.jsonl' # example\n",
    "}\n",
    "DatasetDict({k: load_dataset('json', data_files=v, split='train') for k,v in files.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save to shards (Arrow):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 1053.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dds['train'].save_to_disk('artifacts/datasets/pc_train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reload later:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion', 'metadata'],\n",
       "    num_rows: 8\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "loaded = load_from_disk('artifacts/datasets/pc_train')\n",
    "loaded\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
