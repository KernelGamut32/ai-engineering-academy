{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9155434a",
   "metadata": {},
   "source": [
    "# Lab 05 — Missing Data, Duplicates, & Type Normalization\n",
    "\n",
    "**Focus Area:** Turning messy raw data into consistent, joined datasets by managing missing data, duplicates, and type normalization — `dropna`, `fillna`, `astype`, `to_datetime`, `drop_duplicates`, `Series.apply`\n",
    "\n",
    "---\n",
    "\n",
    "## Outcomes\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Inspect and quantify missingness with `isna`, `info`, and `value_counts(dropna=False)`.\n",
    "2. Impute or remove nulls using `fillna`, `ffill/bfill`, group‑wise imputations, and `dropna` with `subset`.\n",
    "3. Normalize types: parse currency/percent strings to numerics (`str.replace` + `pd.to_numeric`/`astype`), and normalize dates with `pd.to_datetime(..., errors='coerce')` and (optionally) timezone to UTC.\n",
    "4. Identify and remove duplicates with `duplicated`/`drop_duplicates`, including composite keys and \"keep\" strategies.\n",
    "5. Apply `Series.apply` for targeted cleanups and know when to prefer vectorized ops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a9de6",
   "metadata": {},
   "source": [
    "## Setup - Generate Synthetic Messy Dataset\n",
    "\n",
    "We'll create a synthetic dataset with various data quality issues including missing values, duplicates, inconsistent formats, and type issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb76c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "# Generate base dataset\n",
    "n = 1500\n",
    "users = pd.DataFrame({\n",
    "    'user_id': np.arange(n),\n",
    "    'email': [f'user{i}@example.com' if rng.random() > 0.02 else None for i in range(n)],\n",
    "    'age': rng.integers(16, 80, size=n).astype('float'),  # will inject NaNs\n",
    "    'country': rng.choice(['US','U.S.A.','usa','SG','DE','BR','IN', None], size=n, p=[.25,.05,.05,.15,.15,.2,.1,.05]),\n",
    "    'signup_date': rng.choice(['2025-01-05','01/06/2025','06-01-2025','2025/01/07', None], size=n, p=[.25,.25,.25,.2,.05]),\n",
    "    'spend': rng.choice(['$12,345.60','$0.00','$99','1,234.50','€45,00','', None], size=n, p=[.15,.15,.25,.2,.15,.05,.05]),\n",
    "    'is_marketing_opt_in': rng.choice([True, False, None], size=n, p=[.45,.5,.05])\n",
    "})\n",
    "\n",
    "# Inject duplicates (same user_id appearing twice with slight diffs)\n",
    "dup_ids = rng.choice(users['user_id'], size=60, replace=False)\n",
    "users = pd.concat([users, users.loc[users['user_id'].isin(dup_ids)].assign(spend='$0.00')], ignore_index=True)\n",
    "\n",
    "print(f\"Dataset shape: {users.shape}\")\n",
    "users.sample(5, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72090988",
   "metadata": {},
   "source": [
    "## Part A — Inspect & Plan Missing‑Data Strategy\n",
    "\n",
    "### A1. Quick profile of missingness\n",
    "\n",
    "We'll inspect the dataset to understand the extent and distribution of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get overall info about the dataset\n",
    "users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f0891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the fraction of nulls per column\n",
    "null_fraction = users.isna().mean().sort_values(ascending=False).to_frame('null_frac')\n",
    "print(\"\\nNull Fraction by Column:\")\n",
    "print(null_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a20ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine country values including nulls\n",
    "print(\"\\nCountry value counts (including nulls):\")\n",
    "users['country'].value_counts(dropna=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfdeced",
   "metadata": {},
   "source": [
    "**Checkpoint Analysis:**\n",
    "- **Must be present:** `user_id` and `email` are critical business keys - cannot tolerate NaN\n",
    "- **Can impute:** `spend`, `age`, `is_marketing_opt_in` - can be estimated or given safe defaults\n",
    "- **Can normalize:** `country` has inconsistent formats (US, U.S.A., usa) - needs standardization\n",
    "- **Must parse:** `signup_date` has multiple date formats - needs normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb860c3",
   "metadata": {},
   "source": [
    "### A2. Drop rows only when necessary\n",
    "\n",
    "We'll use `dropna` with `subset` to only drop rows where critical fields are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac50ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Require a user_id and email; tolerate other nulls for now\n",
    "clean1 = users.dropna(subset=['user_id','email'])\n",
    "\n",
    "print(f\"Original rows: {len(users)}\")\n",
    "print(f\"After dropping missing user_id/email: {len(clean1)}\")\n",
    "print(f\"Rows dropped: {len(users) - len(clean1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a425c92d",
   "metadata": {},
   "source": [
    "## Part B — Imputation (`fillna`, `ffill/bfill`, groupwise) & Type Fixes\n",
    "\n",
    "### B1. Normalize country category with simple mapping + `fillna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e54463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping for country normalization\n",
    "country_map = {'U.S.A.':'USA','usa':'USA','US':'USA'}\n",
    "\n",
    "# Apply mapping and fill nulls\n",
    "clean1.loc[:, 'country_norm'] = clean1['country'].map(country_map).fillna(clean1['country']).fillna('UNKNOWN')\n",
    "\n",
    "print(\"Normalized country value counts:\")\n",
    "clean1['country_norm'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e320b",
   "metadata": {},
   "source": [
    "### B2. Currency parsing → numeric (`spend`)\n",
    "\n",
    "We'll use vectorized string operations to parse currency values, then apply group-wise median imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fe1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace common currency symbols/commas; handle European comma decimals\n",
    "s = clean1['spend'].astype('string')\n",
    "s1 = s.str.replace('[ $,]', '', regex=True)\n",
    "s1 = s1.str.replace('€', '', regex=False)\n",
    "\n",
    "# Convert comma-decimal like '45,00' -> '45.00' when there is one comma and no dot\n",
    "s1 = s1.where(~(s1.str.contains('^\\\\d+,\\\\d{1,2}$', regex=True)), s1.str.replace(',', '.', regex=False))\n",
    "\n",
    "# Convert to numeric\n",
    "clean1.loc[:, 'spend_usd'] = pd.to_numeric(s1, errors='coerce')\n",
    "\n",
    "print(\"Before imputation:\")\n",
    "print(clean1['spend_usd'].describe())\n",
    "print(f\"\\nNull values: {clean1['spend_usd'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing spend with group median by country\n",
    "med = clean1.groupby('country_norm')['spend_usd'].transform('median')\n",
    "clean1.loc[:, 'spend_usd'] = clean1['spend_usd'].fillna(med).fillna(0.0)\n",
    "\n",
    "print(\"After imputation:\")\n",
    "print(clean1['spend_usd'].describe())\n",
    "print(f\"\\nNull values: {clean1['spend_usd'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f5021",
   "metadata": {},
   "source": [
    "### B3. Dates to `datetime64[ns]` with parsing variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6253a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse signup_date with automatic format detection\n",
    "clean1.loc[:, 'signup_dt'] = pd.to_datetime(clean1['signup_date'], errors='coerce', dayfirst=False)\n",
    "\n",
    "print(\"Date parsing comparison:\")\n",
    "print(clean1[['signup_date','signup_dt']].head(8))\n",
    "print(f\"\\nNull dates: {clean1['signup_dt'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965677c",
   "metadata": {},
   "source": [
    "### B4. Coerce numeric/boolean types with `astype`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eed3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age may have NaN -> use pandas nullable types if you need to keep nulls\n",
    "clean1.loc[:, 'age'] = clean1['age'].astype('Float64')\n",
    "\n",
    "# Normalize boolean with fillna then astype\n",
    "clean1.loc[:, 'is_marketing_opt_in'] = clean1['is_marketing_opt_in'].fillna(False).astype('bool')\n",
    "\n",
    "print(\"Updated data types:\")\n",
    "print(clean1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7085fb89",
   "metadata": {},
   "source": [
    "## Part C — Duplicates & De‑duplication Strategies\n",
    "\n",
    "### C1. Detect duplicates by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a41d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A user should be unique by user_id (business key)\n",
    "dup_mask = clean1.duplicated(subset=['user_id'], keep=False)\n",
    "clean_dups = clean1.loc[dup_mask].sort_values(['user_id','signup_dt'])\n",
    "\n",
    "print(f\"Number of duplicate user_ids: {clean_dups['user_id'].nunique()}\")\n",
    "print(f\"Total duplicate rows: {len(clean_dups)}\")\n",
    "print(\"\\nSample of duplicates:\")\n",
    "clean_dups.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95924bc8",
   "metadata": {},
   "source": [
    "### C2. Resolve duplicates: pick the \"best\" record per key\n",
    "\n",
    "Policy: Prefer newest `signup_dt`, then higher `spend_usd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46958721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Prefer newest signup_dt, then higher spend\n",
    "resolved = (clean1\n",
    "            .sort_values(['user_id','signup_dt','spend_usd'], ascending=[True, False, False])\n",
    "            .drop_duplicates(subset=['user_id'], keep='first'))\n",
    "\n",
    "print(f\"Before deduplication: {len(clean1)} rows\")\n",
    "print(f\"After deduplication: {len(resolved)} rows\")\n",
    "print(f\"Duplicates removed: {len(clean1) - len(resolved)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c2605",
   "metadata": {},
   "source": [
    "### C3. Alternative: custom reducer via `groupby().agg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9048a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coalesce(series):\n",
    "    \"\"\"Return first non-null value\"\"\"\n",
    "    return series.dropna().iloc[0] if series.notna().any() else pd.NA\n",
    "\n",
    "best = (clean1\n",
    "        .sort_values(['signup_dt'], ascending=False)\n",
    "        .groupby('user_id')\n",
    "        .agg(email=('email','first'),\n",
    "             country_norm=('country_norm', 'first'),\n",
    "             signup_dt=('signup_dt','first'),\n",
    "             spend_usd=('spend_usd','max'),\n",
    "             age=('age', coalesce),\n",
    "             is_marketing_opt_in=('is_marketing_opt_in','max'))\n",
    "        .reset_index())\n",
    "\n",
    "print(f\"Rows after groupby aggregation: {len(best)}\")\n",
    "print(\"\\nSample of deduplicated data:\")\n",
    "best.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f456442e",
   "metadata": {},
   "source": [
    "## Part D — `Series.apply` vs Vectorized Ops\n",
    "\n",
    "### D1. When to use `apply`\n",
    "\n",
    "We'll compare the performance of `apply` vs vectorized operations for parsing currency values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_spend(x):\n",
    "    \"\"\"Parse currency value to float\"\"\"\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    s = s.replace('€','').replace(' ','')\n",
    "    if s.count(',') == 1 and s.count('.') == 0:\n",
    "        s = s.replace(',','.')\n",
    "    s = s.replace('$','').replace(',','')\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Test with apply\n",
    "print(\"Testing apply method:\")\n",
    "%timeit clean1['spend'].map(parse_spend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad67258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with vectorized operations\n",
    "print(\"Testing vectorized method:\")\n",
    "%timeit pd.to_numeric(clean1['spend'].astype('string').str.replace('[ $,]','',regex=True).str.replace('€','',regex=False), errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2afc61c",
   "metadata": {},
   "source": [
    "**Guideline:** Prefer vectorized operations for large frames; reserve `apply` for edge cases, then cache results.\n",
    "\n",
    "Vectorized operations are typically 10-100x faster than `apply` on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f571f",
   "metadata": {},
   "source": [
    "## Part E — Bonus (Optional) — Apply to orders artifacts\n",
    "\n",
    "### E1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd86ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load orders data from Lab 03 artifacts\n",
    "from pathlib import Path\n",
    "\n",
    "p_orders = Path('artifacts/parquet/orders')\n",
    "joined_path = Path('artifacts/parquet/orders_joined.parquet')\n",
    "\n",
    "orders = None\n",
    "\n",
    "if joined_path.exists():\n",
    "    orders = pd.read_parquet(joined_path)\n",
    "    print(f\"Loaded {len(orders)} rows from {joined_path}\")\n",
    "elif p_orders.exists():\n",
    "    files = sorted(p_orders.glob('shipcountry=*.parquet'))\n",
    "    if files:\n",
    "        orders = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)\n",
    "        print(f\"Loaded {len(orders)} rows from {len(files)} parquet files\")\n",
    "    else:\n",
    "        print(\"No parquet files found in orders directory\")\n",
    "else:\n",
    "    print(\"Orders data not available - skipping bonus section\")\n",
    "    print(\"Run Lab 03 first to generate the required artifacts\")\n",
    "\n",
    "if orders is not None:\n",
    "    print(\"\\nOrders data preview:\")\n",
    "    display(orders.head())\n",
    "    print(\"\\nData types:\")\n",
    "    print(orders.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a616a22",
   "metadata": {},
   "source": [
    "### E2. Missingness & types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3129de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if orders is not None:\n",
    "    print(\"Missing data analysis:\")\n",
    "    print(orders.isna().mean().sort_values(ascending=False).head(10))\n",
    "    \n",
    "    # Normalize date and numeric columns if they exist\n",
    "    if 'OrderDate' in orders.columns:\n",
    "        orders['OrderDate'] = pd.to_datetime(orders['OrderDate'], errors='coerce')\n",
    "        print(f\"\\nOrderDate converted to datetime. Nulls: {orders['OrderDate'].isna().sum()}\")\n",
    "    \n",
    "    if 'Freight' in orders.columns:\n",
    "        orders['Freight'] = pd.to_numeric(orders['Freight'], errors='coerce')\n",
    "        print(f\"Freight converted to numeric. Nulls: {orders['Freight'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831702c",
   "metadata": {},
   "source": [
    "### E3. Deduplicate line items (composite key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if orders is not None:\n",
    "    # If you have line items with duplicate OrderID/ProductID rows, keep the one with max UnitPrice*Quantity\n",
    "    if {'ProductID','Quantity','UnitPrice'}.issubset(orders.columns):\n",
    "        orders['ext_price'] = orders['Quantity'] * orders['UnitPrice']\n",
    "        dedup = (orders\n",
    "                 .sort_values(['OrderID','ProductID','ext_price'], ascending=[True,True,False])\n",
    "                 .drop_duplicates(subset=['OrderID','ProductID'], keep='first'))\n",
    "        print(f\"Deduplication by OrderID/ProductID composite key\")\n",
    "    else:\n",
    "        # Fallback: dedupe by OrderID only (newest date wins)\n",
    "        if 'OrderDate' in orders.columns:\n",
    "            dedup = (orders\n",
    "                     .sort_values(['OrderID','OrderDate'], ascending=[True,False])\n",
    "                     .drop_duplicates(subset=['OrderID'], keep='first'))\n",
    "            print(f\"Deduplication by OrderID only\")\n",
    "        else:\n",
    "            dedup = orders.drop_duplicates(subset=['OrderID'], keep='first')\n",
    "            print(f\"Simple deduplication by OrderID\")\n",
    "    \n",
    "    print(f\"\\nBefore deduplication: {len(orders)} rows\")\n",
    "    print(f\"After deduplication: {len(dedup)} rows\")\n",
    "    print(f\"Duplicates removed: {len(orders) - len(dedup)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69d1a8b",
   "metadata": {},
   "source": [
    "### E4. Export cleaned orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80110644",
   "metadata": {},
   "outputs": [],
   "source": [
    "if orders is not None:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    \n",
    "    out = Path('artifacts/clean')\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    output_path = out / 'orders_clean.parquet'\n",
    "    pq.write_table(pa.Table.from_pandas(dedup, preserve_index=False), output_path)\n",
    "    \n",
    "    print(f\"Cleaned orders saved to: {output_path}\")\n",
    "    print(f\"File size: {output_path.stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d677c54",
   "metadata": {},
   "source": [
    "## Part F — Wrap‑Up\n",
    "\n",
    "### Summary and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5902a92b",
   "metadata": {},
   "source": [
    "### Answers to Key Questions:\n",
    "\n",
    "#### 1. Which columns did you drop vs impute, and why?\n",
    "\n",
    "**Dropped:**\n",
    "- Rows with missing `user_id` or `email` were dropped because these are critical business keys needed for identification, deduplication, and joins. Without these, the row has no referential integrity.\n",
    "\n",
    "**Imputed:**\n",
    "- `spend_usd`: Used group-wise median by `country_norm`, then filled remaining nulls with 0.0. Rationale: spending patterns may vary by country, so group-wise imputation preserves geographic differences.\n",
    "- `age`: Kept as nullable Float64 to preserve the information that some ages were missing. Could alternatively impute with group median.\n",
    "- `is_marketing_opt_in`: Filled with `False` as the safe default (opt-out by default is safer for compliance).\n",
    "- `country`: Normalized inconsistent formats and filled nulls with 'UNKNOWN' to enable grouping and analysis.\n",
    "\n",
    "#### 2. What deduplication policy did you choose? How does it handle ties or null dates?\n",
    "\n",
    "**Policy:** We used two approaches:\n",
    "\n",
    "1. **Sort-based deduplication**: Sort by `user_id` (ascending), `signup_dt` (descending), and `spend_usd` (descending), then keep='first'. This prefers:\n",
    "   - Newest signup date\n",
    "   - If dates are tied or null, prefer highest spend\n",
    "   - Null dates will sort to the bottom (least preferred)\n",
    "\n",
    "2. **Aggregation-based**: Group by `user_id` and aggregate:\n",
    "   - `email`, `country_norm`, `signup_dt`: take first (after sorting by date desc)\n",
    "   - `spend_usd`: take maximum\n",
    "   - `age`: use custom coalesce to get first non-null\n",
    "   - `is_marketing_opt_in`: take max (True > False)\n",
    "\n",
    "**Handling ties/nulls:** The sort order is deterministic, so ties are resolved consistently. Null dates are treated as \"least preferred\" by pandas' default null-last sorting.\n",
    "\n",
    "#### 3. Show a before/after `dtypes` table and explain at least two `astype`/`to_datetime` choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a71e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dtypes before and after cleaning\n",
    "print(\"BEFORE CLEANING:\")\n",
    "print(users.dtypes)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"AFTER CLEANING:\")\n",
    "print(resolved.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0bec3",
   "metadata": {},
   "source": [
    "**Key type conversion choices:**\n",
    "\n",
    "1. **`pd.to_datetime(signup_date, errors='coerce')`:**\n",
    "   - Converts mixed date formats (YYYY-MM-DD, MM/DD/YYYY, DD-MM-YYYY) into consistent `datetime64[ns]`\n",
    "   - `errors='coerce'` ensures unparseable values become NaT instead of raising errors\n",
    "   - `infer_datetime_format=True` helps pandas auto-detect formats for better parsing\n",
    "   - Essential for time-based operations, sorting, and sequencing\n",
    "\n",
    "2. **`age.astype('Float64')`:**\n",
    "   - Uses pandas nullable Float64 type instead of numpy float64\n",
    "   - Allows explicit NaN values without confusion with special float values\n",
    "   - Better for data that should distinguish \"missing\" from \"zero\"\n",
    "   - Float64 (capital F) is a nullable type; float64 (lowercase) is not\n",
    "\n",
    "3. **`pd.to_numeric(spend_string, errors='coerce')`:**\n",
    "   - Converts currency strings to float after cleaning\n",
    "   - More robust than `astype(float)` which would fail on strings\n",
    "   - `errors='coerce'` turns unparseable values into NaN for safe imputation\n",
    "\n",
    "4. **`is_marketing_opt_in.fillna(False).astype('bool')`:**\n",
    "   - First fills nulls with safe default\n",
    "   - Then converts to strict boolean type (not nullable)\n",
    "   - Ensures downstream logic can rely on True/False without null checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76975a30",
   "metadata": {},
   "source": [
    "### Export Final Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd9cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "out = Path('artifacts/clean')\n",
    "out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Choose which cleaned dataset to export (resolved or best)\n",
    "final = resolved  # or `best` depending on policy preference\n",
    "\n",
    "# Write to parquet\n",
    "output_path = out / 'users_clean.parquet'\n",
    "pq.write_table(pa.Table.from_pandas(final, preserve_index=False), output_path)\n",
    "\n",
    "print(f\"Cleaned users data exported to: {output_path}\")\n",
    "print(f\"Final row count: {len(final)}\")\n",
    "print(f\"File size: {output_path.stat().st_size:,} bytes\")\n",
    "print(f\"\\nFinal data shape: {final.shape}\")\n",
    "print(f\"\\nSample of cleaned data:\")\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c626e8b3",
   "metadata": {},
   "source": [
    "### Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caed314",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA QUALITY METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original rows: {len(users)}\")\n",
    "print(f\"Rows after dropping missing keys: {len(clean1)}\")\n",
    "print(f\"Final rows after deduplication: {len(final)}\")\n",
    "print(f\"Total rows removed: {len(users) - len(final)} ({100*(len(users)-len(final))/len(users):.1f}%)\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print(\"\\nFINAL MISSING DATA:\")\n",
    "print(final.isna().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nDATA TYPE SUMMARY:\")\n",
    "print(final.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nSPEND STATISTICS:\")\n",
    "print(final['spend_usd'].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nCOUNTRY DISTRIBUTION:\")\n",
    "print(final['country_norm'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43fec11",
   "metadata": {},
   "source": [
    "### Common Pitfalls Avoided\n",
    "\n",
    "1. **Over-dropping with `dropna()`**: We used `subset=['user_id','email']` instead of dropping all rows with any nulls, preserving 95%+ of data.\n",
    "\n",
    "2. **`astype(float)` on non-numeric strings**: We used `pd.to_numeric(..., errors='coerce')` which safely handles unparseable values.\n",
    "\n",
    "3. **Inconsistent duplicate resolution**: We sorted by multiple columns before `drop_duplicates()` to ensure deterministic results.\n",
    "\n",
    "4. **Mixing object, string, and nullable dtypes**: We were explicit with types like `Float64` for nullable numerics and proper `bool` after imputation.\n",
    "\n",
    "5. **Ignoring group-wise patterns**: We used country-specific median imputation for spend rather than global median, preserving geographic patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219f174",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, we:\n",
    "\n",
    "✅ Inspected and quantified missing data across multiple dimensions  \n",
    "✅ Applied strategic imputation using group-wise statistics  \n",
    "✅ Normalized inconsistent data formats (currency, dates, categories)  \n",
    "✅ Converted data types appropriately using nullable types where needed  \n",
    "✅ Identified and removed duplicates using composite key strategies  \n",
    "✅ Compared `apply` vs vectorized operations for performance  \n",
    "✅ Exported clean, analysis-ready data in Parquet format  \n",
    "\n",
    "The cleaned dataset is now ready for downstream analytics, machine learning, or LLM pipeline integration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
