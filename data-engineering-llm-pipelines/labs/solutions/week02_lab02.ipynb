{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3084e3a7",
   "metadata": {},
   "source": [
    "# Lab 02 — Local API (`requests`) + SQL Extraction to pandas\n",
    "\n",
    "**Focus Areas:** HTTP APIs with `requests`, SQL → pandas (SQLite)\n",
    "\n",
    "---\n",
    "\n",
    "## Outcomes\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Call a **local REST API** with query parameters, parse JSON, and implement **robust error handling** for status codes.\n",
    "2. Implement **pagination** and an **exponential backoff** retry policy that respects `429 Too Many Requests` and `5xx` errors.\n",
    "3. Extract data from **SQLite** into pandas using **parameterized queries** and `pd.read_sql_query`, including **chunked reads**.\n",
    "4. Persist results to **Parquet** for downstream LLM preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a9056d",
   "metadata": {},
   "source": [
    "## Prerequisites & Setup\n",
    "\n",
    "- Python 3.13 with `requests`, `pandas`, `numpy`, `matplotlib`, `pyarrow` installed.\n",
    "- JupyterLab or VS Code with Jupyter extension.\n",
    "- **SQLite** available (via Python's built‑in `sqlite3`).\n",
    "- **Local API** served by **Datasette** exposing data from a SQLite database.\n",
    "\n",
    "### Setup Steps\n",
    "\n",
    "1. Create a project folder and environment\n",
    "2. Get a sample SQLite DB (Northwind)\n",
    "3. Run a **local REST API** with Datasette\n",
    "4. Start this notebook\n",
    "\n",
    "### 1) Create a project folder and environment\n",
    "\n",
    "```bash\n",
    "mkdir lab02 && cd lab02\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate  # Windows: .venv\\Scripts\\activate\n",
    "pip install --upgrade pip\n",
    "pip install requests pandas numpy matplotlib pyarrow datasette\n",
    "```\n",
    "\n",
    "### 2) Get a sample SQLite DB (Northwind)\n",
    "\n",
    "```bash\n",
    "curl -L -o northwind.db \\\n",
    "  https://raw.githubusercontent.com/jpwhite3/northwind-SQLite3/main/dist/northwind.db\n",
    "```\n",
    "\n",
    "### 3) Run a **local REST API** with Datasette (read‑only JSON over SQLite)\n",
    "\n",
    "```bash\n",
    "# Serves a browsable site *and* JSON endpoints\n",
    "# Terminal will print a local http://127.0.0.1:8001 URL\n",
    "# macOS/Linux:\n",
    "datasette northwind.db -h 127.0.0.1 -p 8001\n",
    "# Windows PowerShell (same command works)\n",
    "```\n",
    "\n",
    "Keep this server running. Open a second terminal for the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef4114f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part A — HTTP API with `requests`\n",
    "\n",
    "> You will call the Datasette JSON API. Endpoint pattern:  \n",
    "> `http://127.0.0.1:8001/<db>/<table>.json?_size=PAGE_SIZE&_next=...`  \n",
    "> We'll use `Orders` and `OrderDetails` tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc3d5f",
   "metadata": {},
   "source": [
    "### A1. Warm‑up: GET with params & `.json()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb15dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "BASE = \"http://127.0.0.1:8001\"\n",
    "DB = \"northwind\"\n",
    "TABLE = \"Orders\"\n",
    "PAGE_SIZE = 50\n",
    "\n",
    "params = {\"_size\": PAGE_SIZE}\n",
    "r = requests.get(f\"{BASE}/{DB}/{TABLE}.json\", params=params, timeout=10)\n",
    "print(r.status_code)\n",
    "data = r.json()  # raises if not JSON\n",
    "list(data.keys()), data.get(\"rows\", [])[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5c1c5",
   "metadata": {},
   "source": [
    "**Checkpoint:** Identify where rows live (Datasette returns `rows`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a990c",
   "metadata": {},
   "source": [
    "### A2. Robust request helper with **status handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f8f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import time\n",
    "\n",
    "class APIError(Exception):\n",
    "    pass\n",
    "\n",
    "def get_json(url: str, params: Dict[str, Any] | None = None, *, max_retries: int = 5) -> dict:\n",
    "    backoff = 0.5\n",
    "    for attempt in range(1, max_retries+1):\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, timeout=15)\n",
    "            status = resp.status_code\n",
    "            if status == 200:\n",
    "                return resp.json()\n",
    "            elif status in (429, 500, 502, 503, 504):\n",
    "                # exponential backoff\n",
    "                time.sleep(backoff)\n",
    "                backoff *= 2\n",
    "                continue\n",
    "            else:\n",
    "                raise APIError(f\"Unexpected status {status}: {resp.text[:200]}\")\n",
    "        except (requests.Timeout, requests.ConnectionError) as e:\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "    raise APIError(f\"Failed after {max_retries} attempts: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080ccff",
   "metadata": {},
   "source": [
    "**Simulating 429/5xx:** Stop the server briefly or change the port in `BASE` to provoke errors and observe retries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb22dc",
   "metadata": {},
   "source": [
    "### A3. **Pagination** (`_next` cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c60aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fetch_all(base: str, db: str, table: str, page_size: int = 100) -> pd.DataFrame:\n",
    "    url = f\"{base}/{db}/{table}.json\"\n",
    "    params = {\"_size\": page_size}\n",
    "    out = []\n",
    "    next_tok = None\n",
    "    while True:\n",
    "        if next_tok:\n",
    "            params[\"_next\"] = next_tok\n",
    "        payload = get_json(url, params)\n",
    "        rows = payload.get(\"rows\", [])\n",
    "        if not rows:\n",
    "            break\n",
    "        out.extend(rows)\n",
    "        next_tok = payload.get(\"next\")  # Datasette provides a cursor token\n",
    "        if not next_tok:\n",
    "            break\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "orders = fetch_all(BASE, DB, \"Orders\", page_size=200)\n",
    "orders.head(), len(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e7e380",
   "metadata": {},
   "source": [
    "### A4. Query filters via params\n",
    "\n",
    "Datasette supports simple filter syntax. Example: find orders with `ShipCountry = 'USA'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee9eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa = get_json(f\"{BASE}/{DB}/Orders.json\", params={\"ShipCountry\": \"USA\", \"_size\": 50})\n",
    "len(usa[\"rows\"]) , usa[\"rows\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a3770",
   "metadata": {},
   "source": [
    "**Checkpoint:** Note how query parameters map to column filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f902b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part B — SQL → pandas with SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af9569",
   "metadata": {},
   "source": [
    "### B1. Parameterized queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, pandas as pd\n",
    "# Update file path as needed\n",
    "conn = sqlite3.connect(\"lab02/northwind.db\")\n",
    "\n",
    "country = \"USA\"  # from user input in real apps\n",
    "q = \"\"\"\n",
    "SELECT OrderID, CustomerID, OrderDate, ShipCountry\n",
    "FROM Orders\n",
    "WHERE ShipCountry = ? AND OrderDate >= ?\n",
    "ORDER BY OrderDate DESC\n",
    "\"\"\"\n",
    "params = (country, \"1997-01-01\")\n",
    "\n",
    "safe_df = pd.read_sql_query(q, conn, params=params)\n",
    "safe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c30460e",
   "metadata": {},
   "source": [
    "> **Why `?` placeholders?** Prevents SQL injection—SQLite driver will safely bind values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea749a",
   "metadata": {},
   "source": [
    "### B2. Chunked reads for large tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cafc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_q = \"SELECT * FROM [Order Details]\"  # space requires brackets in SQLite\n",
    "chunks = pd.read_sql_query(big_q, conn, chunksize=10_000)\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Stream to Parquet in chunks\n",
    "writer = None\n",
    "for i, chunk in enumerate(chunks, start=1):\n",
    "    table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "    if writer is None:\n",
    "        # Update file path as needed\n",
    "        writer = pq.ParquetWriter(\"lab02/order_details.parquet\", table.schema)\n",
    "    writer.write_table(table)\n",
    "\n",
    "if writer is not None:\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5490a",
   "metadata": {},
   "source": [
    "**Checkpoint:** Verify output file size and row count by re‑reading with pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6a43a",
   "metadata": {},
   "source": [
    "### B3. Quick validation snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf2352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Update file path as needed\n",
    "p = pd.read_parquet(\"lab02/order_details.parquet\")\n",
    "print(len(p))\n",
    "print(p.select_dtypes(include='number').describe().T.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5963ca5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part C — Wrap‑Up\n",
    "\n",
    "Answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd454c",
   "metadata": {},
   "source": [
    "### Question 1: How does your retry/backoff behave for 429 vs 500?\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "Both 429 (Too Many Requests) and 500 (Internal Server Error) trigger the same exponential backoff behavior in our `get_json` function. The backoff starts at 0.5 seconds and doubles with each retry attempt (0.5s → 1s → 2s → 4s → 8s) up to a maximum of 5 attempts. This gives the server time to recover while avoiding overwhelming it with immediate retries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7766a80a",
   "metadata": {},
   "source": [
    "### Question 2: Why are **parameterized** queries the default choice? Provide a one‑sentence example of a potential injection if not parameterized.\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "Parameterized queries prevent SQL injection attacks by safely binding user input values instead of concatenating them directly into SQL strings. For example, without parameterization, a malicious user could input `\"'; DROP TABLE Orders; --\"` as a country name, which would delete the entire Orders table if directly concatenated into the SQL query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec24a5f",
   "metadata": {},
   "source": [
    "### Question 3: When would you choose chunked reads? What trade‑off do you incur?\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "Chunked reads are ideal when working with large tables that won't fit into memory all at once, or when processing data incrementally (e.g., streaming to Parquet or performing transformations). The trade-off is increased complexity in code and potentially slower overall processing due to multiple database round-trips, compared to a single bulk read that loads everything into memory at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5051ca8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "- **Datasette tips:** JSON is at `.../table.json` (use `_size`, `_next`, and column filters)\n",
    "- **Simulating errors:** Stop server to trigger connection errors; add a bogus param to force 4xx; explore retry logs.\n",
    "- **Common pitfalls:**\n",
    "  - Missing `timeout` in `requests` → hanging cells.\n",
    "  - Forgetting `params` vs string concatenation in URLs.\n",
    "  - Using f‑strings to inject SQL instead of `params=...`.\n",
    "\n",
    "**Artifacts to retain:** `orders.parquet`, `order_details.parquet`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f764b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix — Solution Snippets Reference\n",
    "\n",
    "**Backoff conditions you might retry:** `429, 500, 502, 503, 504` (idempotent GETs). Use jitter in production to avoid thundering herds.\n",
    "\n",
    "**Cursor pagination recap (Datasette):** Examine `payload[\"next\"]` and pass it back as `_next` with the same `_size` to retrieve subsequent pages.\n",
    "\n",
    "**`read_sql_query` params:** Use `?` for SQLite, `%s` for Postgres, and pass values via `params=...` so the DB‑API binds them safely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
