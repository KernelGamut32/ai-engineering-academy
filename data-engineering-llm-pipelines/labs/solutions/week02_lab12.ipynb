{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86dbfe9a",
   "metadata": {},
   "source": [
    "# Lab 12 â€” Author a Schema + Profiling Report\n",
    "\n",
    "**Focus Areas:** Author a schema (Pandera) + Profiling report (ydataâ€‘profiling)\n",
    "\n",
    "> This capstoneâ€‘style lab combines a **productionâ€‘ish Pandera schema** for your cleaned & joined data with a **focused profiling report**. You'll author constraints (types/ranges/enums), add crossâ€‘column/DF checks, generate an HTML profile, and produce a prioritized **risk list** with mitigationsâ€”all wired for CI.\n",
    "\n",
    "---\n",
    "\n",
    "## Outcomes\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Author a **typed Pandera schema** with column checks (regex, enums) and crossâ€‘column/DFâ€‘level checks; validate **clean** vs **broken** frames with actionable messages.  \n",
    "2. Implement **schema versioning** and light **evolution** (e.g., allow a new category via a controlled update).  \n",
    "3. Generate a **ydataâ€‘profiling** HTML report for a column subset at realistic scale; interpret key sections and extract **machineâ€‘readable metrics**.  \n",
    "4. Produce a **Topâ€‘5 risks** table (with severity & mitigation) and persist artifacts (HTML + JSON + CSV) for review/CI.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites & Setup\n",
    "\n",
    "- Python 3.13 with `pandas`, `numpy`, `pandera>=0.20`, `pydantic>=2.0` (optional), `ydata-profiling`, `pyarrow`.  \n",
    "- JupyterLab or VS Code with Jupyter extension.\n",
    "- Preferred artifacts from previous labs:  \n",
    "  - `artifacts/clean/per_customer_enriched.parquet`  \n",
    "  - `artifacts/clean/per_segment.parquet`  \n",
    "  - If missing, run the synthetic fallback below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0176869",
   "metadata": {},
   "source": [
    "## Directory Setup\n",
    "\n",
    "Create the necessary directories for reports and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0911b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path('artifacts/reports').mkdir(parents=True, exist_ok=True)\n",
    "Path('artifacts/metrics').mkdir(parents=True, exist_ok=True)\n",
    "print(\"Directories created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dde753b",
   "metadata": {},
   "source": [
    "## Load or Synthesize Data\n",
    "\n",
    "Load the per-customer enriched data or create synthetic data if not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7de2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    per_cust_enriched = pd.read_parquet('artifacts/clean/per_customer_enriched.parquet')\n",
    "    print(f\"Loaded {len(per_cust_enriched)} rows from parquet file\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load file: {e}\")\n",
    "    print(\"Generating synthetic fallback data...\")\n",
    "    # Synthetic fallback\n",
    "    rng = np.random.default_rng(3)\n",
    "    N = 60_000\n",
    "    per_cust_enriched = pd.DataFrame({\n",
    "        'CustomerID': [f'C{i:05d}' for i in range(N)],\n",
    "        'country_norm': rng.choice(['USA','DE','SG','BR'], size=N, p=[.58,.18,.16,.08]),\n",
    "        'n_orders': rng.poisson(3, size=N),\n",
    "        'freight_sum': np.round(np.clip(rng.lognormal(3.1, 0.8, N), 0, 2e5), 2),\n",
    "        'freight_mean': np.round(np.clip(rng.lognormal(2.5, 0.6, N), 0, 1e4), 2),\n",
    "        'signup_dt': pd.Timestamp('2025-01-01') + pd.to_timedelta(rng.integers(0, 40, N), unit='D'),\n",
    "        'email': [f'user{i}@example.com' for i in range(N)],\n",
    "        'is_adult': rng.random(N) > 0.1,\n",
    "        'is_high_value': rng.random(N) > 0.9,\n",
    "    })\n",
    "    print(f\"Generated {len(per_cust_enriched)} synthetic rows\")\n",
    "\n",
    "per_cust_enriched.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d88b0e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part A â€” Author a Productionâ€‘ish Pandera Schema\n",
    "\n",
    "### A1. Column types & checks (regex, enums, ranges)\n",
    "\n",
    "Create a comprehensive Pandera schema with type annotations and validation checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab114623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera as pa\n",
    "from pandera import Column, Check\n",
    "\n",
    "SCHEMA_VERSION = '1.0.0'\n",
    "AllowedCountries = ['USA','DE','SG','BR']\n",
    "\n",
    "PerCustomerSchema = pa.DataFrameSchema({\n",
    "    'CustomerID': Column(object, nullable=False, checks=Check.str_matches(r'^C\\d{5}$', error='bad_id')),\n",
    "    'country_norm': Column(object, nullable=False, checks=Check.isin(AllowedCountries)),\n",
    "    'n_orders': Column(pa.Int64, nullable=False, checks=Check.ge(0)),\n",
    "    'freight_sum': Column(float, nullable=False, checks=Check.ge(0)),\n",
    "    'freight_mean': Column(float, nullable=False, checks=Check.ge(0)),\n",
    "    'signup_dt': Column(object, nullable=False),\n",
    "    'email': Column(object, nullable=False, checks=Check.str_matches(r'^.+@.+\\..+$')),\n",
    "    'is_adult': Column(bool, nullable=False),\n",
    "    'is_high_value': Column(bool, nullable=False),\n",
    "},\n",
    "    name=f'PerCustomerSchema_v{SCHEMA_VERSION}', strict=True\n",
    ")\n",
    "\n",
    "print(f\"Schema created: {PerCustomerSchema.name}\")\n",
    "print(f\"Columns: {list(PerCustomerSchema.columns.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eae28a",
   "metadata": {},
   "source": [
    "### A2. Crossâ€‘column rules & DFâ€‘level checks\n",
    "\n",
    "Add DataFrame-level and cross-column validation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d914093",
   "metadata": {},
   "outputs": [],
   "source": [
    "PerCustomerSchema = PerCustomerSchema.add_checks([\n",
    "    # DF-level: mean of freight_mean should be <= mean of freight_sum (approx; demo check)\n",
    "    pa.Check(lambda df: df['freight_mean'].mean() <= max(df['freight_sum'].mean(), 1.0), element_wise=False,\n",
    "             error='freight_mean too large vs freight_sum'),\n",
    "    # DF-level: IDs must be unique\n",
    "    pa.Check(lambda df: df['CustomerID'].is_unique, element_wise=False, error='duplicate CustomerID')\n",
    "])\n",
    "\n",
    "# Row-level cross-field: freight_sum >= freight_mean when n_orders >= 1\n",
    "PerCustomerSchema = PerCustomerSchema.update_checks({\n",
    "    'freight_sum': [Check(lambda s, df: s >= df['freight_mean'], element_wise=True,\n",
    "                          error='sum must be >= mean')]\n",
    "})\n",
    "\n",
    "print(\"Cross-column and DF-level checks added successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a845a99",
   "metadata": {},
   "source": [
    "### A3. Validate clean â†’ then exercise failures with a \"broken\" sample\n",
    "\n",
    "First validate the clean data, then create a broken sample to demonstrate error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ddcf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean should pass\n",
    "ok = PerCustomerSchema.validate(per_cust_enriched, lazy=True)\n",
    "print(f'âœ“ Clean validation passed: {len(ok)} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a small broken copy to see errors\n",
    "broken = per_cust_enriched.copy().iloc[:500].copy()\n",
    "broken.loc[0, 'CustomerID'] = 'BADID'\n",
    "broken.loc[1, 'country_norm'] = 'U.S.A.'\n",
    "broken.loc[2, 'freight_sum'] = -10\n",
    "broken.loc[3, 'email'] = 'nope'\n",
    "\n",
    "try:\n",
    "    PerCustomerSchema.validate(broken, lazy=True)\n",
    "except pa.errors.SchemaErrors as err:\n",
    "    fc = err.failure_cases\n",
    "    rollup = (fc.groupby(['column','check']).size().reset_index(name='n').sort_values('n', ascending=False))\n",
    "    print(\"\\nValidation failures summary:\")\n",
    "    display(rollup.head(10))\n",
    "    print(\"\\nFirst few failure cases:\")\n",
    "    display(fc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db1eb2e",
   "metadata": {},
   "source": [
    "**Checkpoint:** Paste two violations from `rollup` and explain how each protects downstream LLM steps.\n",
    "\n",
    "**Example violations:**\n",
    "\n",
    "1. **bad_id (CustomerID)**: Ensures CustomerID follows the pattern `C\\d{5}`. This protects downstream LLM steps by maintaining consistent ID format that can be reliably parsed and referenced in generated text or queries.\n",
    "\n",
    "2. **isin (country_norm)**: Validates that country codes are within allowed values. This prevents LLM from generating invalid geographic references and ensures consistent handling of location-based logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de0a3af",
   "metadata": {},
   "source": [
    "### A4. Schema evolution: allow a new country (controlled)\n",
    "\n",
    "Demonstrate controlled schema versioning to allow new business requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a business-accepted new category\n",
    "AllowedCountries_v2 = AllowedCountries + ['SE']\n",
    "PerCustomerSchema_v2 = PerCustomerSchema.update_column_checks('country_norm', checks=Check.isin(AllowedCountries_v2))\n",
    "SCHEMA_VERSION = '1.1.0'\n",
    "PerCustomerSchema_v2.name = f'PerCustomerSchema_v{SCHEMA_VERSION}'\n",
    "\n",
    "print(f\"Schema evolved to: {PerCustomerSchema_v2.name}\")\n",
    "print(f\"Allowed countries: {AllowedCountries_v2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad1c30",
   "metadata": {},
   "source": [
    "**Note:** Commit schema files with a version tag; add a CHANGELOG entry for policy changes.\n",
    "\n",
    "---\n",
    "\n",
    "## Part B â€” Focused Profiling Report & Metric Extraction\n",
    "\n",
    "### B1. Create a tuned profile (subset + minimal heavy bits)\n",
    "\n",
    "Generate a comprehensive profiling report for key columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5988b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "cols = ['country_norm','n_orders','freight_sum','freight_mean','is_high_value']\n",
    "subset = per_cust_enriched[cols].sample(40_000, random_state=7) if len(per_cust_enriched) > 40_000 else per_cust_enriched[cols]\n",
    "\n",
    "print(f\"Creating profile for {len(subset)} rows with columns: {cols}\")\n",
    "\n",
    "profile = ProfileReport(\n",
    "    subset,\n",
    "    title='Per-Customer Enriched â€” Focused Profile',\n",
    "    minimal=False, explorative=True,\n",
    "    correlations={'pearson': {'calculate': True}, 'spearman': {'calculate': True}},\n",
    "    progress_bar=True\n",
    ")\n",
    "profile_path = 'artifacts/reports/per_customer_profile.html'\n",
    "profile.to_file(profile_path)\n",
    "print(f\"\\nâœ“ Profile saved to: {profile_path}\")\n",
    "profile_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38024fe9",
   "metadata": {},
   "source": [
    "### B2. Interpret: variables, alerts, correlations\n",
    "\n",
    "- **Variables:** note **skew/outliers** in `freight_sum`; check **distinct counts** for `country_norm`.  \n",
    "- **Alerts:** capture high cardinality or extreme zeros distribution.  \n",
    "- **Correlations:** expect positive `n_orders` â†” `freight_sum`; sanityâ€‘check magnitude.\n",
    "\n",
    "**Checkpoint:** Record one expected correlation and one surprising alert.\n",
    "\n",
    "**Expected correlation:** `n_orders` and `freight_sum` should show positive correlation (more orders = higher total freight).\n",
    "\n",
    "**Surprising alert:** High skewness in `freight_sum` may indicate need for log transformation or outlier treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4319931",
   "metadata": {},
   "source": [
    "### B3. Extract metrics JSON for drift tracking\n",
    "\n",
    "Extract key metrics from the profile for baseline tracking in CI/CD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "summary = profile.to_dict()\n",
    "metrics = {\n",
    "    'n_rows': summary['table']['n'],\n",
    "    'freight_sum_mean': summary['variables']['freight_sum']['mean'],\n",
    "    'freight_sum_std': summary['variables']['freight_sum']['std'],\n",
    "    'n_orders_mean': summary['variables']['n_orders']['mean'],\n",
    "    'n_orders_distinct': summary['variables']['n_orders']['distinct_count'],\n",
    "    'country_cardinality': summary['variables']['country_norm']['distinct_count'],\n",
    "}\n",
    "\n",
    "metrics_path = 'artifacts/metrics/per_customer_profile_metrics.json'\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Metrics saved to: {metrics_path}\")\n",
    "print(\"\\nExtracted metrics:\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62082e27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part C â€” Topâ€‘5 Data Risks & Mitigations\n",
    "\n",
    "### C1. Pull alerts table from profile dict\n",
    "\n",
    "Identify top data quality risks from variable statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39546db",
   "metadata": {},
   "outputs": [],
   "source": [
    "alerts = summary.get('alerts', [])  # ydata-profiling structures alerts differently by version\n",
    "# Build a generic \"risks\" list from variable summaries\n",
    "risks = []\n",
    "for col in ['freight_sum','freight_mean','n_orders','country_norm']:\n",
    "    v = summary['variables'][col]\n",
    "    if v.get('p_zeros', 0) > 0.5:\n",
    "        risks.append((col, 'high zeros fraction', v['p_zeros']))\n",
    "    if v.get('skewness', 0) > 2:\n",
    "        risks.append((col, 'high skewness', v['skewness']))\n",
    "    if v.get('distinct_count', 0) > 0.8 * metrics['n_rows']:\n",
    "        risks.append((col, 'high cardinality', v['distinct_count']))\n",
    "\n",
    "# Prioritize and pick top 5\n",
    "risks_sorted = sorted(risks, key=lambda x: float(x[2]), reverse=True)[:5]\n",
    "print(\"Top 5 data risks identified:\")\n",
    "for i, (col, risk, val) in enumerate(risks_sorted, 1):\n",
    "    print(f\"{i}. {col}: {risk} = {val:.4f}\")\n",
    "risks_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0388285",
   "metadata": {},
   "source": [
    "### C2. Document mitigations (template)\n",
    "\n",
    "Create a structured table of risks with proposed mitigations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1308a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigations = pd.DataFrame([\n",
    "    {'column': c, 'risk': r, 'value': float(val),\n",
    "     'mitigation': 'Log-transform; winsorize 99th pct; monitor via metric drift'}\n",
    "    for c, r, val in risks_sorted\n",
    "])\n",
    "\n",
    "# Customize mitigations based on risk type\n",
    "for idx, row in mitigations.iterrows():\n",
    "    if row['risk'] == 'high cardinality':\n",
    "        mitigations.at[idx, 'mitigation'] = 'Use as join key only; avoid as categorical feature; consider embeddings'\n",
    "    elif row['risk'] == 'high zeros fraction':\n",
    "        mitigations.at[idx, 'mitigation'] = 'Split zero/non-zero populations; engineer indicator features; validate business logic'\n",
    "    elif row['risk'] == 'high skewness':\n",
    "        mitigations.at[idx, 'mitigation'] = 'Log-transform for modeling; winsorize 99th percentile; monitor distribution shifts'\n",
    "\n",
    "display(mitigations)\n",
    "\n",
    "risks_path = 'artifacts/reports/top5_risks.csv'\n",
    "mitigations.to_csv(risks_path, index=False)\n",
    "print(f\"\\nâœ“ Wrote {risks_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e8d69f",
   "metadata": {},
   "source": [
    "**Examples of mitigations:**\n",
    "\n",
    "- **High skew/outliers:** logâ€‘transform features; cap at high quantiles; monitor tails.  \n",
    "- **High cardinality IDs:** avoid as categorical features; use as join keys only.  \n",
    "- **Zeros inflation:** split populations (zero vs nonâ€‘zero) or engineer indicator features.  \n",
    "- **Category drift:** expand schema allowâ€‘list **via versioned change** + DQ alert.\n",
    "\n",
    "---\n",
    "\n",
    "## Part D â€” Integrate: CI Checks & Artifacts\n",
    "\n",
    "### D1. Gate with schema + persist artifacts\n",
    "\n",
    "Validate the data against schema and prepare for CI integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d554d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _ = PerCustomerSchema.validate(per_cust_enriched, lazy=True)\n",
    "    status = 'OK'\n",
    "    print(f\"âœ“ Schema validation: {status}\")\n",
    "except pa.errors.SchemaErrors as err:\n",
    "    status = 'FAIL'\n",
    "    failure_path = 'artifacts/reports/schema_failures.csv'\n",
    "    err.failure_cases.to_csv(failure_path, index=False)\n",
    "    print(f\"âœ— Schema validation: {status}\")\n",
    "    print(f\"  Failures saved to: {failure_path}\")\n",
    "\n",
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda773df",
   "metadata": {},
   "source": [
    "### D2. Minimal CI rule (concept)\n",
    "\n",
    "- Always attach `per_customer_profile.html`, `per_customer_profile_metrics.json`, and `top5_risks.csv` to PRs.  \n",
    "- Fail PR if schema **FAIL** or if key metrics change > **30%** from baseline without a waiver.\n",
    "\n",
    "**Example CI check:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate CI metric drift check\n",
    "baseline_metrics = {\n",
    "    'freight_sum_mean': 30.0,\n",
    "    'n_orders_mean': 3.0,\n",
    "}\n",
    "\n",
    "threshold = 0.30  # 30% drift threshold\n",
    "drift_detected = False\n",
    "\n",
    "print(\"CI Drift Check:\")\n",
    "print(f\"Threshold: {threshold*100}%\\n\")\n",
    "\n",
    "for metric_name, baseline_value in baseline_metrics.items():\n",
    "    current_value = metrics.get(metric_name, 0)\n",
    "    if baseline_value > 0:\n",
    "        drift = abs(current_value - baseline_value) / baseline_value\n",
    "        status_icon = \"âœ—\" if drift > threshold else \"âœ“\"\n",
    "        print(f\"{status_icon} {metric_name}: baseline={baseline_value:.2f}, current={current_value:.2f}, drift={drift*100:.1f}%\")\n",
    "        if drift > threshold:\n",
    "            drift_detected = True\n",
    "\n",
    "print(f\"\\nCI Check: {'FAIL - Drift exceeds threshold' if drift_detected else 'PASS'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3157fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part E â€” Wrapâ€‘Up\n",
    "\n",
    "Add a markdown cell and answer:\n",
    "\n",
    "1. Paste a **schema version** and one change you'd record in a CHANGELOG.  \n",
    "2. List your **Topâ€‘5 risks** and the mitigation you selected for each.  \n",
    "3. Show the two metrics you'll track in CI and the thresholds you chose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5be038",
   "metadata": {},
   "source": [
    "### Final Summary\n",
    "\n",
    "#### 1. Schema Version & CHANGELOG\n",
    "\n",
    "**Schema Version:** v1.1.0\n",
    "\n",
    "**CHANGELOG Entry:**\n",
    "```\n",
    "## [1.1.0] - 2025-01-27\n",
    "### Added\n",
    "- Added 'SE' (Sweden) to allowed country codes in country_norm field\n",
    "- Supports new European market expansion initiative\n",
    "```\n",
    "\n",
    "#### 2. Top-5 Risks & Mitigations\n",
    "\n",
    "| Column | Risk | Mitigation |\n",
    "|--------|------|------------|\n",
    "| freight_sum | High skewness | Log-transform for modeling; winsorize 99th percentile; monitor distribution shifts |\n",
    "| freight_mean | High skewness | Log-transform for modeling; winsorize 99th percentile; monitor distribution shifts |\n",
    "| n_orders | Potential zeros | Split zero/non-zero populations; engineer indicator features; validate business logic |\n",
    "| country_norm | Limited cardinality | Monitor for new countries; use schema versioning for expansions |\n",
    "| CustomerID | Uniqueness critical | Enforce strict regex pattern; maintain unique constraint; fail fast on violations |\n",
    "\n",
    "#### 3. CI Metrics & Thresholds\n",
    "\n",
    "**Tracked Metrics:**\n",
    "\n",
    "1. **freight_sum_mean**\n",
    "   - Baseline: 30.0\n",
    "   - Threshold: Â±30% drift\n",
    "   - Action: Alert on drift > 30%; fail PR on drift > 50%\n",
    "\n",
    "2. **n_orders_mean**\n",
    "   - Baseline: 3.0\n",
    "   - Threshold: Â±30% drift  \n",
    "   - Action: Alert on drift > 30%; fail PR on drift > 50%\n",
    "\n",
    "**Additional Gates:**\n",
    "- Schema validation must return `OK` status\n",
    "- Top 5 risks must be reviewed and mitigations documented\n",
    "- All artifacts (HTML, JSON, CSV) must be generated and attached to PR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59cdfbe",
   "metadata": {},
   "source": [
    "### Artifacts Generated\n",
    "\n",
    "Summary of all artifacts created in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "artifacts = [\n",
    "    'artifacts/reports/per_customer_profile.html',\n",
    "    'artifacts/metrics/per_customer_profile_metrics.json',\n",
    "    'artifacts/reports/top5_risks.csv',\n",
    "]\n",
    "\n",
    "print(\"Generated Artifacts:\")\n",
    "print(\"=\" * 60)\n",
    "for artifact in artifacts:\n",
    "    p = Path(artifact)\n",
    "    exists = \"âœ“\" if p.exists() else \"âœ—\"\n",
    "    size = f\"{p.stat().st_size:,} bytes\" if p.exists() else \"N/A\"\n",
    "    print(f\"{exists} {artifact}\")\n",
    "    print(f\"  Size: {size}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9c1df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "- **Overâ€‘strict schemas:** Block expected evolution; use versioning for controlled changes\n",
    "- **Not sampling profiles:** Full dataset profiling can be slow; sample intelligently\n",
    "- **Forgetting `strict=True`:** Allows unexpected columns to pass validation\n",
    "- **Missing uniqueness checks:** Critical for ID fields and primary keys\n",
    "- **Ignoring cross-column constraints:** Real-world data has implicit relationships\n",
    "- **Not persisting baselines:** Drift detection requires historical metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Solution Snippets (reference)\n",
    "\n",
    "**Update a single column's checks (schema evolution):**\n",
    "\n",
    "```python\n",
    "PerCustomerSchema_v2 = PerCustomerSchema.update_column_checks(\n",
    "    'country_norm', \n",
    "    checks=Check.isin(['USA','DE','SG','BR','SE'])\n",
    ")\n",
    "```\n",
    "\n",
    "**Compact rollâ€‘up of schema failures:**\n",
    "\n",
    "```python\n",
    "try:\n",
    "    PerCustomerSchema.validate(df, lazy=True)\n",
    "except pa.errors.SchemaErrors as err:\n",
    "    roll = (err.failure_cases\n",
    "              .groupby(['column','check'])\n",
    "              .size()\n",
    "              .reset_index(name='n')\n",
    "              .sort_values('n', ascending=False))\n",
    "    print(roll.head(10))\n",
    "```\n",
    "\n",
    "**Pick top risks with a simple heuristic:**\n",
    "\n",
    "```python\n",
    "summary = profile.to_dict()\n",
    "risks = [\n",
    "    (c, 'skew', summary['variables'][c]['skewness']) \n",
    "    for c in ['freight_sum','freight_mean'] \n",
    "    if summary['variables'][c]['skewness'] > 2\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007e764",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab Complete! ðŸŽ‰\n",
    "\n",
    "You have successfully:\n",
    "\n",
    "- âœ“ Created a production-ready Pandera schema with comprehensive validation rules\n",
    "- âœ“ Implemented schema versioning and controlled evolution\n",
    "- âœ“ Generated a focused profiling report with ydata-profiling\n",
    "- âœ“ Extracted machine-readable metrics for CI/CD drift detection\n",
    "- âœ“ Identified and documented top data quality risks with mitigations\n",
    "- âœ“ Prepared all artifacts for integration into a CI/CD pipeline\n",
    "\n",
    "These techniques form the foundation of production data quality monitoring and validation in LLM pipelines."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
