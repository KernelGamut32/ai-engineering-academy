{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3333f4a1",
   "metadata": {},
   "source": [
    "# Lab 14 — Instruction Tuning Schemas\n",
    "\n",
    "**Focus Area:** Common instruction‑tuning schemas; templating; line‑wise JSONL generation; validation; dataset hygiene (dedupe, filtering, length control)\n",
    "\n",
    "This notebook implements the complete solution for Lab 14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ccc327",
   "metadata": {},
   "source": [
    "## Setup - Create Required Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411650ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created successfully\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "for p in ['artifacts/jsonl','artifacts/samples','artifacts/stats']:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Directories created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e1c7f",
   "metadata": {},
   "source": [
    "## Part A — Schemas & Templates\n",
    "\n",
    "### Understanding Two Common Record Layouts\n",
    "\n",
    "**Schema 1: Trio** *(popular in Flan / Alpaca style)*\n",
    "```json\n",
    "{\"instruction\": \"...\", \"input\": \"...\", \"output\": \"...\", \"metadata\": {\"source\": \"...\"}}\n",
    "```\n",
    "*Pros:* separates task from context; easy to mix with or without `input`  \n",
    "*Cons:* needs templating at train time; slightly larger records\n",
    "\n",
    "**Schema 2: Prompt‑Completion** *(OpenAI fine‑tuning style)*\n",
    "```json\n",
    "{\"prompt\": \"...\", \"completion\": \"...\", \"metadata\": {\"source\": \"...\"}}\n",
    "```\n",
    "*Pros:* simple; direct fit to APIs expecting prompt/completion  \n",
    "*Cons:* prompt formatting is baked in; harder to change templates later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e17d907",
   "metadata": {},
   "source": [
    "## Part B — Build a Trio Dataset from Lab 13 SFT View\n",
    "\n",
    "### B1. Loader + light cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77026bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Trio dataset: artifacts/jsonl/instruct_trio.jsonl\n",
      "Records kept: 63\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import orjson\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "SRC = Path('artifacts/jsonl/corpus_sft.jsonl')\n",
    "TRIO = Path('artifacts/jsonl/instruct_trio.jsonl')\n",
    "\n",
    "# Check if source file exists\n",
    "if not SRC.exists():\n",
    "    print(f\"WARNING: Source file {SRC} not found!\")\n",
    "    print(\"This lab requires artifacts from Lab 13.\")\n",
    "    print(\"Please run Lab 13 first to generate the required files.\")\n",
    "else:\n",
    "    lang_allow = {\"en\"}\n",
    "    min_out_chars = 20\n",
    "\n",
    "    with SRC.open('r', encoding='utf-8') as fin, TRIO.open('w', encoding='utf-8') as fout:\n",
    "        kept = 0\n",
    "        seen = set()\n",
    "        for line in fin:\n",
    "            obj = json.loads(line)\n",
    "            # Expected keys in Lab 13: input, output, metadata{doc_id,type,lang}\n",
    "            lang = (obj.get('metadata') or {}).get('lang', 'en')\n",
    "            if lang not in lang_allow:\n",
    "                continue\n",
    "            instruction = obj['input']\n",
    "            inp = \"\"  # Lab 13 'input' already contains the task; keep Trio input empty here\n",
    "            out = re.sub(r\"\\s+\", \" \", obj['output']).strip()\n",
    "            if len(out) < min_out_chars:\n",
    "                continue\n",
    "            # Deduplicate by (instruction, out)\n",
    "            key = (instruction, out)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            rec = {\n",
    "                'instruction': instruction,\n",
    "                'input': inp,\n",
    "                'output': out,\n",
    "                'metadata': {\n",
    "                    'doc_id': (obj.get('metadata') or {}).get('doc_id'),\n",
    "                    'schema_version': 'trio-v1'\n",
    "                }\n",
    "            }\n",
    "            fout.write(orjson.dumps(rec).decode() + '\\n')\n",
    "            kept += 1\n",
    "\n",
    "    print(f\"Created Trio dataset: {TRIO}\")\n",
    "    print(f\"Records kept: {kept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c97be82",
   "metadata": {},
   "source": [
    "### B2. Add a templated prompt‑completion view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ffa8af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Prompt-Completion dataset: artifacts/jsonl/instruct_prompt_completion.jsonl\n",
      "Records converted: 63\n"
     ]
    }
   ],
   "source": [
    "PROMPT = Path('artifacts/jsonl/instruct_prompt_completion.jsonl')\n",
    "TEMPLATE = \"\"\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\"\"\n",
    "\n",
    "if TRIO.exists():\n",
    "    with TRIO.open('r', encoding='utf-8') as fin, PROMPT.open('w', encoding='utf-8') as fout:\n",
    "        count = 0\n",
    "        for line in fin:\n",
    "            t = json.loads(line)\n",
    "            prompt = TEMPLATE.format(instruction=t['instruction'])\n",
    "            completion = t['output']\n",
    "            fout.write(orjson.dumps({\n",
    "                'prompt': prompt, \n",
    "                'completion': completion, \n",
    "                'metadata': t.get('metadata')\n",
    "            }).decode()+'\\n')\n",
    "            count += 1\n",
    "\n",
    "    print(f\"Created Prompt-Completion dataset: {PROMPT}\")\n",
    "    print(f\"Records converted: {count}\")\n",
    "else:\n",
    "    print(f\"Trio file not found: {TRIO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bedaf3",
   "metadata": {},
   "source": [
    "### Checkpoint: Show sample records from both formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc5b4e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAMPLE TRIO RECORDS (first 3)\n",
      "============================================================\n",
      "\n",
      "Record 1:\n",
      "  Instruction: Summarize the key steps from: How to configure SSO (v5). (Overview)....\n",
      "  Input: (empty)\n",
      "  Output: Key steps: enable SAML; map claims; verify time sync; check audience URI; review...\n",
      "  Metadata: {'doc_id': 'DOC-0484', 'schema_version': 'trio-v1'}\n",
      "\n",
      "Record 2:\n",
      "  Instruction: Summarize the key steps from: Data Retention Policy — Region US (Troubleshooting...\n",
      "  Input: (empty)\n",
      "  Output: Key steps: enable SAML; map claims; verify time sync; check audience URI; review...\n",
      "  Metadata: {'doc_id': 'DOC-0914', 'schema_version': 'trio-v1'}\n",
      "\n",
      "Record 3:\n",
      "  Instruction: Summarize the key steps from: Release 202502 — Key fixes (Setup)....\n",
      "  Input: (empty)\n",
      "  Output: Key steps: enable SAML; map claims; verify time sync; check audience URI; review...\n",
      "  Metadata: {'doc_id': 'DOC-0652', 'schema_version': 'trio-v1'}\n",
      "\n",
      "============================================================\n",
      "SAMPLE PROMPT-COMPLETION RECORDS (first 3)\n",
      "============================================================\n",
      "\n",
      "Record 1:\n",
      "  Prompt: ### Instruction:\n",
      "Summarize the key steps from: How to configure SSO (v5). (Overview).\n",
      "\n",
      "### Response:...\n",
      "  Completion: Key steps: enable SAML; map claims; verify time sync; check audience URI; review...\n",
      "  Metadata: {'doc_id': 'DOC-0484', 'schema_version': 'trio-v1'}\n",
      "\n",
      "Record 2:\n",
      "  Prompt: ### Instruction:\n",
      "Summarize the key steps from: Data Retention Policy — Region US (Troubleshooting).\n",
      "...\n",
      "  Completion: Key steps: enable SAML; map claims; verify time sync; check audience URI; review...\n",
      "  Metadata: {'doc_id': 'DOC-0914', 'schema_version': 'trio-v1'}\n",
      "\n",
      "Record 3:\n",
      "  Prompt: ### Instruction:\n",
      "Summarize the key steps from: Release 202502 — Key fixes (Setup).\n",
      "\n",
      "### Response:\n",
      "...\n",
      "  Completion: Key steps: enable SAML; map claims; verify time sync; check audience URI; review...\n",
      "  Metadata: {'doc_id': 'DOC-0652', 'schema_version': 'trio-v1'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAMPLE TRIO RECORDS (first 3)\")\n",
    "print(\"=\"*60)\n",
    "if TRIO.exists():\n",
    "    with TRIO.open('r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 3:\n",
    "                break\n",
    "            obj = json.loads(line)\n",
    "            print(f\"\\nRecord {i+1}:\")\n",
    "            print(f\"  Instruction: {obj['instruction'][:80]}...\")\n",
    "            print(f\"  Input: {obj['input'] or '(empty)'}\")\n",
    "            print(f\"  Output: {obj['output'][:80]}...\")\n",
    "            print(f\"  Metadata: {obj.get('metadata')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PROMPT-COMPLETION RECORDS (first 3)\")\n",
    "print(\"=\"*60)\n",
    "if PROMPT.exists():\n",
    "    with PROMPT.open('r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 3:\n",
    "                break\n",
    "            obj = json.loads(line)\n",
    "            print(f\"\\nRecord {i+1}:\")\n",
    "            print(f\"  Prompt: {obj['prompt'][:100]}...\")\n",
    "            print(f\"  Completion: {obj['completion'][:80]}...\")\n",
    "            print(f\"  Metadata: {obj.get('metadata')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ee9bf",
   "metadata": {},
   "source": [
    "## Part C — Synthesize Trio from RAG Chunks\n",
    "\n",
    "### C1. Build summarization tasks from chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf35a05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Trio dataset from RAG chunks: artifacts/jsonl/instruct_trio_from_rag.jsonl\n",
      "Records kept: 195\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import orjson\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "RAG = Path('artifacts/jsonl/rag_chunks_from_csv.jsonl')\n",
    "TRIO_RAG = Path('artifacts/jsonl/instruct_trio_from_rag.jsonl')\n",
    "\n",
    "min_text_chars = 200   # avoid trivially short chunks\n",
    "max_text_chars = 1200  # avoid overly long chunks\n",
    "\n",
    "if not RAG.exists():\n",
    "    print(f\"WARNING: RAG chunks file {RAG} not found!\")\n",
    "    print(\"This lab requires artifacts from Lab 13.\")\n",
    "    print(\"Please run Lab 13 first to generate the required files.\")\n",
    "else:\n",
    "    with RAG.open('r', encoding='utf-8') as fin, TRIO_RAG.open('w', encoding='utf-8') as fout:\n",
    "        kept = 0\n",
    "        seen = set()\n",
    "        for line in fin:\n",
    "            obj = json.loads(line)\n",
    "            text = re.sub(r\"\\s+\", \" \", obj['text']).strip()\n",
    "            if not (min_text_chars <= len(text) <= max_text_chars):\n",
    "                continue\n",
    "            instruction = \"Summarize the following section in 2–3 sentences focusing on the key steps and caveats.\"\n",
    "            inp = text\n",
    "            # weak target: headerized bullet summary; in production use a labeler or heuristics\n",
    "            target = \"Key steps: ensure SSO claims are mapped; verify time sync; review settings as noted.\"\n",
    "            key = (hash(text) % (10**12))  # approximate dedupe by text hash\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            rec = {\n",
    "                'instruction': instruction,\n",
    "                'input': inp,\n",
    "                'output': target,\n",
    "                'metadata': {\n",
    "                    'doc_id': obj.get('doc_id'),\n",
    "                    'chunk_id': obj.get('chunk_id'),\n",
    "                    'schema_version': 'trio-from-rag-v1'\n",
    "                }\n",
    "            }\n",
    "            fout.write(orjson.dumps(rec).decode() + '\\n')\n",
    "            kept += 1\n",
    "\n",
    "    print(f\"Created Trio dataset from RAG chunks: {TRIO_RAG}\")\n",
    "    print(f\"Records kept: {kept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac71181",
   "metadata": {},
   "source": [
    "### C2. Convert to a chat‑style prompt‑completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f763fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Chat-style Prompt-Completion dataset: artifacts/jsonl/instruct_chat_from_rag.jsonl\n",
      "Records converted: 195\n"
     ]
    }
   ],
   "source": [
    "CHAT = Path('artifacts/jsonl/instruct_chat_from_rag.jsonl')\n",
    "CHAT_TEMPLATE = (\n",
    "    \"<system>You are a concise technical assistant.</system>\\n\"\n",
    "    \"<user>Instruction: {instruction}\\n\\nContext: {context}</user>\\n\"\n",
    "    \"<assistant>\"\n",
    ")\n",
    "\n",
    "if TRIO_RAG.exists():\n",
    "    with TRIO_RAG.open('r', encoding='utf-8') as fin, CHAT.open('w', encoding='utf-8') as fout:\n",
    "        count = 0\n",
    "        for line in fin:\n",
    "            t = json.loads(line)\n",
    "            prompt = CHAT_TEMPLATE.format(instruction=t['instruction'], context=t['input'])\n",
    "            completion = t['output']\n",
    "            fout.write(orjson.dumps({\n",
    "                'prompt': prompt, \n",
    "                'completion': completion, \n",
    "                'metadata': t.get('metadata')\n",
    "            }).decode()+'\\n')\n",
    "            count += 1\n",
    "    \n",
    "    print(f\"Created Chat-style Prompt-Completion dataset: {CHAT}\")\n",
    "    print(f\"Records converted: {count}\")\n",
    "else:\n",
    "    print(f\"Trio RAG file not found: {TRIO_RAG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd009947",
   "metadata": {},
   "source": [
    "### Checkpoint: Compare token length distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eae3c8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Length Statistics (word-based proxy):\n",
      "============================================================\n",
      "Prompt tokens:\n",
      "  Mean: 90.4\n",
      "  Median: 91.0\n",
      "  Min: 60\n",
      "  Max: 122\n",
      "  95th percentile: 122.0\n",
      "\n",
      "Completion tokens:\n",
      "  Mean: 14.0\n",
      "  Median: 14.0\n",
      "  Min: 14\n",
      "  Max: 14\n",
      "  95th percentile: 14.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "if CHAT.exists():\n",
    "    prompt_lens = []\n",
    "    completion_lens = []\n",
    "    \n",
    "    with CHAT.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            # Token proxy: count words\n",
    "            prompt_lens.append(len(obj['prompt'].split()))\n",
    "            completion_lens.append(len(obj['completion'].split()))\n",
    "    \n",
    "    print(\"Token Length Statistics (word-based proxy):\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Prompt tokens:\")\n",
    "    print(f\"  Mean: {np.mean(prompt_lens):.1f}\")\n",
    "    print(f\"  Median: {np.median(prompt_lens):.1f}\")\n",
    "    print(f\"  Min: {np.min(prompt_lens)}\")\n",
    "    print(f\"  Max: {np.max(prompt_lens)}\")\n",
    "    print(f\"  95th percentile: {np.percentile(prompt_lens, 95):.1f}\")\n",
    "    print(f\"\\nCompletion tokens:\")\n",
    "    print(f\"  Mean: {np.mean(completion_lens):.1f}\")\n",
    "    print(f\"  Median: {np.median(completion_lens):.1f}\")\n",
    "    print(f\"  Min: {np.min(completion_lens)}\")\n",
    "    print(f\"  Max: {np.max(completion_lens)}\")\n",
    "    print(f\"  95th percentile: {np.percentile(completion_lens, 95):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c05d90",
   "metadata": {},
   "source": [
    "## Part D — Validation, Length Governance, and Hygiene\n",
    "\n",
    "### D1. Pydantic validators (optional but recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d39805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pydantic validation OK\n",
      "  - Trio schema validated\n",
      "  - Prompt-Completion schema validated\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import Optional, Dict\n",
    "\n",
    "class Trio(BaseModel):\n",
    "    instruction: str\n",
    "    input: str\n",
    "    output: str\n",
    "    metadata: Optional[Dict] = Field(default_factory=dict)\n",
    "\n",
    "class PC(BaseModel):\n",
    "    prompt: str\n",
    "    completion: str\n",
    "    metadata: Optional[Dict] = Field(default_factory=dict)\n",
    "\n",
    "# Validate a sample\n",
    "try:\n",
    "    _ = Trio(instruction='Do X', input='', output='Done', metadata={'doc_id':'DOC-0001'})\n",
    "    _ = PC(prompt='### Instruction:\\nDo X\\n\\n### Response:\\n', completion='Done')\n",
    "    print('✓ Pydantic validation OK')\n",
    "    print('  - Trio schema validated')\n",
    "    print('  - Prompt-Completion schema validated')\n",
    "except ValidationError as e:\n",
    "    print('✗ Validation failed:')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e6304",
   "metadata": {},
   "source": [
    "### D2. Length filters and simple decontamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65fff78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleansed dataset created: artifacts/jsonl/instruct_prompt_completion_cleansed.jsonl\n",
      "============================================================\n",
      "Records kept: 0\n",
      "Filtered by length: 0\n",
      "Filtered by decontamination: 63\n",
      "Total filtered: 63\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Token proxy: count words as a rough stand‑in\n",
    "def token_proxy(s):\n",
    "    return max(1, len(s.split()))\n",
    "\n",
    "MAX_PROMPT_TOK = 700\n",
    "MAX_COMP_TOK = 350\n",
    "\n",
    "INP = Path('artifacts/jsonl/instruct_prompt_completion.jsonl')\n",
    "OUT = Path('artifacts/jsonl/instruct_prompt_completion_cleansed.jsonl')\n",
    "\n",
    "# Suppose we have eval prompts to avoid leakage\n",
    "EVAL = {\"How many orders does customer\", \"Summarize the key steps from:\"}\n",
    "\n",
    "if INP.exists():\n",
    "    kept = 0\n",
    "    filtered_length = 0\n",
    "    filtered_decontam = 0\n",
    "    \n",
    "    with INP.open('r', encoding='utf-8') as fin, OUT.open('w', encoding='utf-8') as fout:\n",
    "        for line in fin:\n",
    "            obj = json.loads(line)\n",
    "            p, c = obj['prompt'], obj['completion']\n",
    "            \n",
    "            # Decontamination check\n",
    "            if any(trigger in p for trigger in EVAL):\n",
    "                filtered_decontam += 1\n",
    "                continue  # decontaminate against eval set\n",
    "            \n",
    "            # Length check\n",
    "            if token_proxy(p) > MAX_PROMPT_TOK or token_proxy(c) > MAX_COMP_TOK:\n",
    "                filtered_length += 1\n",
    "                continue\n",
    "            \n",
    "            fout.write(line)\n",
    "            kept += 1\n",
    "    \n",
    "    print(f\"Cleansed dataset created: {OUT}\")\n",
    "    print(f\"=\"*60)\n",
    "    print(f\"Records kept: {kept}\")\n",
    "    print(f\"Filtered by length: {filtered_length}\")\n",
    "    print(f\"Filtered by decontamination: {filtered_decontam}\")\n",
    "    print(f\"Total filtered: {filtered_length + filtered_decontam}\")\n",
    "else:\n",
    "    print(f\"Input file not found: {INP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba94ac",
   "metadata": {},
   "source": [
    "### D3. Dataset stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "128cf557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "============================================================\n",
      "{\n",
      "  \"counts\": {\n",
      "    \"trio\": 63,\n",
      "    \"pc\": 63,\n",
      "    \"chat_pc\": 195,\n",
      "    \"cleansed_pc\": 0,\n",
      "    \"trio_from_rag\": 195\n",
      "  },\n",
      "  \"prompt_tokens_mean\": 71.85658914728683,\n",
      "  \"prompt_tokens_median\": 60.0,\n",
      "  \"prompt_tokens_max\": 122,\n",
      "  \"completion_tokens_mean\": 14.0,\n",
      "  \"completion_tokens_median\": 14.0,\n",
      "  \"completion_tokens_max\": 14\n",
      "}\n",
      "\n",
      "Stats saved to: artifacts/stats/instruct_stats.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "STATS = Path('artifacts/stats/instruct_stats.json')\n",
    "counts = {'trio':0,'pc':0,'chat_pc':0,'cleansed_pc':0,'trio_from_rag':0}\n",
    "lengths = {'prompt':[], 'completion':[]}\n",
    "\n",
    "file_map = [\n",
    "    ('artifacts/jsonl/instruct_trio.jsonl', 'trio'),\n",
    "    ('artifacts/jsonl/instruct_prompt_completion.jsonl', 'pc'),\n",
    "    ('artifacts/jsonl/instruct_chat_from_rag.jsonl', 'chat_pc'),\n",
    "    ('artifacts/jsonl/instruct_prompt_completion_cleansed.jsonl', 'cleansed_pc'),\n",
    "    ('artifacts/jsonl/instruct_trio_from_rag.jsonl', 'trio_from_rag')\n",
    "]\n",
    "\n",
    "for filepath, key in file_map:\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                counts[key] += 1\n",
    "                \n",
    "                # Collect length stats for prompt-completion formats\n",
    "                if 'prompt' in obj and 'completion' in obj:\n",
    "                    lengths['prompt'].append(len(obj['prompt'].split()))\n",
    "                    lengths['completion'].append(len(obj['completion'].split()))\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "stats_output = {\n",
    "    'counts': counts,\n",
    "    'prompt_tokens_mean': float(np.mean(lengths['prompt'])) if lengths['prompt'] else 0.0,\n",
    "    'prompt_tokens_median': float(np.median(lengths['prompt'])) if lengths['prompt'] else 0.0,\n",
    "    'prompt_tokens_max': int(np.max(lengths['prompt'])) if lengths['prompt'] else 0,\n",
    "    'completion_tokens_mean': float(np.mean(lengths['completion'])) if lengths['completion'] else 0.0,\n",
    "    'completion_tokens_median': float(np.median(lengths['completion'])) if lengths['completion'] else 0.0,\n",
    "    'completion_tokens_max': int(np.max(lengths['completion'])) if lengths['completion'] else 0\n",
    "}\n",
    "\n",
    "STATS.write_text(json.dumps(stats_output, indent=2))\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(stats_output, indent=2))\n",
    "print(f\"\\nStats saved to: {STATS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b8ceb",
   "metadata": {},
   "source": [
    "## Part E — Wrap‑Up\n",
    "\n",
    "### Analysis Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d59555",
   "metadata": {},
   "source": [
    "#### 1. Schema Choice: Trio vs Prompt‑Completion vs Chat\n",
    "\n",
    "**For this project, I would choose the Trio schema (`{instruction, input, output}`).**\n",
    "\n",
    "**Reasons:**\n",
    "1. **Flexibility at Training Time**: The Trio schema separates the task description (`instruction`) from the context (`input`), allowing us to experiment with different prompt templates during training without regenerating the entire dataset. This is critical for A/B testing different instruction formats.\n",
    "\n",
    "2. **Handles Variable Context**: Some tasks have no context (just an instruction), while others require substantial context. The Trio schema handles both cases elegantly—we can leave `input` empty when not needed, making it more versatile than a baked-in prompt format.\n",
    "\n",
    "3. **Composability**: The clear separation makes it easier to:\n",
    "   - Add system messages or prefixes later\n",
    "   - Support multi-turn conversations by chaining instructions\n",
    "   - Mix different task types in one dataset without template conflicts\n",
    "\n",
    "**When to use alternatives:**\n",
    "- **Prompt-Completion**: If deploying directly to an API that expects this format and you're confident about your prompt template\n",
    "- **Chat**: For multi-turn conversational fine-tuning where role distinctions (system/user/assistant) are critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232a6b44",
   "metadata": {},
   "source": [
    "#### 2. Template Example and Modification Strategy\n",
    "\n",
    "**Training-time template:**\n",
    "```python\n",
    "template = (\n",
    "    \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n\"\n",
    "    \"### Input:\\n{input}\\n\\n\"\n",
    "    \"### Response:\\n\"\n",
    ")\n",
    "```\n",
    "\n",
    "**How to change without rewriting the dataset:**\n",
    "\n",
    "Since we stored data in Trio format, we can modify the template at training time without touching the JSONL:\n",
    "\n",
    "```python\n",
    "# Option A: Minimal template\n",
    "minimal_template = \"{instruction}\\n{input}\\n\\n\"\n",
    "\n",
    "# Option B: Chat-style template\n",
    "chat_template = (\n",
    "    \"<system>You are a helpful assistant.</system>\\n\"\n",
    "    \"<user>{instruction}\\n{input}</user>\\n\"\n",
    "    \"<assistant>\"\n",
    ")\n",
    "\n",
    "# Option C: Few-shot template (add examples before the test case)\n",
    "fewshot_template = (\n",
    "    \"Here are some examples:\\n{examples}\\n\\n\"\n",
    "    \"Now complete this:\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Key benefit:** The raw data (`instruction`, `input`, `output`) remains clean and reusable. Template changes happen in the training dataloader, not in the dataset itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff211986",
   "metadata": {},
   "source": [
    "#### 3. Length Filtering Thresholds\n",
    "\n",
    "**Current thresholds:**\n",
    "- `MAX_PROMPT_TOK = 700` (word-based proxy)\n",
    "- `MAX_COMP_TOK = 350` (word-based proxy)\n",
    "\n",
    "**Rationale:**\n",
    "- Assumes a model with ~2048 token context (e.g., GPT-2 medium, smaller LLaMA variants)\n",
    "- Leaves headroom for template overhead (~50-100 tokens) and response generation\n",
    "- 700 prompt + 350 completion ≈ 1,050 tokens, well under 2K with safety margin\n",
    "\n",
    "**Adjustments for different models:**\n",
    "\n",
    "| Model Context | MAX_PROMPT_TOK | MAX_COMP_TOK | Reasoning |\n",
    "|---------------|----------------|--------------|------------|\n",
    "| **1K tokens** (small models) | 400 | 200 | Aggressive filtering; prioritize short, focused examples |\n",
    "| **4K tokens** (GPT-3.5, LLaMA-7B) | 1,500 | 800 | More room for detailed instructions and longer responses |\n",
    "| **8K+ tokens** (Claude, GPT-4) | 3,000 | 1,500 | Can include long-form documents; still cap to avoid outliers |\n",
    "| **32K+ tokens** (GPT-4-32K, Claude-2) | 10,000 | 5,000 | Book-length context; useful for RAG with multiple chunks |\n",
    "\n",
    "**Additional considerations:**\n",
    "- Use a proper tokenizer (tiktoken, sentencepiece) instead of word-splitting for accurate counts\n",
    "- Set 95th percentile caps, not just means, to avoid rare long examples causing OOM\n",
    "- For instruction-tuning, shorter is often better—focus on quality over length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27320d",
   "metadata": {},
   "source": [
    "#### 4. Decontamination Pipeline Strategy\n",
    "\n",
    "**Where to run decontamination:**\n",
    "\n",
    "1. **Early stage (this lab's approach)**: During JSONL generation, immediately after cleaning\n",
    "   - ✓ Prevents contaminated data from ever entering downstream pipelines\n",
    "   - ✓ Easy to audit and version-control the decontamination rules\n",
    "   - ✗ Requires re-generating JSONL if eval set changes\n",
    "\n",
    "2. **Split-time decontamination** (recommended for production):\n",
    "   ```python\n",
    "   # After generating full dataset, before train/val split\n",
    "   eval_prompts = load_eval_set()  # Load held-out eval prompts\n",
    "   train_data = [r for r in full_data if not any(ep in r['prompt'] for ep in eval_prompts)]\n",
    "   ```\n",
    "   - ✓ More flexible—can change eval set without regenerating base data\n",
    "   - ✓ Can use fuzzy matching (edit distance, embeddings) for better detection\n",
    "\n",
    "3. **Runtime decontamination** (during training):\n",
    "   - Only if dynamic eval sets are created during training\n",
    "   - Adds overhead; generally avoided\n",
    "\n",
    "**Decontamination checklist:**\n",
    "- [ ] **Exact substring matching**: Remove training examples containing exact eval prompts\n",
    "- [ ] **Fuzzy matching**: Use edit distance or n-gram overlap (e.g., Jaccard similarity > 0.8)\n",
    "- [ ] **Semantic deduplication**: Embed prompts and remove training examples within cosine similarity threshold of eval\n",
    "- [ ] **Source-based filtering**: If eval comes from specific docs, remove all training data from those docs\n",
    "- [ ] **Date-based splitting**: For time-series data, ensure training data predates eval data\n",
    "\n",
    "**For this project:**\n",
    "We implemented **early-stage exact substring matching** (D2 above). For production, I would add:\n",
    "- Fuzzy matching with edit distance < 10 or n-gram overlap > 0.7\n",
    "- Store decontamination logs to track what was filtered and why\n",
    "- Re-run decontamination before each model training run to catch new eval additions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d598832",
   "metadata": {},
   "source": [
    "## Final Outputs Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36109fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL OUTPUTS SUMMARY\n",
      "============================================================\n",
      "✓ Trio schema dataset\n",
      "  Path: artifacts/jsonl/instruct_trio.jsonl\n",
      "  Size: 16,321 bytes\n",
      "  Records: 63\n",
      "\n",
      "✓ Prompt-Completion dataset\n",
      "  Path: artifacts/jsonl/instruct_prompt_completion.jsonl\n",
      "  Size: 17,896 bytes\n",
      "  Records: 63\n",
      "\n",
      "✓ Trio from RAG chunks\n",
      "  Path: artifacts/jsonl/instruct_trio_from_rag.jsonl\n",
      "  Size: 152,173 bytes\n",
      "  Records: 195\n",
      "\n",
      "✓ Chat-style from RAG\n",
      "  Path: artifacts/jsonl/instruct_chat_from_rag.jsonl\n",
      "  Size: 171,088 bytes\n",
      "  Records: 195\n",
      "\n",
      "✓ Cleansed dataset\n",
      "  Path: artifacts/jsonl/instruct_prompt_completion_cleansed.jsonl\n",
      "  Size: 0 bytes\n",
      "  Records: 0\n",
      "\n",
      "✓ Dataset statistics\n",
      "  Path: artifacts/stats/instruct_stats.json\n",
      "  Size: 322 bytes\n",
      "\n",
      "============================================================\n",
      "Lab 14 Complete!\n",
      "============================================================\n",
      "\n",
      "Final Statistics:\n",
      "{\n",
      "  \"counts\": {\n",
      "    \"trio\": 63,\n",
      "    \"pc\": 63,\n",
      "    \"chat_pc\": 195,\n",
      "    \"cleansed_pc\": 0,\n",
      "    \"trio_from_rag\": 195\n",
      "  },\n",
      "  \"prompt_tokens_mean\": 71.85658914728683,\n",
      "  \"prompt_tokens_median\": 60.0,\n",
      "  \"prompt_tokens_max\": 122,\n",
      "  \"completion_tokens_mean\": 14.0,\n",
      "  \"completion_tokens_median\": 14.0,\n",
      "  \"completion_tokens_max\": 14\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL OUTPUTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "outputs = [\n",
    "    ('artifacts/jsonl/instruct_trio.jsonl', 'Trio schema dataset'),\n",
    "    ('artifacts/jsonl/instruct_prompt_completion.jsonl', 'Prompt-Completion dataset'),\n",
    "    ('artifacts/jsonl/instruct_trio_from_rag.jsonl', 'Trio from RAG chunks'),\n",
    "    ('artifacts/jsonl/instruct_chat_from_rag.jsonl', 'Chat-style from RAG'),\n",
    "    ('artifacts/jsonl/instruct_prompt_completion_cleansed.jsonl', 'Cleansed dataset'),\n",
    "    ('artifacts/stats/instruct_stats.json', 'Dataset statistics')\n",
    "]\n",
    "\n",
    "for path, desc in outputs:\n",
    "    p = Path(path)\n",
    "    if p.exists():\n",
    "        if p.is_file():\n",
    "            size = p.stat().st_size\n",
    "            # Count lines for JSONL files\n",
    "            if path.endswith('.jsonl'):\n",
    "                with open(p, 'r') as f:\n",
    "                    line_count = sum(1 for _ in f)\n",
    "                print(f\"✓ {desc}\")\n",
    "                print(f\"  Path: {path}\")\n",
    "                print(f\"  Size: {size:,} bytes\")\n",
    "                print(f\"  Records: {line_count:,}\")\n",
    "            else:\n",
    "                print(f\"✓ {desc}\")\n",
    "                print(f\"  Path: {path}\")\n",
    "                print(f\"  Size: {size:,} bytes\")\n",
    "    else:\n",
    "        print(f\"✗ {desc} - NOT FOUND\")\n",
    "        print(f\"  Expected: {path}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Lab 14 Complete!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display final stats\n",
    "stats_file = Path('artifacts/stats/instruct_stats.json')\n",
    "if stats_file.exists():\n",
    "    print(\"\\nFinal Statistics:\")\n",
    "    with open(stats_file, 'r') as f:\n",
    "        stats = json.load(f)\n",
    "        print(json.dumps(stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff6c77",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Schema Selection\n",
    "- **Trio**: Best for flexibility and template experimentation\n",
    "- **Prompt-Completion**: Best for direct API deployment\n",
    "- **Chat**: Best for conversational fine-tuning\n",
    "\n",
    "### Dataset Hygiene\n",
    "1. **Deduplication**: Content-based hashing prevents redundant training\n",
    "2. **Length Filtering**: Model context budget compliance\n",
    "3. **Decontamination**: Prevents eval leakage\n",
    "4. **Language Filtering**: Ensures target language consistency\n",
    "\n",
    "### Best Practices\n",
    "- Store raw data in flexible schemas (Trio)\n",
    "- Apply templates at training time, not generation time\n",
    "- Use proper tokenizers for accurate length measurement\n",
    "- Log all filtering decisions for reproducibility\n",
    "- Validate JSONL line-by-line before training\n",
    "\n",
    "### Common Pitfalls Avoided\n",
    "- ✓ Separated instruction from input (no template mixing)\n",
    "- ✓ Applied decontamination early in pipeline\n",
    "- ✓ Enforced length budgets with headroom\n",
    "- ✓ Maintained provenance metadata throughout\n",
    "- ✓ Validated schema compliance with Pydantic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
