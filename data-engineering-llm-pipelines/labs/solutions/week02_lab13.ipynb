{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c706f2",
   "metadata": {},
   "source": [
    "# Lab 13 — From Tables/Text to JSONL for LLM Workloads\n",
    "\n",
    "**Focus Area:** JSON Lines (JSONL) for large **LLM** corpora — streaming, memory efficiency, one-record-per-line, provenance, and validation\n",
    "\n",
    "This notebook implements the complete solution for Lab 13."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b26cf1",
   "metadata": {},
   "source": [
    "## Setup - Create Required Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea5ce8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created successfully\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "for p in ['artifacts/jsonl','artifacts/samples','tools','data']:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Directories created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54f9a3",
   "metadata": {},
   "source": [
    "## Part A — Build an LLM-Native Source CSV\n",
    "\n",
    "### A1. Generate `data/corpus_llm.csv` (1,000 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037a6533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/corpus_llm.csv rows= 1010\n"
     ]
    }
   ],
   "source": [
    "import csv, random\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(42)\n",
    "N = 1000\n",
    "TYPES = [\"help_article\",\"policy\",\"release_note\",\"faq\"]\n",
    "SECTIONS = [\"Overview\",\"Setup\",\"Troubleshooting\",\"FAQ\"]\n",
    "TAGS = [\"billing\",\"security\",\"compliance\",\"sso\",\"api\",\"governance\",\"export\",\"retention\",\"privacy\",\"rate_limits\"]\n",
    "LANGS = [\"en\",\"en\",\"en\",\"de\",\"fr\"]  # mostly English\n",
    "CONF = [\"public\",\"internal\"]  # governance labels\n",
    "now = datetime(2025, 2, 10)\n",
    "\n",
    "boiler = (\n",
    "    \"This article explains how to configure single sign-on with step-by-step instructions. \"\n",
    "    \"Use the admin console to enable SAML and verify claim mappings. \"\n",
    "    \"Common pitfalls include clock skew and incorrect audience URIs. \"\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for i in range(1, N+1):\n",
    "    kind = random.choice(TYPES)\n",
    "    doc_id = f\"DOC-{i:04d}\"\n",
    "    title = {\n",
    "        \"help_article\": f\"How to configure SSO (v{random.randint(1,5)}).\",\n",
    "        \"policy\": f\"Data Retention Policy — Region {random.choice(['US','EU','APAC'])}\",\n",
    "        \"release_note\": f\"Release 2025{random.randint(1,12):02d} — Key fixes\",\n",
    "        \"faq\": f\"FAQ: {random.choice(['Exports','Rate Limits','Privacy','Billing'])}\"\n",
    "    }[kind]\n",
    "    section = random.choice(SECTIONS)\n",
    "    # Simulate occasional missing/short text and duplicates later\n",
    "    body = (boiler * random.randint(1,3)) + f\"Additional details about {random.choice(TAGS)} and {random.choice(TAGS)}.\\n\"\n",
    "    tags = \",\".join(random.sample(TAGS, k=random.randint(2,4)))\n",
    "    created = (now - timedelta(days=random.randint(0, 240))).strftime('%Y-%m-%d')\n",
    "    updated = (now - timedelta(days=random.randint(0, 30))).strftime('%Y-%m-%d')\n",
    "    language = random.choice(LANGS)\n",
    "    confidentiality = random.choice(CONF)\n",
    "    source_url = f\"https://example.local/{kind}/{doc_id.lower()}\"\n",
    "    rows.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"type\": kind,\n",
    "        \"title\": title,\n",
    "        \"section\": section,\n",
    "        \"body_text\": body.strip(),\n",
    "        \"tags\": tags,\n",
    "        \"source_url\": source_url,\n",
    "        \"created_at\": created,\n",
    "        \"updated_at\": updated,\n",
    "        \"language\": language,\n",
    "        \"confidentiality\": confidentiality,\n",
    "    })\n",
    "\n",
    "# Add deliberate duplicates and a few missing values\n",
    "for j in range(10):\n",
    "    rows.append({**rows[j], \"doc_id\": f\"DOC-DUP-{j:02d}\"})\n",
    "for k in range(5):\n",
    "    rows[k][\"body_text\"] = \"\"  # missing text\n",
    "\n",
    "out_csv = Path('data/corpus_llm.csv')\n",
    "with out_csv.open('w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=rows[0].keys())\n",
    "    writer.writeheader(); writer.writerows(rows)\n",
    "print('Wrote', out_csv, 'rows=', len(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba836f39",
   "metadata": {},
   "source": [
    "### A2. Quick peek at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d132299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1010\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "doc_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "section",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "body_text",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "tags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "created_at",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "updated_at",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "language",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "confidentiality",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a6d155b1-1bae-41cb-8f60-0b507abfb19a",
       "rows": [
        [
         "0",
         "DOC-0001",
         "help_article",
         "How to configure SSO (v1).",
         "Setup",
         null,
         "rate_limits,export",
         "https://example.local/help_article/doc-0001",
         "2025-02-02",
         "2025-02-10",
         "en",
         "public"
        ],
        [
         "1",
         "DOC-0002",
         "policy",
         "Data Retention Policy — Region APAC",
         "FAQ",
         null,
         "billing,compliance,export",
         "https://example.local/policy/doc-0002",
         "2024-11-15",
         "2025-02-02",
         "en",
         "public"
        ],
        [
         "2",
         "DOC-0003",
         "release_note",
         "Release 202507 — Key fixes",
         "Troubleshooting",
         null,
         "retention,privacy",
         "https://example.local/release_note/doc-0003",
         "2025-01-10",
         "2025-01-12",
         "de",
         "public"
        ],
        [
         "3",
         "DOC-0004",
         "release_note",
         "Release 202510 — Key fixes",
         "Overview",
         null,
         "sso,security",
         "https://example.local/release_note/doc-0004",
         "2024-11-05",
         "2025-02-02",
         "de",
         "internal"
        ],
        [
         "4",
         "DOC-0005",
         "policy",
         "Data Retention Policy — Region EU",
         "Overview",
         null,
         "sso,compliance,retention,rate_limits",
         "https://example.local/policy/doc-0005",
         "2024-12-03",
         "2025-01-12",
         "fr",
         "public"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>body_text</th>\n",
       "      <th>tags</th>\n",
       "      <th>source_url</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>language</th>\n",
       "      <th>confidentiality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOC-0001</td>\n",
       "      <td>help_article</td>\n",
       "      <td>How to configure SSO (v1).</td>\n",
       "      <td>Setup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rate_limits,export</td>\n",
       "      <td>https://example.local/help_article/doc-0001</td>\n",
       "      <td>2025-02-02</td>\n",
       "      <td>2025-02-10</td>\n",
       "      <td>en</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DOC-0002</td>\n",
       "      <td>policy</td>\n",
       "      <td>Data Retention Policy — Region APAC</td>\n",
       "      <td>FAQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>billing,compliance,export</td>\n",
       "      <td>https://example.local/policy/doc-0002</td>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>2025-02-02</td>\n",
       "      <td>en</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DOC-0003</td>\n",
       "      <td>release_note</td>\n",
       "      <td>Release 202507 — Key fixes</td>\n",
       "      <td>Troubleshooting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>retention,privacy</td>\n",
       "      <td>https://example.local/release_note/doc-0003</td>\n",
       "      <td>2025-01-10</td>\n",
       "      <td>2025-01-12</td>\n",
       "      <td>de</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DOC-0004</td>\n",
       "      <td>release_note</td>\n",
       "      <td>Release 202510 — Key fixes</td>\n",
       "      <td>Overview</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sso,security</td>\n",
       "      <td>https://example.local/release_note/doc-0004</td>\n",
       "      <td>2024-11-05</td>\n",
       "      <td>2025-02-02</td>\n",
       "      <td>de</td>\n",
       "      <td>internal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOC-0005</td>\n",
       "      <td>policy</td>\n",
       "      <td>Data Retention Policy — Region EU</td>\n",
       "      <td>Overview</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sso,compliance,retention,rate_limits</td>\n",
       "      <td>https://example.local/policy/doc-0005</td>\n",
       "      <td>2024-12-03</td>\n",
       "      <td>2025-01-12</td>\n",
       "      <td>fr</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id          type                                title  \\\n",
       "0  DOC-0001  help_article           How to configure SSO (v1).   \n",
       "1  DOC-0002        policy  Data Retention Policy — Region APAC   \n",
       "2  DOC-0003  release_note           Release 202507 — Key fixes   \n",
       "3  DOC-0004  release_note           Release 202510 — Key fixes   \n",
       "4  DOC-0005        policy    Data Retention Policy — Region EU   \n",
       "\n",
       "           section body_text                                  tags  \\\n",
       "0            Setup       NaN                    rate_limits,export   \n",
       "1              FAQ       NaN             billing,compliance,export   \n",
       "2  Troubleshooting       NaN                     retention,privacy   \n",
       "3         Overview       NaN                          sso,security   \n",
       "4         Overview       NaN  sso,compliance,retention,rate_limits   \n",
       "\n",
       "                                    source_url  created_at  updated_at  \\\n",
       "0  https://example.local/help_article/doc-0001  2025-02-02  2025-02-10   \n",
       "1        https://example.local/policy/doc-0002  2024-11-15  2025-02-02   \n",
       "2  https://example.local/release_note/doc-0003  2025-01-10  2025-01-12   \n",
       "3  https://example.local/release_note/doc-0004  2024-11-05  2025-02-02   \n",
       "4        https://example.local/policy/doc-0005  2024-12-03  2025-01-12   \n",
       "\n",
       "  language confidentiality  \n",
       "0       en          public  \n",
       "1       en          public  \n",
       "2       de          public  \n",
       "3       de        internal  \n",
       "4       fr          public  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_peek = pd.read_csv('data/corpus_llm.csv')\n",
    "print(f\"Total rows: {len(df_peek)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df_peek.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee238c44",
   "metadata": {},
   "source": [
    "## Part B — Expose the CSV via a Local API\n",
    "\n",
    "### B1. Create `tools/corpus_api.py` (FastAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad55e981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API file created: tools/corpus_api.py\n",
      "\n",
      "To run the API, execute in a terminal from the tools folder (optionally in a virtual environment):\n",
      "  uvicorn corpus_api:app --reload --port 8000\n",
      "\n",
      "Note: The API now handles NaN values by converting them to empty strings\n"
     ]
    }
   ],
   "source": [
    "api_code = '''# tools/corpus_api.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "app = FastAPI(title=\"LLM Corpus API\", version=\"1.0.0\")\n",
    "DF = None\n",
    "\n",
    "class Doc(BaseModel):\n",
    "    doc_id: str\n",
    "    type: str\n",
    "    title: str\n",
    "    section: str\n",
    "    body_text: str\n",
    "    tags: str\n",
    "    source_url: str\n",
    "    created_at: str\n",
    "    updated_at: str\n",
    "    language: str\n",
    "    confidentiality: str\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_data():\n",
    "    global DF\n",
    "    DF = pd.read_csv('../data/corpus_llm.csv')\n",
    "    # Convert NaN to empty strings for all string columns\n",
    "    string_cols = ['body_text', 'title', 'section', 'tags', 'source_url']\n",
    "    for col in string_cols:\n",
    "        DF[col] = DF[col].fillna('')\n",
    "\n",
    "@app.get('/health')\n",
    "async def health():\n",
    "    return {\"ok\": True}\n",
    "\n",
    "@app.get('/v1/corpus', response_model=List[Doc])\n",
    "async def list_docs(page: int = 1, page_size: int = 50, q: str | None = None,\n",
    "                    language: str | None = None, conf: str | None = None):\n",
    "    if page < 1 or page_size < 1 or page_size > 200:\n",
    "        raise HTTPException(400, 'bad paging params')\n",
    "    df = DF\n",
    "    if q:\n",
    "        mask = df['body_text'].astype(str).str.contains(q, case=False, na=False) \\\\\n",
    "             | df['title'].astype(str).str.contains(q, case=False, na=False)\n",
    "        df = df[mask]\n",
    "    if language:\n",
    "        df = df[df['language'] == language]\n",
    "    if conf:\n",
    "        df = df[df['confidentiality'] == conf]\n",
    "    start = (page - 1) * page_size\n",
    "    end = start + page_size\n",
    "    recs = df.iloc[start:end].to_dict(orient='records')\n",
    "    return recs\n",
    "\n",
    "@app.get('/v1/corpus/{doc_id}', response_model=Doc)\n",
    "async def get_doc(doc_id: str):\n",
    "    row = DF.loc[DF['doc_id'] == doc_id]\n",
    "    if row.empty:\n",
    "        raise HTTPException(404, 'not found')\n",
    "    return row.iloc[0].to_dict()\n",
    "'''\n",
    "\n",
    "with open('tools/corpus_api.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(api_code)\n",
    "print(\"API file created: tools/corpus_api.py\")\n",
    "print(\"\\nTo run the API, execute in a terminal from the tools folder (optionally in a virtual environment):\")\n",
    "print(\"  uvicorn corpus_api:app --reload --port 8000\")\n",
    "print(\"\\nNote: The API now handles NaN values by converting them to empty strings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712c8197",
   "metadata": {},
   "source": [
    "## Part C — Ingest, Clean, Filter, De-duplicate\n",
    "\n",
    "### C1. Load CSV and define cleaners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "934d1d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rows loaded: 1010\n",
      "Rows after normalization: 1010\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "raw = pd.read_csv('data/corpus_llm.csv')\n",
    "print(f\"Raw rows loaded: {len(raw)}\")\n",
    "\n",
    "# Basic cleaners\n",
    "def normalize_ws(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", str(s or \"\")).strip()\n",
    "    return s\n",
    "\n",
    "def clean_row(row):\n",
    "    row['title'] = normalize_ws(row.get('title',''))\n",
    "    row['section'] = normalize_ws(row.get('section','')) or 'Overview'\n",
    "    row['body_text'] = normalize_ws(row.get('body_text',''))\n",
    "    return row\n",
    "\n",
    "clean = raw.apply(clean_row, axis=1)\n",
    "print(f\"Rows after normalization: {len(clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f28a6f",
   "metadata": {},
   "source": [
    "### C2. Governance & language filters + missing handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d33e7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after governance (public) and language (en) filtering: 305\n",
      "Dropped for empty/short body: 2\n",
      "Rows remaining: 303\n"
     ]
    }
   ],
   "source": [
    "# Keep only public + English for a typical RAG public index\n",
    "flt = (clean['confidentiality'] == 'public') & (clean['language'] == 'en')\n",
    "clean = clean.loc[flt].copy()\n",
    "print(f\"Rows after governance (public) and language (en) filtering: {len(clean)}\")\n",
    "\n",
    "# Drop rows with empty body; log counts\n",
    "before = len(clean)\n",
    "clean = clean[clean['body_text'].str.len() >= 30].copy()  # min length\n",
    "dropped_empty = before - len(clean)\n",
    "print(f'Dropped for empty/short body: {dropped_empty}')\n",
    "print(f'Rows remaining: {len(clean)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e1213",
   "metadata": {},
   "source": [
    "### C3. De-duplicate by content signature (keep latest update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cef86816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed: 8\n",
      "Rows after dedupe: 295\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def content_key(row):\n",
    "    sig = normalize_ws(row['title'] + ' ' + row['body_text']).lower()\n",
    "    return hashlib.sha1(sig.encode('utf-8')).hexdigest()\n",
    "\n",
    "clean['content_key'] = clean.apply(content_key, axis=1)\n",
    "# Keep most recent updated_at per content_key\n",
    "clean['updated_at'] = pd.to_datetime(clean['updated_at'])\n",
    "clean.sort_values(['content_key','updated_at'], ascending=[True, False], inplace=True)\n",
    "\n",
    "before_dedupe = len(clean)\n",
    "clean = clean.drop_duplicates(subset=['content_key'], keep='first')\n",
    "clean.drop(columns=['content_key'], inplace=True)\n",
    "duplicates_removed = before_dedupe - len(clean)\n",
    "print(f'Duplicates removed: {duplicates_removed}')\n",
    "print(f'Rows after dedupe: {len(clean)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885c6dd",
   "metadata": {},
   "source": [
    "### Summary of Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "094dad7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLEANING PIPELINE SUMMARY\n",
      "============================================================\n",
      "Starting rows: 1010\n",
      "After governance/language filter: 305\n",
      "After empty/short body removal: 303\n",
      "After deduplication: 295\n",
      "Total rows removed: 715\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CLEANING PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Starting rows: {len(raw)}\")\n",
    "print(f\"After governance/language filter: {len(clean) + duplicates_removed + dropped_empty}\")\n",
    "print(f\"After empty/short body removal: {len(clean) + duplicates_removed}\")\n",
    "print(f\"After deduplication: {len(clean)}\")\n",
    "print(f\"Total rows removed: {len(raw) - len(clean)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316d069",
   "metadata": {},
   "source": [
    "### C4. Optional API ingestion (simulate service boundary)\n",
    "\n",
    "**Note:** This section requires the API to be running. Since we're working in a notebook, we'll demonstrate the code but note that it requires the API server to be active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f103f11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/sysadmin/llm_venv/lib/python3.13/site-packages (2.32.5)\n",
      "Requirement already satisfied: orjson in /home/sysadmin/llm_venv/lib/python3.13/site-packages (3.11.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sysadmin/llm_venv/lib/python3.13/site-packages (from requests) (2025.10.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "API fetched rows: 305\n",
      "API ingestion code ready\n",
      "For this solution, we'll continue with the CSV-based clean dataframe\n"
     ]
    }
   ],
   "source": [
    "%pip install requests orjson\n",
    "\n",
    "# This code would work if the API is running at http://127.0.0.1:8000\n",
    "# Uncomment and run if you have the API server started\n",
    "\n",
    "import requests, time, orjson\n",
    "\n",
    "API = 'http://127.0.0.1:8000'\n",
    "page = 1; PAGE_SIZE = 100\n",
    "api_rows = []\n",
    "while True:\n",
    "    try:\n",
    "        r = requests.get(f\"{API}/v1/corpus\", params={'page': page, 'page_size': PAGE_SIZE, 'language':'en', 'conf':'public'}, timeout=5)\n",
    "        if r.status_code != 200:\n",
    "            time.sleep(1); continue\n",
    "        batch = r.json()\n",
    "        if not batch:\n",
    "            break\n",
    "        api_rows.extend(batch)\n",
    "        page += 1\n",
    "    except Exception as e:\n",
    "        print(f\"API not available: {e}\")\n",
    "        break\n",
    "        \n",
    "if api_rows:\n",
    "    api_df = pd.DataFrame(api_rows)\n",
    "    print('API fetched rows:', len(api_df))\n",
    "else:\n",
    "    print(\"API ingestion skipped - using CSV-based clean dataframe\")\n",
    "\n",
    "print(\"API ingestion code ready\")\n",
    "print(\"For this solution, we'll continue with the CSV-based clean dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc9b94d",
   "metadata": {},
   "source": [
    "## Part D — Write JSONL (RAG Chunks + SFT/Eval)\n",
    "\n",
    "### D1. Chunking helper (overlap to preserve context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c74d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text length: 2500 characters\n",
      "Number of chunks created: 17\n",
      "First chunk length: 200\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_chunks(text: str, max_chars=900, overlap=150):\n",
    "    text = re.sub(r\"\\s+\", \" \", str(text)).strip()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        end = min(i + max_chars, len(text))\n",
    "        chunk = text[i:end]\n",
    "        chunks.append(chunk)\n",
    "        if end == len(text): break\n",
    "        i = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "# Test the chunking function\n",
    "test_text = \"This is a test sentence. \" * 100\n",
    "test_chunks = split_chunks(test_text, max_chars=200, overlap=50)\n",
    "print(f\"Test text length: {len(test_text)} characters\")\n",
    "print(f\"Number of chunks created: {len(test_chunks)}\")\n",
    "print(f\"First chunk length: {len(test_chunks[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c5adfc",
   "metadata": {},
   "source": [
    "### D2. CSV→RAG JSONL with provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a066a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG JSONL created: artifacts/jsonl/rag_chunks_from_csv.jsonl\n",
      "Documents processed: 295\n",
      "Total chunks created: 295\n"
     ]
    }
   ],
   "source": [
    "import orjson\n",
    "from datetime import timezone\n",
    "from pathlib import Path\n",
    "\n",
    "rag_path = Path('artifacts/jsonl/rag_chunks_from_csv.jsonl')\n",
    "chunk_count = 0\n",
    "doc_count = 0\n",
    "\n",
    "with rag_path.open('w', encoding='utf-8') as f:\n",
    "    for _, r in clean.iterrows():\n",
    "        chunks = split_chunks(r['body_text'])\n",
    "        doc_count += 1\n",
    "        for j, ch in enumerate(chunks):\n",
    "            rec = {\n",
    "                'doc_id': r['doc_id'],\n",
    "                'chunk_id': f\"{r['doc_id']}-{j:04d}\",\n",
    "                'text': ch,\n",
    "                'metadata': {\n",
    "                    'title': r['title'], \n",
    "                    'section': r['section'], \n",
    "                    'tags': r['tags'],\n",
    "                    'source_url': r['source_url'], \n",
    "                    'language': r['language'],\n",
    "                    'confidentiality': r['confidentiality'], \n",
    "                    'schema_version': 'rag-chunk-v1'\n",
    "                }\n",
    "            }\n",
    "            f.write(orjson.dumps(rec).decode() + '\\n')\n",
    "            chunk_count += 1\n",
    "\n",
    "print(f\"RAG JSONL created: {rag_path}\")\n",
    "print(f\"Documents processed: {doc_count}\")\n",
    "print(f\"Total chunks created: {chunk_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5aa13",
   "metadata": {},
   "source": [
    "### D3. Build an SFT/Eval JSONL view (instruction → output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6da07dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT/Eval JSONL created: artifacts/jsonl/corpus_sft.jsonl\n",
      "Total instruction-output pairs: 120\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "sample = clean.sample(min(120, len(clean)), random_state=7)\n",
    "sft_path = Path('artifacts/jsonl/corpus_sft.jsonl')\n",
    "sft_count = 0\n",
    "\n",
    "with sft_path.open('w', encoding='utf-8') as f:\n",
    "    for _, r in sample.iterrows():\n",
    "        prompt = f\"Summarize the key steps from: {r['title']} ({r['section']}).\"\n",
    "        # Simple templated target; in real life, use human-authored or heuristic extraction\n",
    "        target = \"Key steps: enable SAML; map claims; verify time sync; check audience URI; review settings.\"\n",
    "        obj = {\n",
    "            'input': prompt, \n",
    "            'output': target,\n",
    "            'metadata': {\n",
    "                'doc_id': r['doc_id'], \n",
    "                'type': r['type'], \n",
    "                'lang': r['language']\n",
    "            }\n",
    "        }\n",
    "        f.write(orjson.dumps(obj).decode() + '\\n')\n",
    "        sft_count += 1\n",
    "\n",
    "print(f\"SFT/Eval JSONL created: {sft_path}\")\n",
    "print(f\"Total instruction-output pairs: {sft_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08322e8f",
   "metadata": {},
   "source": [
    "### D4. Validate JSONL & write reviewer samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c4e9a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "JSONL VALIDATION RESULTS\n",
      "============================================================\n",
      "\n",
      "File: artifacts/jsonl/rag_chunks_from_csv.jsonl\n",
      "  Total lines: 295\n",
      "  Bad lines: 0\n",
      "  ✓ All lines valid!\n",
      "\n",
      "File: artifacts/jsonl/corpus_sft.jsonl\n",
      "  Total lines: 120\n",
      "  Bad lines: 0\n",
      "  ✓ All lines valid!\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import itertools\n",
    "\n",
    "def validate_jsonl(path):\n",
    "    bad = 0\n",
    "    total = 0\n",
    "    errors = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            total += 1\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                assert isinstance(obj, dict)\n",
    "            except Exception as e:\n",
    "                bad += 1\n",
    "                if len(errors) < 5:  # Keep first 5 errors\n",
    "                    errors.append(f\"Line {i}: {str(e)}\")\n",
    "    return total, bad, errors\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"JSONL VALIDATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for p in ['artifacts/jsonl/rag_chunks_from_csv.jsonl','artifacts/jsonl/corpus_sft.jsonl']:\n",
    "    t, b, errs = validate_jsonl(p)\n",
    "    print(f\"\\nFile: {p}\")\n",
    "    print(f\"  Total lines: {t}\")\n",
    "    print(f\"  Bad lines: {b}\")\n",
    "    if b > 0:\n",
    "        print(f\"  Sample errors:\")\n",
    "        for err in errs:\n",
    "            print(f\"    {err}\")\n",
    "    else:\n",
    "        print(f\"  ✓ All lines valid!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d358fc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviewer samples created: artifacts/samples/jsonl_samples.jsonl\n",
      "\n",
      "Showing first 3 lines from each JSONL file:\n",
      "============================================================\n",
      "# Samples from artifacts/jsonl/rag_chunks_from_csv.jsonl\n",
      "{\"doc_id\":\"DOC-0879\",\"chunk_id\":\"DOC-0879-0000\",\"text\":\"This article explains how to configure single sign-on with step-by-step instructions. Use the admin console to enable SAML and verify claim mappings. Common pitfalls include clock skew and incorrect audience URIs. This article explains how to configure single sign-on with step-by-step instructions. Use the admin console to enable SAML and verify claim mappings. Common pitfalls include clock skew and incorrect audience URIs. This article explains how to configure single sign-on with step-by-step instructions. Use the admin console to enable SAML and verify claim mappings. Common pitfalls include clock skew and incorrect audience URIs. Additional details about export and rate_limits.\",\"metadata\":{\"title\":\"FAQ: Billing\",\"section\":\"Overview\",\"tags\":\"privacy,compliance,rate_limits,billing\",\"source_url\":\"https://example.local/faq/doc-0879\",\"language\":\"en\",\"confidentiality\":\"public\",\"schema_version\":\"rag-chunk-v1\"}}\n",
      "{\"doc_id\":\"DOC-0302\",\"chunk_id\":\"DOC-0302-0000\",\"text\":\"This article explains how to configure single sign-on with step-by-step instructions. Use the admin console to enable SAML and verify claim mappings. Common pitfalls include clock skew and incorrect audience URIs. Additional details about retention and retention.\",\"metadata\":{\"title\":\"FAQ: Privacy\",\"section\":\"Overview\",\"tags\":\"sso,privacy\",\"source_url\":\"https://example.local/faq/doc-0302\",\"language\":\"en\",\"confidentiality\":\"public\",\"schema_version\":\"rag-chunk-v1\"}}\n",
      "{\"doc_id\":\"DOC-0813\",\"chunk_id\":\"DOC-0813-0000\",\"text\":\"This article explains how to configure single sign-on with step-by-step instructions. Use the admin console to enable SAML and verify claim mappings. Common pitfalls include clock skew and incorrect audience URIs. This article explains how to configure single sign-on with step-by-step instructions. Use the admin console to enable SAML and verify claim mappings. Common pitfalls inc\n",
      "... (truncated)\n"
     ]
    }
   ],
   "source": [
    "# Create small reviewer sample\n",
    "sample_path = Path('artifacts/samples/jsonl_samples.jsonl')\n",
    "with sample_path.open('w', encoding='utf-8') as out:\n",
    "    for p in ['artifacts/jsonl/rag_chunks_from_csv.jsonl','artifacts/jsonl/corpus_sft.jsonl']:\n",
    "        out.write(f\"# Samples from {p}\\n\")\n",
    "        with open(p, 'r', encoding='utf-8') as f:\n",
    "            for line in itertools.islice(f, 3):\n",
    "                out.write(line)\n",
    "        out.write(\"\\n\")\n",
    "\n",
    "print(f\"Reviewer samples created: {sample_path}\")\n",
    "print(\"\\nShowing first 3 lines from each JSONL file:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with sample_path.open('r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    print(content[:2000])  # Show first 2000 chars\n",
    "    if len(content) > 2000:\n",
    "        print(\"... (truncated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944ff2e",
   "metadata": {},
   "source": [
    "## Part E — Wrap-Up\n",
    "\n",
    "### Analysis Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a9d93",
   "metadata": {},
   "source": [
    "#### 1. Three reasons JSONL is preferred vs a single JSON array for LLM corpora\n",
    "\n",
    "1. **Streaming & Memory Efficiency**: JSONL allows line-by-line processing without loading the entire dataset into memory. This is critical for large corpora (GB-TB scale) that would otherwise cause OOM errors with a single JSON array.\n",
    "\n",
    "2. **Append-Friendly & Incremental Updates**: New records can be appended to JSONL files without parsing/rewriting the entire file. This enables continuous ingestion pipelines and incremental updates to training data.\n",
    "\n",
    "3. **Recovery & Parallelization**: Each line is a self-contained JSON object, making it easy to recover from partial failures (skip corrupted lines), parallelize processing with map-reduce frameworks, and distribute chunks across workers without complex parsing logic.\n",
    "\n",
    "#### 2. Governance + Language filters applied\n",
    "\n",
    "**Filters Applied:**\n",
    "- **Governance Filter**: `confidentiality == 'public'` - Removed all internal/confidential documents to ensure only public-facing content enters the RAG index\n",
    "- **Language Filter**: `language == 'en'` - Kept only English documents for a monolingual RAG system\n",
    "\n",
    "**Stage:** Applied after initial normalization (Part C2) but before missing value handling and deduplication. This order is efficient because:\n",
    "- We normalize first to ensure consistent filtering\n",
    "- We filter early to reduce processing overhead for subsequent steps\n",
    "- We dedupe after filtering since governance/language changes the candidate set\n",
    "\n",
    "#### 3. Deduplication Strategy\n",
    "\n",
    "**Method:** Content-based deduplication using SHA-1 hash of normalized `title + body_text`\n",
    "\n",
    "**Key Used:** `content_key = sha1(normalize(title + ' ' + body_text).lower())`\n",
    "\n",
    "**Why This Approach:**\n",
    "- **Content-focused**: Using `doc_id` alone would miss semantic duplicates with different IDs (like our deliberate `DOC-DUP-*` entries)\n",
    "- **Normalization**: Lowercasing and whitespace normalization ensures minor formatting differences don't create false negatives\n",
    "- **Latest-wins**: When duplicates are found, we keep the record with the most recent `updated_at` timestamp, preserving the freshest content\n",
    "- **Efficiency**: SHA-1 provides fast hashing with negligible collision risk for our corpus size\n",
    "\n",
    "#### 4. Essential Metadata: RAG vs SFT\n",
    "\n",
    "**RAG-Essential Metadata:**\n",
    "- `doc_id` & `chunk_id`: Unique identification for retrieval and citation\n",
    "- `title`, `section`, `tags`: Semantic filtering and ranking (e.g., \"only return security-related chunks\")\n",
    "- `source_url`: Provenance for user citations and verification\n",
    "- `language`, `confidentiality`: Runtime filtering for multi-tenant or multi-lingual systems\n",
    "- `schema_version`: Schema evolution tracking\n",
    "\n",
    "**SFT/Eval-Essential Metadata:**\n",
    "- `doc_id`: Traceability back to source material\n",
    "- `type`: Document type helps balance training distribution (e.g., ensure mix of help_articles, policies, FAQs)\n",
    "- `lang`: Language-specific fine-tuning or eval splits\n",
    "- Optional: `created_at`/`updated_at` for temporal splits (train on older data, eval on recent)\n",
    "\n",
    "**Key Difference:** RAG needs rich filtering metadata for retrieval-time decisions, while SFT needs distribution/provenance metadata for training quality and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e3d7a",
   "metadata": {},
   "source": [
    "## Final Outputs Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89886ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL OUTPUTS SUMMARY\n",
      "============================================================\n",
      "✓ Source CSV corpus\n",
      "  Path: data/corpus_llm.csv\n",
      "  Size: 635,990 bytes\n",
      "\n",
      "✓ FastAPI corpus server\n",
      "  Path: tools/corpus_api.py\n",
      "  Size: 1,792 bytes\n",
      "\n",
      "✓ RAG chunks JSONL\n",
      "  Path: artifacts/jsonl/rag_chunks_from_csv.jsonl\n",
      "  Size: 224,444 bytes\n",
      "\n",
      "✓ SFT/Eval JSONL\n",
      "  Path: artifacts/jsonl/corpus_sft.jsonl\n",
      "  Size: 29,315 bytes\n",
      "\n",
      "✓ Reviewer samples\n",
      "  Path: artifacts/samples/jsonl_samples.jsonl\n",
      "  Size: 3,338 bytes\n",
      "\n",
      "============================================================\n",
      "Lab 13 Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL OUTPUTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "outputs = [\n",
    "    ('data/corpus_llm.csv', 'Source CSV corpus'),\n",
    "    ('tools/corpus_api.py', 'FastAPI corpus server'),\n",
    "    ('artifacts/jsonl/rag_chunks_from_csv.jsonl', 'RAG chunks JSONL'),\n",
    "    ('artifacts/jsonl/corpus_sft.jsonl', 'SFT/Eval JSONL'),\n",
    "    ('artifacts/samples/jsonl_samples.jsonl', 'Reviewer samples')\n",
    "]\n",
    "\n",
    "for path, desc in outputs:\n",
    "    p = Path(path)\n",
    "    if p.exists():\n",
    "        if p.is_file():\n",
    "            size = p.stat().st_size\n",
    "            print(f\"✓ {desc}\")\n",
    "            print(f\"  Path: {path}\")\n",
    "            print(f\"  Size: {size:,} bytes\")\n",
    "        else:\n",
    "            print(f\"✓ {desc}\")\n",
    "            print(f\"  Path: {path}\")\n",
    "    else:\n",
    "        print(f\"✗ {desc} - NOT FOUND\")\n",
    "        print(f\"  Expected: {path}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Lab 13 Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ece7ad",
   "metadata": {},
   "source": [
    "## Bonus: Gzip Compression (Optional)\n",
    "\n",
    "For production use, compress JSONL files to save storage and bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3b49436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 224,444 bytes\n",
      "Compressed size: 8,793 bytes\n",
      "Compression ratio: 96.1%\n",
      "Compressed file: artifacts/jsonl/rag_chunks_from_csv.jsonl.gz\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "# Compress the RAG chunks JSONL\n",
    "rag_jsonl = 'artifacts/jsonl/rag_chunks_from_csv.jsonl'\n",
    "rag_gz = 'artifacts/jsonl/rag_chunks_from_csv.jsonl.gz'\n",
    "\n",
    "with open(rag_jsonl, 'r', encoding='utf-8') as src, \\\n",
    "     gzip.open(rag_gz, 'wt', encoding='utf-8') as dst:\n",
    "    for line in src: \n",
    "        dst.write(line)\n",
    "\n",
    "original_size = Path(rag_jsonl).stat().st_size\n",
    "compressed_size = Path(rag_gz).stat().st_size\n",
    "compression_ratio = (1 - compressed_size / original_size) * 100\n",
    "\n",
    "print(f\"Original size: {original_size:,} bytes\")\n",
    "print(f\"Compressed size: {compressed_size:,} bytes\")\n",
    "print(f\"Compression ratio: {compression_ratio:.1f}%\")\n",
    "print(f\"Compressed file: {rag_gz}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
