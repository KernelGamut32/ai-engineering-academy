{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 16 — RAG Data Prep: Chunking, Overlap, and Metadata\n",
    "\n",
    "**Focus Area:** Chunking strategies (sentences, headings, tokens), overlap tuning, metadata (source, timestamp, section), and careful text cleaning (retain meaning‑bearing punctuation)\n",
    "\n",
    "> This lab **builds on Labs 13-15**. You will transform your cleaned LLM corpus into a high‑quality RAG index input by experimenting with **three chunkers** (sentence‑window, heading‑aware, token‑bounded), adding provenance‑rich metadata, and validating that punctuation needed for semantics (e.g., dates, decimals, clause boundaries) is preserved.\n",
    "\n",
    "---\n",
    "\n",
    "## Outcomes\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Implement **sentence‑window**, **heading‑aware**, and **token‑bounded** chunkers with configurable **overlap**.  \n",
    "2. Attach consistent **metadata** (source, filename/URI, section, timestamps, schema version) to each chunk.  \n",
    "3. Apply **cleaners** that normalize whitespace without removing **meaning‑bearing punctuation**.  \n",
    "4. Generate **JSONL shards** for embeddings/retrieval and validate them line‑by‑line.  \n",
    "5. Compare chunk distributions and select defaults for your project.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites & Setup\n",
    "\n",
    "- Python 3.13 with `pandas`, `regex`, `orjson`, `numpy`, `tqdm`, `tokenizers` (re‑use tiny BPE from **Lab 15**)  \n",
    "- **Artifacts from prior labs:**  \n",
    "  - `artifacts/jsonl/rag_chunks_from_csv.jsonl` (Lab 13) — optional baseline  \n",
    "  - `artifacts/tokenizer/bytebpe.json` (Lab 15) — used for token counting  \n",
    "  - (If starting from raw text) `data/text/` directory as in Lab 13\n",
    "\n",
    "**Start a notebook:** `week02_lab16.ipynb`\n",
    "\n",
    "Create directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "for p in ['artifacts/rag','artifacts/samples','artifacts/stats']:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part A — Cleaners that Preserve Meaning\n",
    "\n",
    "### A1. Normalize whitespace but **keep** punctuation that carries semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    # Collapse whitespace but keep punctuation (.,:;?!%$-@/&) and digits\n",
    "    # Preserve newlines as soft boundaries for heading detection\n",
    "    s = s.replace('\\r', '')\n",
    "    # Normalize multiple spaces but keep single spaces and newlines\n",
    "    s = re.sub(r\"[\\t ]+\", \" \", s)\n",
    "    s = re.sub(r\" *\\n *\", \"\\n\", s)\n",
    "    # Strip leading/trailing whitespace lines\n",
    "    s = s.strip('\\n ')\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Quick before/after sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price was $1,234.50 on 2025-01-03\n",
      "\n",
      "See Section 2.1: Rate Limits? Yes!\n"
     ]
    }
   ],
   "source": [
    "raw = \"Price was $1,234.50 on 2025-01-03\\n\\nSee Section 2.1: Rate Limits? Yes!\"\n",
    "print(normalize_text(raw))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why:** In RAG, punctuation like decimals, hyphens, colons, and question marks often **change meaning**. Avoid `re.sub(r\"\\W\", \" \", text)`‑style cleaners that strip them.\n",
    "\n",
    "---\n",
    "\n",
    "## Part B — Chunkers\n",
    "\n",
    "We’ll implement three chunking strategies with **overlap**. Choose one as your default and compare stats.\n",
    "\n",
    "### B1. Sentence‑window chunker (regex‑based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from typing import List\n",
    "\n",
    "# Lightweight sentence splitter (no external models)\n",
    "sent_split = re.compile(r\"(?<=\\S[\\.!?])\\s+(?=[A-Z0-9])\")\n",
    "\n",
    "def to_sentences(text: str) -> List[str]:\n",
    "    text = normalize_text(text)\n",
    "    sents = sent_split.split(text)\n",
    "    # Merge very short trailing sentences into previous\n",
    "    merged = []\n",
    "    for s in sents:\n",
    "        s = s.strip()\n",
    "        if not s: continue\n",
    "        if merged and len(s) < 40:  # chars\n",
    "            merged[-1] += \" \" + s\n",
    "        else:\n",
    "            merged.append(s)\n",
    "    return merged\n",
    "\n",
    "# Sliding window over sentences\n",
    "from itertools import islice\n",
    "\n",
    "def chunk_by_sentences(text: str, window: int = 5, overlap: int = 2):\n",
    "    sents = to_sentences(text)\n",
    "    i = 0\n",
    "    while i < len(sents):\n",
    "        end = min(i + window, len(sents))\n",
    "        yield \" \".join(sents[i:end])\n",
    "        if end == len(sents):\n",
    "            break\n",
    "        i = max(0, end - overlap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Heading‑aware chunker (Markdown/plaintext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "heading_re = re.compile(r\"^(?:\\s*#+\\s+.+|\\s*[A-Z][A-Z0-9 .:/-]{3,}$)\", re.M)\n",
    "\n",
    "def split_by_headings(text: str):\n",
    "    text = normalize_text(text)\n",
    "    # Find heading spans\n",
    "    spans = [(m.start(), m.end()) for m in heading_re.finditer(text)]\n",
    "    if not spans:\n",
    "        return [text]\n",
    "    chunks = []\n",
    "    pos = 0\n",
    "    for start, end in spans:\n",
    "        if start > pos:\n",
    "            chunks.append(text[pos:start].strip('\\n '))\n",
    "        pos = start\n",
    "    chunks.append(text[pos:].strip('\\n '))\n",
    "    # Post‑process: attach small sections to neighbors\n",
    "    merged = []\n",
    "    for ch in chunks:\n",
    "        if not ch: continue\n",
    "        if merged and len(ch) < 300:\n",
    "            merged[-1] += \"\\n\" + ch\n",
    "        else:\n",
    "            merged.append(ch)\n",
    "    return merged\n",
    "\n",
    "# Add overlap at paragraph granularity\n",
    "def chunk_by_headings(text: str, overlap_chars: int = 1500, max_chars: int = 3000):\n",
    "    for section in split_by_headings(text):\n",
    "        start = 0\n",
    "        while start < len(section):\n",
    "            end = min(start + max_chars, len(section))\n",
    "            yield section[start:end]\n",
    "            if end == len(section):\n",
    "                break\n",
    "            start = max(0, end - overlap_chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Token‑bounded chunker (uses tokenizer from Lab 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "tok = Tokenizer.from_file('artifacts/tokenizer/bytebpe.json')\n",
    "\n",
    "def token_len(s: str) -> int:\n",
    "    return len(tok.encode(s).ids)\n",
    "\n",
    "# Greedy grow with token overlap\n",
    "\n",
    "def chunk_by_tokens(text: str, max_tokens: int = 300, overlap_tokens: int = 60):\n",
    "    text = normalize_text(text)\n",
    "    words = text.split(' ')\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        cur = []\n",
    "        cur_len = 0\n",
    "        j = i\n",
    "        while j < len(words):\n",
    "            candidate = (\" \".join(cur + [words[j]])).strip()\n",
    "            if token_len(candidate) > max_tokens:\n",
    "                break\n",
    "            cur.append(words[j]); j += 1\n",
    "        chunk = \" \".join(cur)\n",
    "        if not chunk:\n",
    "            # fallback: force‑add a single long word\n",
    "            chunk = words[j]\n",
    "            j += 1\n",
    "        yield chunk\n",
    "        if j >= len(words):\n",
    "            break\n",
    "        # step back by overlap tokens\n",
    "        # approximate by words: walk backward until token budget ~ overlap\n",
    "        back = 0\n",
    "        while back < len(cur) and token_len(\" \".join(cur[-(back+1):])) < overlap_tokens:\n",
    "            back += 1\n",
    "        i = max(i + len(cur) - back, i + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part C — Build RAG Chunks + Metadata + Shards\n",
    "\n",
    "We’ll read from **either** a directory of text files (`data/text/`) or prior **Lab 13** JSONL (use its `text` and `metadata.source`).\n",
    "\n",
    "### C1. Source loader (text dir or JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, orjson\n",
    "\n",
    "USE_JSONL = Path('artifacts/jsonl/rag_chunks_from_csv.jsonl').exists()\n",
    "\n",
    "def iter_documents():\n",
    "    if USE_JSONL:\n",
    "        # Each line: {doc_id, chunk_id?, text?, metadata{source?...}}\n",
    "        with open('artifacts/jsonl/rag_chunks_from_csv.jsonl','r',encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                yield obj.get('metadata',{}).get('source', 'jsonl://rag_chunks_from_csv'), obj.get('text','')\n",
    "    else:\n",
    "        for path in Path('data/text').glob('*'):\n",
    "            yield str(path), Path(path).read_text(encoding='utf-8', errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2. Choose a chunker & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKER = 'tokens'   # choose: 'sentences' | 'headings' | 'tokens'\n",
    "PARAMS  = {\n",
    "    'sentences': {'window': 6, 'overlap': 2},\n",
    "    'headings':  {'max_chars': 2600, 'overlap_chars': 500},\n",
    "    'tokens':    {'max_tokens': 320, 'overlap_tokens': 64}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3. Emit JSONL shards with provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "295it [00:02, 110.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "schema = 'rag-chunk-v2'\n",
    "max_shard_bytes = 50_000_000\n",
    "cur = 0; idx = 0\n",
    "\n",
    "out = open(f'artifacts/rag/rag_{CHUNKER}_{idx:03d}.jsonl','w',encoding='utf-8')\n",
    "\n",
    "def write_line(obj):\n",
    "    global cur, idx, out\n",
    "    line = orjson.dumps(obj).decode() + '\\n'\n",
    "    if cur + len(line.encode('utf-8')) > max_shard_bytes:\n",
    "        out.close(); idx += 1; cur = 0\n",
    "        out = open(f'artifacts/rag/rag_{CHUNKER}_{idx:03d}.jsonl','w',encoding='utf-8')\n",
    "    out.write(line); cur += len(line.encode('utf-8'))\n",
    "\n",
    "for source, text in tqdm(iter_documents()):\n",
    "    text = normalize_text(text)\n",
    "    # choose chunker\n",
    "    if CHUNKER == 'sentences':\n",
    "        chunks = list(chunk_by_sentences(text, **PARAMS['sentences']))\n",
    "    elif CHUNKER == 'headings':\n",
    "        chunks = list(chunk_by_headings(text, **PARAMS['headings']))\n",
    "    else:\n",
    "        chunks = list(chunk_by_tokens(text, **PARAMS['tokens']))\n",
    "\n",
    "    # stable doc id from source path\n",
    "    doc_id = hashlib.sha1(source.encode('utf-8')).hexdigest()[:16]\n",
    "    for j, ch in enumerate(chunks):\n",
    "        meta = {\n",
    "            'source': source,\n",
    "            'schema_version': schema,\n",
    "            'created_at': datetime.now(timezone.utc).isoformat(),\n",
    "            'chunk_index': j,\n",
    "            'n_chars': len(ch),\n",
    "            'chunker': CHUNKER\n",
    "        }\n",
    "        rec = {\n",
    "            'doc_id': doc_id,\n",
    "            'chunk_id': f'{doc_id}-{j:04d}',\n",
    "            'text': ch,\n",
    "            'metadata': meta\n",
    "        }\n",
    "        write_line(rec)\n",
    "\n",
    "out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Count shards/lines and show 3 example chunks with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHARDS: ['artifacts/rag/rag_tokens_000.jsonl']\n",
      "TOTAL LINES: 582\n",
      "{\"doc_id\":\"5631b16dc265bbf2\",\"chunk_id\":\"5631b16dc265bbf2-0000\",\"text\":\"This article explains how to configure single sign-on with step-by-step instructions. Use the admin console to enable SAML and verify claim mappings. Common pitfalls include clock skew and incorrect audience URIs. This article explains how to configure single sign-on with step-by-step instructions. Use the admin console to enable SAML and verify\",\"metadata\":{\"source\":\"jsonl://rag_chunks_from_csv\",\"schema_version\":\"rag-chunk-v2\",\"created_at\":\"2025-10-30T01:44:37.982746+00:00\",\"chunk_index\":0,\"n_chars\":347,\"chunker\":\"tokens\"}}\n",
      "\n",
      "{\"doc_id\":\"5631b16dc265bbf2\",\"chunk_id\":\"5631b16dc265bbf2-0001\",\"text\":\"instructions. Use the admin console to enable SAML and verify claim mappings. Common pitfalls include clock skew and incorrect audience URIs. This article explains how to configure single sign-on with step-by-step instructions. Use the admin console to enable SAML and verify claim mappings. Common pitfalls include clock skew and incorrect\",\"metadata\":{\"source\":\"jsonl://rag_chunks_from_csv\",\"schema_version\":\"rag-chunk-v2\",\"created_at\":\"2025-10-30T01:44:37.982808+00:00\",\"chunk_index\":1,\"n_chars\":340,\"chunker\":\"tokens\"}}\n",
      "\n",
      "{\"doc_id\":\"5631b16dc265bbf2\",\"chunk_id\":\"5631b16dc265bbf2-0002\",\"text\":\"claim mappings. Common pitfalls include clock skew and incorrect audience URIs. Additional details about export and rate_limits.\",\"metadata\":{\"source\":\"jsonl://rag_chunks_from_csv\",\"schema_version\":\"rag-chunk-v2\",\"created_at\":\"2025-10-30T01:44:37.982822+00:00\",\"chunk_index\":2,\"n_chars\":128,\"chunker\":\"tokens\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import itertools\n",
    "\n",
    "files = sorted(glob(f'artifacts/rag/rag_{CHUNKER}_*.jsonl'))\n",
    "print('SHARDS:', files)\n",
    "lines = sum(1 for p in files for _ in open(p,'r',encoding='utf-8'))\n",
    "print('TOTAL LINES:', lines)\n",
    "\n",
    "with open(files[0],'r',encoding='utf-8') as f:\n",
    "    print('\\n'.join(list(itertools.islice(f, 3))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part D — Validate, Compare, and Choose Defaults\n",
    "\n",
    "### D1. Line validator (presence + minimal length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file': 'artifacts/rag/rag_tokens_000.jsonl', 'total': 582, 'bad': 0}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def validate_jsonl(path):\n",
    "    total = bad = 0\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                ok = isinstance(obj, dict) and 'text' in obj and 'metadata' in obj and len(obj['text']) >= 40\n",
    "                if not ok: bad += 1\n",
    "            except Exception:\n",
    "                bad += 1\n",
    "    return total, bad\n",
    "\n",
    "stats = []\n",
    "for p in files:\n",
    "    t,b = validate_jsonl(p)\n",
    "    stats.append({'file': p, 'total': t, 'bad': b})\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2. Distribution stats by chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 582,\n",
       " 'mean_chars': 267.6975945017182,\n",
       " 'p50': 265.5,\n",
       " 'p90': 347.0,\n",
       " 'p99': 347.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "lens = []\n",
    "for p in files:\n",
    "    with open(p,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            lens.append(len(obj['text']))\n",
    "\n",
    "import math\n",
    "summary = {\n",
    "    'count': len(lens),\n",
    "    'mean_chars': float(np.mean(lens)) if lens else 0,\n",
    "    'p50': float(np.percentile(lens, 50)) if lens else 0,\n",
    "    'p90': float(np.percentile(lens, 90)) if lens else 0,\n",
    "    'p99': float(np.percentile(lens, 99)) if lens else 0,\n",
    "}\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D3. Token budget check (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_tokens': 248.24, 'p95_tokens': 320.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "tok = Tokenizer.from_file('artifacts/tokenizer/bytebpe.json')\n",
    "\n",
    "sample_tokens = []\n",
    "for p in files[:1]:\n",
    "    with open(p,'r',encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 200: break\n",
    "            text = json.loads(line)['text']\n",
    "            sample_tokens.append(len(tok.encode(text).ids))\n",
    "\n",
    "{'mean_tokens': float(np.mean(sample_tokens)), 'p95_tokens': float(np.percentile(sample_tokens,95))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Using your model’s max context, decide: Do you need shorter chunks or less overlap? Note the trade‑off: retrieval recall vs embedding cost.\n",
    "\n",
    "---\n",
    "\n",
    "## Wrap‑Up\n",
    "\n",
    "Add a markdown cell and answer:\n",
    "\n",
    "1. Which chunker and parameters did you choose (window/overlap or max_tokens/overlap_tokens) and **why**?  \n",
    "2. Which metadata fields are essential for your retriever (e.g., `source`, `section`, `created_at`)?  \n",
    "3. Show validator results (bad/total). What rules would you tighten before production?  \n",
    "4. If a downstream embedder requires sentence boundaries, how would you modify your chunker?\n",
    "\n",
    "Confirm outputs:\n",
    "\n",
    "- `artifacts/rag/rag_<CHUNKER>_*.jsonl`  \n",
    "- `artifacts/samples/` (optional samples you created)  \n",
    "- `artifacts/stats/` (optional summary you saved)\n",
    "\n",
    "---\n",
    "\n",
    "- **Common pitfalls:** Over‑aggressive cleaning that kills punctuation; no overlap leading to context loss; metadata gaps; shards without validation.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution Snippets (reference)\n",
    "\n",
    "**Switch chunkers quickly:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CHUNKER in ['sentences','headings','tokens']:\n",
    "    PARAMS = {\n",
    "        'sentences': {'window': 6, 'overlap': 2},\n",
    "        'headings':  {'max_chars': 2600, 'overlap_chars': 500},\n",
    "        'tokens':    {'max_tokens': 320, 'overlap_tokens': 64}\n",
    "    }\n",
    "    # re‑run Part C with chosen CHUNKER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample a few lines from every shard:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts/rag/rag_tokens_000.jsonl\n",
      "{\"doc_id\":\"5631b16dc265bbf2\",\"chunk_id\":\"5631b16dc265bbf2-0000\",\"text\":\"This article explains how to configure single sign-on with step-by-step instructions. Use the admin console to enable SAML and verify claim mappings. Common pitfalls include clock skew and incorrect audience URIs. This article explains how to configure single sign-on with step-by-step instructions. Use the admin console to enable SAML and verify\",\"metadata\":{\"source\":\"jsonl://rag_chunks_from_csv\",\"schema_version\":\"rag-chunk-v2\",\"created_at\":\"2025-10-30T01:44:37.982746+00:00\",\"chunk_index\":0,\"n_chars\":347,\"chunker\":\"tokens\"}}\n",
      "{\"doc_id\":\"5631b16dc265bbf2\",\"chunk_id\":\"5631b16dc265bbf2-0001\",\"text\":\"instructions. Use the admin console to enable SAML and verify claim mappings. Common pitfalls include clock skew and incorrect audience URIs. This article explains how to configure single sign-on with step-by-step instructions. Use the admin console to enable SAML and verify claim mappings. Common pitfalls include clock skew and incorrect\",\"metadata\":{\"source\":\"jsonl://rag_chunks_from_csv\",\"schema_version\":\"rag-chunk-v2\",\"created_at\":\"2025-10-30T01:44:37.982808+00:00\",\"chunk_index\":1,\"n_chars\":340,\"chunker\":\"tokens\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools, glob\n",
    "for p in sorted(glob.glob('artifacts/rag/rag_*_*.jsonl')):\n",
    "    with open(p,'r',encoding='utf-8') as f:\n",
    "        print(p); print(''.join(list(itertools.islice(f,2))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minimal dedupe by content hash (optional):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib, json\n",
    "seen = set()\n",
    "\n",
    "def dedupe_jsonl(inp, out):\n",
    "    with open(inp,'r',encoding='utf-8') as f, open(out,'w',encoding='utf-8') as g:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            key = hashlib.sha1(obj['text'].strip().lower().encode('utf-8')).hexdigest()\n",
    "            if key in seen: continue\n",
    "            seen.add(key)\n",
    "            g.write(line)\n",
    "# dedupe_jsonl('artifacts/rag/rag_tokens_000.jsonl','artifacts/rag/rag_tokens_000_dedup.jsonl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
