{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea54965",
   "metadata": {},
   "source": [
    "# Lab 03 — Resilient API Harvester + SQL Extractor with Provenance\n",
    "\n",
    "**Focus Areas:** API Harvester (rate‑limited, paginated, with retries & snapshots) and SQL Extractor (parameterized, chunked, Parquet)\n",
    "\n",
    "See **Appendices** below for additional information on what *with Provenance* means.\n",
    "\n",
    "---\n",
    "\n",
    "## Outcomes\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Run a **local REST API** backed by SQLite (Datasette) and a lightweight **auth + rate‑limit proxy** that enforces Bearer tokens and emits `429` with `Retry-After`.\n",
    "2. Implement a production‑style **API harvester** that supports **query params**, **cursor pagination**, **exponential backoff with jitter**, and **idempotent incremental fetch**.\n",
    "3. Persist **raw JSON snapshots** per page with a **provenance manifest** (source URL, params, status, checksum, timestamp).\n",
    "4. Normalize harvested JSON into pandas DataFrames and write to **Parquet** (optionally partitioned) for downstream validation/profiling.\n",
    "5. Use **parameterized SQL** against SQLite via `pd.read_sql_query` with **chunked reads** and append to Parquet in a streaming fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fc24df",
   "metadata": {},
   "source": [
    "## Prerequisites & Setup\n",
    "\n",
    "- Python 3.13 with `requests`, `pandas`, `numpy`, `pyarrow`, `datasette`, `fastapi`, `uvicorn`, `httpx`, `aiofiles` installed (proxy uses FastAPI).\n",
    "- JupyterLab or VS Code with Jupyter extension.\n",
    "- Completion of Lab 02 (reuse of **SQLite** setup).\n",
    "\n",
    "### Setup Steps\n",
    "\n",
    "1. Create a clean workspace and env\n",
    "2. Reuse / obtain the SQLite DB (Northwind)\n",
    "3. Start Datasette on port 8001 (same as Lab 02)\n",
    "4. Start an **Auth + Rate‑Limit Proxy** (FastAPI) on port 8002\n",
    "5. Start this notebook\n",
    "\n",
    "### 1) Create a clean workspace and env\n",
    "\n",
    "```bash\n",
    "mkdir -p lab03 && cd lab03\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate   # Windows: .venv\\Scripts\\activate\n",
    "pip install --upgrade pip\n",
    "pip install requests pandas numpy pyarrow datasette fastapi uvicorn aiofiles httpx\n",
    "```\n",
    "\n",
    "### 2) Reuse / obtain the SQLite DB (Northwind)\n",
    "\n",
    "```bash\n",
    "# If you already have northwind.db from Lab 02, copy it here. Otherwise:\n",
    "curl -L -o northwind.db \\\n",
    "  https://raw.githubusercontent.com/jpwhite3/northwind-SQLite3/main/dist/northwind.db\n",
    "```\n",
    "\n",
    "### 3) Start Datasette on port 8001 (same as Lab 02)\n",
    "\n",
    "```bash\n",
    "datasette northwind.db -h 127.0.0.1 -p 8001\n",
    "```\n",
    "\n",
    "Leave this running. Open a new terminal for the **proxy**.\n",
    "\n",
    "### 4) Start an **Auth + Rate‑Limit Proxy** (FastAPI) on port 8002\n",
    "\n",
    "Create `proxy.py` with the following contents:\n",
    "\n",
    "```python\n",
    "# proxy.py — Bearer auth + fixed-window rate limit + pass-through to Datasette\n",
    "import time, asyncio, os\n",
    "from typing import Optional\n",
    "from fastapi import FastAPI, Request, Response, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "import httpx\n",
    "\n",
    "DATASETTE = os.getenv(\"UPSTREAM\", \"http://127.0.0.1:8001\")\n",
    "REQUIRED_TOKEN = os.getenv(\"API_TOKEN\", \"super-secret-token\")\n",
    "RATE_LIMIT = int(os.getenv(\"RATE_LIMIT\", 60))  # requests per minute per token\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# simple in-memory counters (sufficient for lab)\n",
    "_counters = {}\n",
    "_window_starts = {}\n",
    "\n",
    "async def check_rate_limit(token: str) -> Optional[int]:\n",
    "    now = int(time.time())\n",
    "    window = now // 60\n",
    "    key = (token, window)\n",
    "    if _window_starts.get(key) is None:\n",
    "        _window_starts[key] = window\n",
    "        _counters[key] = 0\n",
    "    _counters[key] += 1\n",
    "    remaining = RATE_LIMIT - _counters[key]\n",
    "    if remaining < 0:\n",
    "        reset = (window + 1) * 60 - now\n",
    "        return reset\n",
    "    return None\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def enforce_auth_and_rate_limit(request: Request, call_next):\n",
    "    # enforce bearer token\n",
    "    auth = request.headers.get(\"authorization\", \"\")\n",
    "    if not auth.lower().startswith(\"bearer \"):\n",
    "        return JSONResponse({\"error\": \"missing bearer token\"}, status_code=401)\n",
    "    token = auth.split(\" \", 1)[1]\n",
    "    if token != REQUIRED_TOKEN:\n",
    "        return JSONResponse({\"error\": \"invalid token\"}, status_code=403)\n",
    "\n",
    "    # rate limit\n",
    "    reset = await check_rate_limit(token)\n",
    "    if reset is not None:\n",
    "        headers = {\"Retry-After\": str(reset), \"X-RateLimit-Reset\": str(reset)}\n",
    "        return JSONResponse({\"error\": \"rate limit exceeded\"}, status_code=429, headers=headers)\n",
    "\n",
    "    return await call_next(request)\n",
    "\n",
    "@app.api_route(\"/{path:path}\", methods=[\"GET\"])\n",
    "async def proxy(path: str, request: Request):\n",
    "    # Very small pass-through for GET to Datasette\n",
    "    params = dict(request.query_params)\n",
    "    upstream_url = f\"{DATASETTE}/{path}\"\n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        r = await client.get(upstream_url, params=params)\n",
    "        return Response(content=r.content, status_code=r.status_code, headers=dict(r.headers), media_type=r.headers.get(\"content-type\"))\n",
    "```\n",
    "\n",
    "Run the proxy:\n",
    "\n",
    "```bash\n",
    "export API_TOKEN=super-secret-token\n",
    "uvicorn proxy:app --host 127.0.0.1 --port 8002 --reload\n",
    "```\n",
    "\n",
    "> The proxy exposes the same endpoints as Datasette but now requires `Authorization: Bearer super-secret-token` and enforces a per‑minute request cap (default 60). It returns `429` with `Retry-After` when exceeded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e9c31",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part A — API Harvester with Retries, Pagination, and Snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3efe406",
   "metadata": {},
   "source": [
    "### A1. Project scaffolding\n",
    "\n",
    "Set up the base configuration and create necessary directories for artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, hashlib, time, datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = \"http://127.0.0.1:8002\"   # go through proxy\n",
    "DB   = \"northwind\"\n",
    "TABLE = \"Orders\"\n",
    "PAGE_SIZE = 200\n",
    "TOKEN = \"super-secret-token\"\n",
    "\n",
    "ARTIFACTS = Path(\"artifacts\")\n",
    "RAW_DIR = ARTIFACTS / \"raw\"\n",
    "MANIFEST = ARTIFACTS / \"manifest.jsonl\"\n",
    "PARQUET_DIR = ARTIFACTS / \"parquet\"\n",
    "\n",
    "for d in [RAW_DIR, PARQUET_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Created artifact directories: {RAW_DIR}, {PARQUET_DIR}\")\n",
    "print(f\"✓ Manifest will be saved to: {MANIFEST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beacec3c",
   "metadata": {},
   "source": [
    "### A2. Resilient GET helper with exponential backoff + jitter and `Retry-After`\n",
    "\n",
    "This function implements robust retry logic with:\n",
    "- Exponential backoff with jitter\n",
    "- Respect for `Retry-After` headers (429 responses)\n",
    "- Handling of transient errors (500, 502, 503, 504)\n",
    "- Timeout and connection error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be24ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, random\n",
    "\n",
    "class HarvestError(Exception):\n",
    "    pass\n",
    "\n",
    "def resilient_get(url, params=None, headers=None, max_retries=6):\n",
    "    \"\"\"GET with exponential backoff, jitter, and Retry-After support.\"\"\"\n",
    "    backoff = 0.5\n",
    "    headers = headers or {}\n",
    "    \n",
    "    for attempt in range(1, max_retries+1):\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, headers=headers, timeout=20)\n",
    "            \n",
    "            if resp.status_code == 200:\n",
    "                return resp\n",
    "            \n",
    "            if resp.status_code == 429:\n",
    "                ra = resp.headers.get(\"Retry-After\")\n",
    "                delay = float(ra) if ra else backoff\n",
    "            elif resp.status_code in (500, 502, 503, 504):\n",
    "                delay = backoff\n",
    "            else:\n",
    "                raise HarvestError(f\"Unexpected {resp.status_code}: {resp.text[:200]}\")\n",
    "            \n",
    "            # exponential backoff with jitter\n",
    "            jitter = random.uniform(0, 0.25 * delay)\n",
    "            print(f\"  → Attempt {attempt}: status {resp.status_code}, sleeping {delay+jitter:.2f}s\")\n",
    "            time.sleep(delay + jitter)\n",
    "            backoff = min(backoff * 2, 8.0)\n",
    "            \n",
    "        except (requests.Timeout, requests.ConnectionError) as e:\n",
    "            print(f\"  → Attempt {attempt}: {type(e).__name__}, sleeping {backoff:.2f}s\")\n",
    "            time.sleep(backoff)\n",
    "            backoff = min(backoff * 2, 8.0)\n",
    "    \n",
    "    raise HarvestError(f\"Failed after {max_retries} retries: {url}\")\n",
    "\n",
    "print(\"✓ Resilient GET helper defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fae9fe",
   "metadata": {},
   "source": [
    "### A3. Cursor pagination + raw snapshot persistence\n",
    "\n",
    "Implement the core harvesting logic:\n",
    "- Paginate through API results using cursor tokens\n",
    "- Save raw JSON snapshots for each page\n",
    "- Record provenance metadata in a manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ddbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "HEADERS = {\"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "\n",
    "def sha256_bytes(b: bytes) -> str:\n",
    "    \"\"\"Compute SHA-256 hash of bytes.\"\"\"\n",
    "    return hashlib.sha256(b).hexdigest()\n",
    "\n",
    "def save_snapshot(payload_bytes: bytes, meta: dict) -> Path:\n",
    "    \"\"\"Save raw JSON snapshot and append metadata to manifest.\"\"\"\n",
    "    ts = dt.datetime.now(dt.UTC).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    fname = f\"{meta['table']}_{ts}_{meta['page']:05d}.json\"\n",
    "    fpath = RAW_DIR / fname\n",
    "    fpath.write_bytes(payload_bytes)\n",
    "    \n",
    "    # append to manifest as JSONL\n",
    "    record = {\n",
    "        **meta,\n",
    "        \"timestamp\": ts,\n",
    "        \"sha256\": sha256_bytes(payload_bytes),\n",
    "        \"bytes\": len(payload_bytes)\n",
    "    }\n",
    "    with MANIFEST.open(\"a\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(json.dumps(record) + \"\\n\")\n",
    "    \n",
    "    return fpath\n",
    "\n",
    "def harvest_table(base, db, table, page_size=100, start_cursor=None, extra_params=None):\n",
    "    \"\"\"Harvest a table with cursor pagination, saving snapshots.\"\"\"\n",
    "    url = f\"{base}/{db}/{table}.json\"\n",
    "    params = {\"_size\": page_size}\n",
    "    if extra_params:\n",
    "        params.update(extra_params)\n",
    "    \n",
    "    next_tok = start_cursor\n",
    "    page = 0\n",
    "    all_rows = []\n",
    "    columns = None  # Store column names from first response\n",
    "    \n",
    "    while True:\n",
    "        if next_tok:\n",
    "            params[\"_next\"] = next_tok\n",
    "        \n",
    "        resp = resilient_get(url, params=params, headers=HEADERS)\n",
    "        payload_bytes = resp.content\n",
    "        payload = resp.json()\n",
    "        rows = payload.get(\"rows\", [])\n",
    "        \n",
    "        # Capture column names from first page\n",
    "        if columns is None:\n",
    "            columns = payload.get(\"columns\", [])\n",
    "            print(f\"Columns: {columns}\")\n",
    "        \n",
    "        page += 1\n",
    "        \n",
    "        save_snapshot(payload_bytes, {\n",
    "            \"source\": url,\n",
    "            \"params\": params.copy(),\n",
    "            \"table\": table,\n",
    "            \"status\": resp.status_code,\n",
    "            \"page\": page,\n",
    "            \"columns\": columns  # Add to manifest\n",
    "        })\n",
    "        \n",
    "        print(f\"Page {page}: {len(rows)} rows\")\n",
    "        \n",
    "        if not rows:\n",
    "            break\n",
    "        \n",
    "        all_rows.extend(rows)\n",
    "        next_tok = payload.get(\"next\")\n",
    "        if not next_tok:\n",
    "            break\n",
    "    \n",
    "    # Create DataFrame with explicit column names\n",
    "    df = pd.DataFrame(all_rows, columns=columns)\n",
    "    return df    \n",
    "\n",
    "print(\"✓ Harvesting functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b940885d",
   "metadata": {},
   "source": [
    "### Harvest the Orders table\n",
    "\n",
    "Execute the initial harvest of all Orders data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5bd1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Harvesting {TABLE} from {BASE}/{DB}...\\n\")\n",
    "orders = harvest_table(BASE, DB, \"Orders\", page_size=PAGE_SIZE)\n",
    "\n",
    "print(f\"\\n✓ Harvested {len(orders)} total rows\")\n",
    "print(f\"✓ Shape: {orders.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4700dab",
   "metadata": {},
   "source": [
    "### Verify snapshots and manifest\n",
    "\n",
    "Check that raw snapshots and the provenance manifest were created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6533ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List raw snapshots\n",
    "snapshots = list(RAW_DIR.glob(\"*.json\"))\n",
    "print(f\"✓ Created {len(snapshots)} snapshot files:\")\n",
    "for snap in snapshots[:5]:\n",
    "    print(f\"  - {snap.name}\")\n",
    "if len(snapshots) > 5:\n",
    "    print(f\"  ... and {len(snapshots) - 5} more\")\n",
    "\n",
    "# Check manifest\n",
    "if MANIFEST.exists():\n",
    "    manifest_lines = MANIFEST.read_text().strip().split(\"\\n\")\n",
    "    print(f\"\\n✓ Manifest has {len(manifest_lines)} entries\")\n",
    "    print(f\"\\nSample manifest entry:\")\n",
    "    print(json.dumps(json.loads(manifest_lines[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab442de",
   "metadata": {},
   "source": [
    "### A4. Incremental harvesting with a high‑watermark\n",
    "\n",
    "Implement incremental data fetching using a watermark strategy based on `OrderDate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c18b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WATERMARK_FILE = ARTIFACTS / \"last_watermark.txt\"\n",
    "\n",
    "def read_watermark(default=\"1997-01-01\"):\n",
    "    \"\"\"Read the last watermark value, or return default.\"\"\"\n",
    "    if WATERMARK_FILE.exists():\n",
    "        return WATERMARK_FILE.read_text().strip()\n",
    "    return default\n",
    "\n",
    "def write_watermark(value: str):\n",
    "    \"\"\"Write the new watermark value.\"\"\"\n",
    "    WATERMARK_FILE.write_text(value)\n",
    "\n",
    "print(\"✓ Watermark functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5aaa2f",
   "metadata": {},
   "source": [
    "### Perform incremental harvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ea583",
   "metadata": {},
   "outputs": [],
   "source": [
    "last = read_watermark()\n",
    "print(f\"Starting watermark: {last}\\n\")\n",
    "\n",
    "# Harvest only rows since last watermark\n",
    "orders_inc = harvest_table(\n",
    "    BASE, DB, \"Orders\", page_size=PAGE_SIZE,\n",
    "    extra_params={\"OrderDate__gte\": last}  # Datasette accepts column filters\n",
    ")\n",
    "\n",
    "# Advance watermark to the max OrderDate we saw (if any)\n",
    "if not orders_inc.empty and \"OrderDate\" in orders_inc.columns:\n",
    "    new_wm = max(str(d) for d in orders_inc[\"OrderDate\"])\n",
    "    write_watermark(new_wm)\n",
    "    print(f\"\\n✓ Advanced watermark to: {new_wm}\")\n",
    "else:\n",
    "    print(f\"\\n✓ No new rows; watermark unchanged: {last}\")\n",
    "\n",
    "print(f\"\\nIncremental harvest retrieved {len(orders_inc)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046777e4",
   "metadata": {},
   "source": [
    "### A5. Normalize and persist to Parquet (partitioned)\n",
    "\n",
    "Transform the harvested data and write partitioned Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96889194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow as pa, pyarrow.parquet as pq\n",
    "\n",
    "# Check actual column names and strip table prefix if present\n",
    "print(\"Available columns:\", orders_inc.columns.tolist())\n",
    "print(\"DataFrame shape:\", orders_inc.shape)\n",
    "\n",
    "# Simple normalization: select a subset + parse dates\n",
    "use = orders_inc[[\"OrderID\",\"CustomerID\",\"OrderDate\",\"ShipCountry\",\"Freight\"]].copy()\n",
    "use[\"OrderDate\"] = pd.to_datetime(use[\"OrderDate\"], errors=\"coerce\").dt.date\n",
    "\n",
    "# Partition by ShipCountry for faster downstream filters\n",
    "out_root = PARQUET_DIR / \"orders\"\n",
    "out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for country, g in use.groupby(\"ShipCountry\"):\n",
    "    table = pa.Table.from_pandas(g, preserve_index=False)\n",
    "    pq.write_table(table, out_root / f\"shipcountry={country}.parquet\")\n",
    "\n",
    "# Quick QA\n",
    "files = list(out_root.glob(\"*.parquet\"))\n",
    "print(f\"\\n✓ Created {len(files)} partitioned Parquet files\")\n",
    "print(f\"\\nSample files:\")\n",
    "for f in files[:5]:\n",
    "    print(f\"  - {f.name}\")\n",
    "if len(files) > 5:\n",
    "    print(f\"  ... and {len(files) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac39d54",
   "metadata": {},
   "source": [
    "### A6. (Optional) Harvest a second table and join\n",
    "\n",
    "Demonstrate harvesting additional tables and performing joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34838e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Harvesting Order Details table...\\n\")\n",
    "DETAILS_PAGE_SIZE = 1_000\n",
    "# To make faster, modify max requests in proxy.py and restart\n",
    "details = harvest_table(BASE, DB, \"Order+Details\", page_size=DETAILS_PAGE_SIZE)\n",
    "\n",
    "# Normalize column names (Datasette may include spaces)\n",
    "details.columns = [c.replace(\" \", \"_\") for c in details.columns]\n",
    "\n",
    "print(f\"\\n✓ Harvested {len(details)} detail rows\")\n",
    "print(f\"\\nColumns: {list(details.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b93b0",
   "metadata": {},
   "source": [
    "### Join orders to details and persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join orders to details on OrderID\n",
    "merged = details.merge(use, on=\"OrderID\", how=\"inner\")\n",
    "\n",
    "print(f\"✓ Joined dataset has {len(merged)} rows\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ce8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist merged data to Parquet\n",
    "line_items_path = PARQUET_DIR / \"line_items.parquet\"\n",
    "merged.to_parquet(line_items_path, index=False)\n",
    "\n",
    "print(f\"✓ Saved joined data to {line_items_path}\")\n",
    "print(f\"  File size: {line_items_path.stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad3e5d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part B — SQL Extractor with Parameterization & Chunked Reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1565e28",
   "metadata": {},
   "source": [
    "### B1. Parameterized queries with user inputs\n",
    "\n",
    "Demonstrate safe SQL queries using parameterization to prevent SQL injection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d687bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "# Update file path as needed\n",
    "conn = sqlite3.connect(\"lab03/northwind.db\")\n",
    "\n",
    "country = \"USA\"\n",
    "start_date = \"1997-01-01\"\n",
    "\n",
    "q = \"\"\"\n",
    "SELECT o.OrderID, o.CustomerID, o.OrderDate, o.ShipCountry,\n",
    "       d.ProductID, d.UnitPrice, d.Quantity, d.Discount\n",
    "FROM Orders o\n",
    "JOIN [Order Details] d ON o.OrderID = d.OrderID\n",
    "WHERE o.ShipCountry = ? AND o.OrderDate >= ?\n",
    "ORDER BY o.OrderDate\n",
    "\"\"\"\n",
    "params = (country, start_date)\n",
    "\n",
    "rows = pd.read_sql_query(q, conn, params=params)\n",
    "\n",
    "print(f\"✓ Parameterized query returned {len(rows)} rows\")\n",
    "print(f\"\\nQuery: ShipCountry = '{country}' AND OrderDate >= '{start_date}'\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "rows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778aa296",
   "metadata": {},
   "source": [
    "### B2. Chunked reads → streaming Parquet append\n",
    "\n",
    "Process large result sets in chunks to manage memory efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8794a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = PARQUET_DIR / \"orders_joined.parquet\"\n",
    "writer = None\n",
    "total_rows = 0\n",
    "\n",
    "print(\"Processing query in chunks of 25,000 rows...\\n\")\n",
    "\n",
    "for i, chunk in enumerate(pd.read_sql_query(q, conn, params=params, chunksize=25_000), 1):\n",
    "    # Optional transforms\n",
    "    chunk[\"OrderDate\"] = pd.to_datetime(chunk[\"OrderDate\"], errors=\"coerce\")\n",
    "    \n",
    "    table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "    \n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(out_path, table.schema)\n",
    "    \n",
    "    writer.write_table(table)\n",
    "    total_rows += len(chunk)\n",
    "    print(f\"  Chunk {i}: {len(chunk)} rows (total: {total_rows})\")\n",
    "\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(f\"\\n✓ Wrote {total_rows} total rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeecaff",
   "metadata": {},
   "source": [
    "### Verify the streamed Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df2771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ver = pd.read_parquet(out_path)\n",
    "\n",
    "print(f\"✓ Verification: read {len(ver)} rows from Parquet\")\n",
    "print(f\"\\nRandom sample:\")\n",
    "ver.sample(3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff910da2",
   "metadata": {},
   "source": [
    "### B3. (Optional) Partitioned write by date\n",
    "\n",
    "Partition data by year for optimized queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96eae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition by year of OrderDate\n",
    "rows2 = pd.read_sql_query(q, conn, params=params)\n",
    "rows2[\"OrderDate\"] = pd.to_datetime(rows2[\"OrderDate\"], errors=\"coerce\")\n",
    "rows2[\"year\"] = rows2[\"OrderDate\"].dt.year\n",
    "\n",
    "print(\"Writing partitioned files by year...\\n\")\n",
    "\n",
    "for yr, g in rows2.groupby(\"year\"):\n",
    "    yr_path = PARQUET_DIR / f\"orders_year={yr}.parquet\"\n",
    "    pq.write_table(pa.Table.from_pandas(g, preserve_index=False), yr_path)\n",
    "    print(f\"  Year {yr}: {len(g)} rows → {yr_path.name}\")\n",
    "\n",
    "year_files = list(PARQUET_DIR.glob(\"orders_year=*.parquet\"))\n",
    "print(f\"\\n✓ Created {len(year_files)} year-partitioned files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4550f6a",
   "metadata": {},
   "source": [
    "### Clean up database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e58fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "print(\"✓ Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e28b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part C — Wrap‑Up & Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd64b66e",
   "metadata": {},
   "source": [
    "### Reflection Questions\n",
    "\n",
    "**1. How does your harvester treat `429` vs `500`? What signal does `Retry-After` convey? Why add jitter?**\n",
    "\n",
    "- **429 (Rate Limit):** The harvester respects the `Retry-After` header when present, which tells us exactly how long to wait before retrying. If no `Retry-After` is present, we fall back to exponential backoff. This is a \"client fault\" that we can resolve by waiting.\n",
    "\n",
    "- **500+ (Server Errors):** These are transient server-side failures. We use exponential backoff to give the server time to recover.\n",
    "\n",
    "- **Jitter:** Adding random jitter (0-25% of the delay) prevents the \"thundering herd\" problem where multiple clients retry simultaneously, potentially overwhelming a recovering service. It spreads out retry attempts across time.\n",
    "\n",
    "**2. Explain your high‑watermark logic. What happens if the API returns out‑of‑order data?**\n",
    "\n",
    "- The **high-watermark** tracks the maximum `OrderDate` seen in each harvest run and is persisted to `last_watermark.txt`.\n",
    "- On subsequent runs, we only request rows with `OrderDate >= last_watermark`, making the harvest incremental and efficient.\n",
    "- **Out-of-order data:** If data arrives late (e.g., an order backdated after we've advanced the watermark), we'll miss it. Solutions:\n",
    "  - Use `>=` with deduplication on write\n",
    "  - Use an `updated_at` field instead of `created_at`\n",
    "  - Run periodic backfills with an overlap window\n",
    "\n",
    "**3. Why are parameterized queries critical, even for local labs? Provide a small example of a risky string‑formatted SQL.**\n",
    "\n",
    "**Critical because:**\n",
    "- Prevents SQL injection attacks\n",
    "- Handles special characters correctly (quotes, backslashes)\n",
    "- Separates code from data\n",
    "- Good habit for production code\n",
    "\n",
    "**Risky example:**\n",
    "```python\n",
    "# DANGEROUS - Never do this!\n",
    "country = \"USA'; DROP TABLE Orders; --\"\n",
    "query = f\"SELECT * FROM Orders WHERE ShipCountry = '{country}'\"\n",
    "# This would execute: SELECT * FROM Orders WHERE ShipCountry = 'USA'; DROP TABLE Orders; --'\n",
    "```\n",
    "\n",
    "**Safe example:**\n",
    "```python\n",
    "# SAFE - Always do this\n",
    "query = \"SELECT * FROM Orders WHERE ShipCountry = ?\"\n",
    "params = (country,)\n",
    "pd.read_sql_query(query, conn, params=params)\n",
    "```\n",
    "\n",
    "**4. When would you choose chunked reads? Name one trade‑off.**\n",
    "\n",
    "**Use chunked reads when:**\n",
    "- Working with datasets larger than available RAM\n",
    "- Streaming data to disk or another system\n",
    "- Need to start processing before full query completes\n",
    "- Want to show progress for long-running queries\n",
    "\n",
    "**Trade-off:**\n",
    "- **Complexity vs Memory:** Chunked processing adds code complexity (managing the writer, aggregating results) but prevents memory exhaustion. You also can't easily perform operations that require the full dataset (like certain aggregations or sorts) without collecting chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589452aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary of Artifacts Created\n",
    "\n",
    "Let's review what was created during this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77450a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ARTIFACT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Raw snapshots\n",
    "snapshots = list(RAW_DIR.glob(\"*.json\"))\n",
    "print(f\"\\n📁 Raw JSON Snapshots: {len(snapshots)} files\")\n",
    "print(f\"   Location: {RAW_DIR}\")\n",
    "total_bytes = sum(f.stat().st_size for f in snapshots)\n",
    "print(f\"   Total size: {total_bytes:,} bytes\")\n",
    "\n",
    "# Manifest\n",
    "if MANIFEST.exists():\n",
    "    manifest_lines = len(MANIFEST.read_text().strip().split(\"\\n\"))\n",
    "    print(f\"\\n📋 Provenance Manifest: {manifest_lines} entries\")\n",
    "    print(f\"   Location: {MANIFEST}\")\n",
    "\n",
    "# Watermark\n",
    "if WATERMARK_FILE.exists():\n",
    "    wm = WATERMARK_FILE.read_text().strip()\n",
    "    print(f\"\\n🔖 High-Watermark: {wm}\")\n",
    "    print(f\"   Location: {WATERMARK_FILE}\")\n",
    "\n",
    "# Parquet files\n",
    "parquet_files = list(PARQUET_DIR.rglob(\"*.parquet\"))\n",
    "print(f\"\\n📊 Parquet Files: {len(parquet_files)} files\")\n",
    "print(f\"   Location: {PARQUET_DIR}\")\n",
    "for pf in parquet_files:\n",
    "    size = pf.stat().st_size\n",
    "    rel_path = pf.relative_to(PARQUET_DIR)\n",
    "    print(f\"   - {rel_path} ({size:,} bytes)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ Lab 03 Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99525949",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Resilient API Harvesting:** Implemented exponential backoff with jitter and proper handling of rate limits and transient errors.\n",
    "\n",
    "2. **Provenance Tracking:** Every page fetch is recorded with checksums, timestamps, and metadata for full audit trails.\n",
    "\n",
    "3. **Incremental Processing:** High-watermark strategy enables efficient incremental harvests without re-downloading existing data.\n",
    "\n",
    "4. **Partitioned Storage:** Parquet files organized by dimensions (ShipCountry, year) for optimized downstream queries.\n",
    "\n",
    "5. **SQL Best Practices:** Parameterized queries prevent injection attacks; chunked reads manage memory efficiently.\n",
    "\n",
    "6. **Production-Ready Patterns:** Auth headers, retry budgets, structured manifests, and idempotent operations prepare you for real-world data pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
