{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4dc09e",
   "metadata": {},
   "source": [
    "# Miniâ€‘Capstone Solution Notebook â€” CSV â†’ RAG & Instruction Datasets\n",
    "**This notebook contains one possible reference solution.** It follows the capstone tasks stepâ€‘byâ€‘step, producing the required artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8ac8daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/ask_assistant_corpus.csv')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, regex as re, json, orjson, hashlib, itertools, math, random\n",
    "from datetime import datetime, timezone\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "BASE = Path('.')\n",
    "ART = Path('artifacts')\n",
    "for p in [ART/'jsonl', ART/'samples', ART/'stats', ART/'prompts', ART/'datasets', ART/'tokenizer']:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "source = Path('ask_assistant_corpus.csv')\n",
    "DATA = Path('data')\n",
    "DATA.mkdir(parents=True, exist_ok=True)\n",
    "shutil.copy2(source, DATA)\n",
    "\n",
    "CSV_FILE = DATA / 'ask_assistant_corpus.csv'\n",
    "CSV_FILE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb0344a",
   "metadata": {},
   "source": [
    "## Part A â€” Ingest & Hygiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f0811b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,\n",
       "   id             category    topic difficulty                         source  \\\n",
       " 0  1  security_compliance  printer     medium           seed://developer/api   \n",
       " 1  2  security_compliance      sso       easy  seed://product_faq/dashboards   \n",
       " \n",
       "    created_at                                           question  \\\n",
       " 0  2025-04-03         FYI, How do I reset my AcmeCloud password?   \n",
       " 1  2025-04-29  Sorry if this is basicâ€”How do I cancel my subs...   \n",
       " \n",
       "                                               answer  \n",
       " 0  Yes. Visit Integrations and connect **Make**. ...  \n",
       " 1  Enable 2FA in Settings â†’ Security. Choose **em...  )"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Robust load\n",
    "df = pd.read_csv(CSV_FILE, dtype={\n",
    "    'id': str, 'category': str, 'topic': str, 'difficulty': str,\n",
    "    'question': str, 'answer': str, 'source': str, 'created_at': str\n",
    "})\n",
    "len(df), df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21cb39e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_text(s: str) -> str:\n",
    "    # Preserve meaning-bearing punctuation, collapse whitespace\n",
    "    s = s or \"\"\n",
    "    s = s.replace('\\r','')\n",
    "    s = re.sub(r\"[\\t ]+\", \" \", s)\n",
    "    s = re.sub(r\" *\\n *\", \"\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "df['question_norm'] = df['question'].map(normalize_text)\n",
    "df['answer_norm'] = df['answer'].map(normalize_text)\n",
    "\n",
    "# Governance gates\n",
    "MIN_Q = 5\n",
    "MIN_A_CHARS = 20\n",
    "mask = (df['question_norm'].str.len() >= MIN_Q) & (df['answer_norm'].str.len() >= MIN_A_CHARS)\n",
    "df_clean = df.loc[mask].copy()\n",
    "\n",
    "# Deduplicate by (normalized question + answer); keep most recent created_at\n",
    "df_clean['created_at_dt'] = pd.to_datetime(df_clean['created_at'], errors='coerce')\n",
    "sig = (df_clean['question_norm'].str.lower() + ' || ' + df_clean['answer_norm'].str.lower()).map(\n",
    "    lambda s: hashlib.sha1(s.encode('utf-8')).hexdigest()\n",
    ")\n",
    "df_clean['content_key'] = sig\n",
    "df_clean.sort_values(['content_key','created_at_dt'], ascending=[True, False], inplace=True)\n",
    "df_clean = df_clean.drop_duplicates(subset=['content_key'], keep='first')\n",
    "before, after = len(df), len(df_clean)\n",
    "before, after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ca8222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('artifacts/samples/capstone_csv_sample.jsonl')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviewer sample\n",
    "samp_path = ART/'samples'/'capstone_csv_sample.jsonl'\n",
    "with samp_path.open('w', encoding='utf-8') as f:\n",
    "    for _, r in df_clean.head(5).iterrows():\n",
    "        rec = {\n",
    "            'id': r['id'],\n",
    "            'question': r['question_norm'],\n",
    "            'answer': r['answer_norm'],\n",
    "            'category': r['category'],\n",
    "            'topic': r['topic'],\n",
    "            'difficulty': r['difficulty']\n",
    "        }\n",
    "        f.write(orjson.dumps(rec).decode() + '\\n')\n",
    "samp_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff41c3",
   "metadata": {},
   "source": [
    "## Part B â€” RAG Chunks with Provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eeecb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:00<00:00, 11457.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['artifacts/jsonl/rag_chunks_from_csv_000.jsonl'], 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def split_chunks(text: str, max_chars=1100, overlap=180):\n",
    "    text = re.sub(r\"\\s+\", \" \", str(text)).strip()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        end = min(i + max_chars, len(text))\n",
    "        chunks.append(text[i:end])\n",
    "        if end == len(text):\n",
    "            break\n",
    "        i = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "schema = 'rag-chunk-v1'\n",
    "max_shard_bytes = 50_000_000\n",
    "idx = 0; cur = 0\n",
    "out = open(ART/'jsonl'/f\"rag_chunks_from_csv_{idx:03d}.jsonl\", 'w', encoding='utf-8')\n",
    "\n",
    "def write_line(obj):\n",
    "    global idx, cur, out\n",
    "    line = orjson.dumps(obj).decode() + '\\n'\n",
    "    if cur + len(line.encode('utf-8')) > max_shard_bytes:\n",
    "        out.close(); idx += 1; cur = 0\n",
    "        out = open(ART/'jsonl'/f\"rag_chunks_from_csv_{idx:03d}.jsonl\", 'w', encoding='utf-8')\n",
    "    out.write(line); cur += len(line.encode('utf-8'))\n",
    "\n",
    "for _, r in tqdm(df_clean.iterrows(), total=len(df_clean)):\n",
    "    doc_id = r['id']\n",
    "    text = r['answer_norm']\n",
    "    chunks = split_chunks(text)\n",
    "    for j, ch in enumerate(chunks):\n",
    "        rec = {\n",
    "            'doc_id': doc_id,\n",
    "            'chunk_id': f\"{doc_id}-{j:04d}\",\n",
    "            'text': ch,\n",
    "            'metadata': {\n",
    "                'category': r['category'],\n",
    "                'topic': r['topic'],\n",
    "                'difficulty': r['difficulty'],\n",
    "                'source': r['source'],\n",
    "                'schema_version': schema\n",
    "            }\n",
    "        }\n",
    "        write_line(rec)\n",
    "\n",
    "out.close()\n",
    "files = sorted(glob(str(ART/'jsonl'/'rag_chunks_from_csv_*.jsonl')))\n",
    "files[:3], len(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a56d232a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file': 'artifacts/jsonl/rag_chunks_from_csv_000.jsonl',\n",
       "  'total': 8000,\n",
       "  'bad': 0,\n",
       "  'short_text': 0}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate & sample\n",
    "import json, itertools\n",
    "def validate_jsonl(path):\n",
    "    total = bad = short = 0\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                ok = isinstance(obj, dict) and 'text' in obj and 'metadata' in obj and len(obj['text']) >= 40\n",
    "                if not ok:\n",
    "                    bad += 1\n",
    "                if len(obj.get('text','')) < 40:\n",
    "                    short += 1\n",
    "            except Exception:\n",
    "                bad += 1\n",
    "    return {'file': path, 'total': total, 'bad': bad, 'short_text': short}\n",
    "\n",
    "val = [validate_jsonl(p) for p in files]\n",
    "val[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf96477f",
   "metadata": {},
   "source": [
    "## Part C â€” Instructionâ€‘Tuning JSONL (Trio & Promptâ€‘Completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e22db565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRIO = ART/'jsonl'/'instruct_trio.jsonl'\n",
    "PC   = ART/'jsonl'/'instruct_prompt_completion.jsonl'\n",
    "\n",
    "TEMPLATE = \"### Instruction:\\n{q}\\n\\n### Response:\\n\"\n",
    "\n",
    "kept = 0\n",
    "with TRIO.open('w', encoding='utf-8') as ft, PC.open('w', encoding='utf-8') as fp:\n",
    "    for _, r in df_clean.iterrows():\n",
    "        instruction = r['question_norm']\n",
    "        output = r['answer_norm']\n",
    "        if len(output) < 20: \n",
    "            continue\n",
    "        meta = {\n",
    "            'id': r['id'], 'category': r['category'], 'topic': r['topic'],\n",
    "            'difficulty': r['difficulty'], 'created_at': r['created_at'],\n",
    "            'schema_version': 'trio-v1'\n",
    "        }\n",
    "        ft.write(orjson.dumps({'instruction': instruction, 'input': '', 'output': output, 'metadata': meta}).decode() + '\\n')\n",
    "        prompt = TEMPLATE.format(q=instruction)\n",
    "        fp.write(orjson.dumps({'prompt': prompt, 'completion': output, 'metadata': meta}).decode() + '\\n')\n",
    "        kept += 1\n",
    "kept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41826564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pc_count': 8000,\n",
       " 'prompt_mean_words': 15.288125,\n",
       " 'completion_mean_words': 16.8485}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length governance & stats\n",
    "import json, numpy as np\n",
    "def token_proxy(s): return max(1, len(s.split()))\n",
    "\n",
    "stats = {'count_pc': 0, 'prompt_tokens': [], 'completion_tokens': []}\n",
    "with open(PC, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        o = json.loads(line)\n",
    "        stats['count_pc'] += 1\n",
    "        stats['prompt_tokens'].append(token_proxy(o['prompt']))\n",
    "        stats['completion_tokens'].append(token_proxy(o['completion']))\n",
    "\n",
    "import statistics as st\n",
    "summary = {\n",
    "    'pc_count': stats['count_pc'],\n",
    "    'prompt_mean_words': float(st.mean(stats['prompt_tokens'])) if stats['prompt_tokens'] else 0.0,\n",
    "    'completion_mean_words': float(st.mean(stats['completion_tokens'])) if stats['completion_tokens'] else 0.0\n",
    "}\n",
    "Path(ART/'stats'/'instruction_stats.json').write_text(json.dumps(summary, indent=2), encoding='utf-8')\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af03a3",
   "metadata": {},
   "source": [
    "## Part D â€” ðŸ¤— Datasets + (Optional) Offline Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dc45827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sysadmin/llm_venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 8000 examples [00:00, 141154.04 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:00<00:00, 50060.77 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion', 'metadata'],\n",
       "        num_rows: 6400\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'completion', 'metadata'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'completion', 'metadata'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds = load_dataset('json', data_files=str(PC), split='train')\n",
    "ds = ds.filter(lambda ex: len(ex['prompt'])>0 and len(ex['completion'])>0)\n",
    "seed = 42\n",
    "train_test = ds.train_test_split(test_size=0.2, seed=seed)\n",
    "val_test = train_test['test'].train_test_split(test_size=0.5, seed=seed)\n",
    "dds = DatasetDict({'train': train_test['train'], 'validation': val_test['train'], 'test': val_test['test']})\n",
    "for k in dds: dds[k] = dds[k].shuffle(seed=seed)\n",
    "dds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7377279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6400/6400 [00:00<00:00, 89531.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:00<00:00, 62564.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:00<00:00, 54789.01 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'artifacts/datasets/pc_splits'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dds.save_to_disk(str(ART/'datasets'/'pc_splits'))\n",
    "str(ART/'datasets'/'pc_splits')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "768c387b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6400/6400 [00:01<00:00, 3705.79 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:00<00:00, 3632.71 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:00<00:00, 3570.27 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 6400\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional tiny tokenizer (Byteâ€‘Level BPE) and mapping\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.processors import ByteLevel as ByteLevelProcessor\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "\n",
    "train_txt = ART/'tokenizer'/'train_prompts.txt'\n",
    "with train_txt.open('w', encoding='utf-8') as f:\n",
    "    for ex in dds['train']:\n",
    "        f.write(ex['prompt'] + '\\n')\n",
    "\n",
    "tok = Tokenizer(BPE(unk_token='<unk>'))\n",
    "tok.pre_tokenizer = ByteLevel()\n",
    "trainer = BpeTrainer(vocab_size=8000, min_frequency=2, special_tokens=['<unk>','<pad>','<bos>','<eos>'])\n",
    "tok.train([str(train_txt)], trainer)\n",
    "tok.post_processor = ByteLevelProcessor()\n",
    "tok.decoder = ByteLevelDecoder()\n",
    "tok.save(str(ART/'tokenizer'/'bytebpe.json'))\n",
    "\n",
    "# Map prompts+completions to ids (simple concat)\n",
    "from tokenizers import Tokenizer as T2\n",
    "tok2 = T2.from_file(str(ART/'tokenizer'/'bytebpe.json'))\n",
    "pad_id = tok2.token_to_id('<pad>')\n",
    "eos_id = tok2.token_to_id('<eos>') or 0\n",
    "MAX_LEN = 512\n",
    "\n",
    "def encode_batch(batch):\n",
    "    ids, attn = [], []\n",
    "    for p, c in zip(batch['prompt'], batch['completion']):\n",
    "        text = p + c\n",
    "        enc = tok2.encode(text)\n",
    "        x = enc.ids[:MAX_LEN-1] + [eos_id]\n",
    "        pad_len = MAX_LEN - len(x)\n",
    "        if pad_len > 0:\n",
    "            x = x + [pad_id]*pad_len\n",
    "        m = [1]*min(len(enc.ids)+1, MAX_LEN) + [0]*max(0, MAX_LEN - (min(len(enc.ids)+1, MAX_LEN)))\n",
    "        ids.append(x); attn.append(m)\n",
    "    return {'input_ids': ids, 'attention_mask': attn}\n",
    "\n",
    "batched = dds.map(encode_batch, batched=True, batch_size=256, remove_columns=dds['train'].column_names)\n",
    "batched\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497266ff",
   "metadata": {},
   "source": [
    "## Part E â€” Fewâ€‘Shot Exemplar Bank & Prompt Packs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aaf09bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build exemplar bank from Trio\n",
    "bank = []\n",
    "with open(ART/'jsonl'/'instruct_trio.jsonl','r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        o = json.loads(line)\n",
    "        if len(o.get('output','').split()) >= 5:\n",
    "            bank.append({'instruction': o['instruction'], 'input': o.get('input',''), 'output': o['output']})\n",
    "len(bank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddc38a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction:\\nHow do I set up SSO with SSO (SAML)?\\n\\n### Input:\\n\\n\\n### Response:\\nYes. Visit Integrations and connect **Microsoft Teams**. You may need admin permissions in both systems.\\n\\n### Instruction:\\nHow do I set up SSO with SSO (SAML)?\\n\\n### Input:\\n\\n\\n### Response:\\nOpen Reports â†’ Export and choose **JSON**. Exports are emailed and available in your download history.\\n\\n### Instruction:\\nHow do I set up SSO with SSO (SAML)?\\n\\n### Input:\\n\\n\\n### Response:\\nUse Import â†’ Upload and select **JSON**. Map fields, run a preview, and start the import; invalid rows are reported.\\n\\n### Instruction:\\nSummarize'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFâ€‘IDF selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "bank_df = pd.DataFrame(bank)\n",
    "corpus = (bank_df['instruction'] + ' ' + bank_df['input']).tolist()\n",
    "vec = TfidfVectorizer(min_df=2, max_df=0.9, ngram_range=(1,2))\n",
    "X = vec.fit_transform(corpus)\n",
    "\n",
    "def topk(query_instruction: str, k: int = 3):\n",
    "    qv = vec.transform([query_instruction])\n",
    "    sims = cosine_similarity(qv, X)[0]\n",
    "    idxs = np.argsort(-sims)[:k]\n",
    "    return bank_df.iloc[idxs][['instruction','input','output']].to_dict(orient='records')\n",
    "\n",
    "HDR_TMPL = \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    "def build_fewshot(task_instruction: str, task_input: str = '', k: int = 3):\n",
    "    shots = topk(task_instruction, k=k)\n",
    "    parts = []\n",
    "    for ex in shots:\n",
    "        parts.append(HDR_TMPL.format(instruction=ex['instruction'], input=ex.get('input','')) + ex['output'] + \"\\n\\n\")\n",
    "    parts.append(HDR_TMPL.format(instruction=task_instruction, input=task_input))\n",
    "    return ''.join(parts), shots\n",
    "\n",
    "prompt, used = build_fewshot(\"Summarize SSO setup steps\", k=3)\n",
    "prompt[:600]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b610e83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artifacts/prompts/fewshot_prompt_pack.jsonl'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persist prompt pack\n",
    "pack = ART/'prompts'/'fewshot_prompt_pack.jsonl'\n",
    "with pack.open('w', encoding='utf-8') as f:\n",
    "    tasks = [\n",
    "        {'instruction':'Summarize SSO setup steps','input':'', 'k':3},\n",
    "        {'instruction':'Explain rate limits policy','input':'', 'k':4},\n",
    "        {'instruction':'Create a troubleshooting checklist for SAML claims','input':'', 'k':3},\n",
    "    ]\n",
    "    for t in tasks:\n",
    "        p, used = build_fewshot(t['instruction'], t.get('input',''), k=t['k'])\n",
    "        rec = {'prompt': p, 'metadata': {'k': t['k'], 'used': used}}\n",
    "        f.write(orjson.dumps(rec).decode() + '\\n')\n",
    "str(pack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc94b81",
   "metadata": {},
   "source": [
    "## Wrapâ€‘Up & Artifacts\n",
    "- JSONL: `artifacts/jsonl/rag_chunks_from_csv_*.jsonl`, `artifacts/jsonl/instruct_trio.jsonl`, `artifacts/jsonl/instruct_prompt_completion.jsonl`\n",
    "- Datasets: `artifacts/datasets/pc_splits/`\n",
    "- Tokenizer: `artifacts/tokenizer/bytebpe.json`\n",
    "- Prompts: `artifacts/prompts/fewshot_prompt_pack.jsonl`\n",
    "- Samples/Stats under `artifacts/samples/` and `artifacts/stats/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
